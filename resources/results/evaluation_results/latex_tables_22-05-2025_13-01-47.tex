% LaTeX Tables for Evaluation Results
% Generated on 2025-05-22 13:01:47
% Based on evaluation files:
% - evaluation_results_22-05-2025_12-59-17.csv
% - evaluation_summary_22-05-2025_12-59-17.json
% - classification_metrics_22-05-2025_12-59-17.json

\begin{table}[H]
\centering
\caption{Evaluation Summary Table}
\label{tab:evaluation_summary}
\begin{tabular}{@{}lp{2cm}@{}}
\toprule
\textbf{Field} & \textbf{Result} \\
\midrule
Predicted Outcome (Accu.) & \textbf{85.51\%} \\
Justification Outcome (SED) &  \textbf{0.8628} \\
Justification Payment (SED) &  \textbf{0.8439} \\
Justification Outcome (IoU) &  \textbf{0.8660} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{String Edit Distance Similarity Results}
\label{tab:string_edit_distance_results}
\begin{tabular}{lc}
\toprule
\textbf{Similarity Metric} & \textbf{Average Score} \\
\midrule
Outcome Justification Similarity & 0.8628 \\
Payment Justification Similarity & 0.8439 \\
Combined Justification Similarity & 0.8534 \\
Justification IoU & 0.8660 \\
\midrule
\multicolumn{2}{p{13cm}}{\textit{Note:} Similarity scores range from 0.0 (completely different) to 1.0 (identical). 
The scores measure the normalized Levenshtein edit distance between predicted and ground truth justifications.
IoU (Intersection over Union) measures the word-level overlap between texts.} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{String Edit Distance Similarity Interpretation}
\label{tab:string_edit_distance_interpretation}
\begin{tabular}{p{3cm}p{10cm}}
\toprule
\textbf{Similarity Score} & \textbf{Interpretation} \\
\midrule
$1.0$ & Perfect match (identical strings or both null/empty) \\
$0.8 - 0.99$ & Very high similarity (minor differences) \\
$0.6 - 0.79$ & Substantial similarity (some differences) \\
$0.4 - 0.59$ & Moderate similarity (significant differences) \\
$0.2 - 0.39$ & Low similarity (major differences) \\
$0.0 - 0.19$ & Very low similarity (almost completely different) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Confusion Matrix of Outcome Classifications}
\label{tab:confusion_matrix}
\begin{tabular}{lccc|c}
\toprule
\multirow{2}{*}{\textbf{Actual Outcome}} & \multicolumn{3}{c}{\textbf{Predicted Outcome}} & \multirow{2}{*}{\textbf{Total}} \\
\cmidrule{2-4}
& \textbf{Yes} & \textbf{\begin{tabular}[c]{@{}c@{}}No - Unrelated\\event\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}No - condition(s)\\not met\end{tabular}} & \\
\midrule
\textbf{Yes} & 6 & 0 & 1 & 7 \\
\textbf{No \- Unrelated event} & 3 & 53 & 0 & 56 \\
\textbf{No \- condition(s) not met} & 2 & 4 & 0 & 6 \\
\midrule
\textbf{Total} & 11 & 57 & 1 & 69 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Performance Metrics by Outcome Category}
\label{tab:classification_metrics}
\begin{tabular}{lccccc}
\toprule
\textbf{Outcome Category} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} & \textbf{Accuracy} \\
\midrule
Yes & 0.5455 & 0.8571 & 0.6667 & 7 & \multirow{1}{*}{} \\
No \- Unrelated event & 0.9298 & 0.9464 & 0.9381 & 56 & \multirow{1}{*}{} \\
No \- condition(s) not met & 0.0000 & 0.0000 & 0.0000 & 6 & \multirow{1}{*}{} \\
\midrule
\textbf{Weighted Average} & 0.8100 & 0.8551 & 0.8290 & 69 & 0.8551 \\
\bottomrule
\multicolumn{6}{p{14cm}}{\textit{Note:} Overall outcome classification accuracy: 0.8551 (85.51\%).} \\
\end{tabular}
\end{table}
