% LaTeX Tables for Evaluation Results
% Generated on 2025-05-08 14:50:05
% Based on evaluation files:
% - evaluation_results_08-05-2025_14-48-58.csv
% - evaluation_summary_08-05-2025_14-48-58.json
% - classification_metrics_08-05-2025_14-48-58.json

\begin{table}[H]
\centering
\caption{Evaluation Summary Table}
\label{tab:evaluation_summary}
\begin{tabular}{@{}lp{2cm}@{}}
\toprule
\textbf{Field} & \textbf{Result} \\
\midrule
Predicted Outcome & \textbf{70.00\%} \\
Justification Outcome &  \textbf{0.1017} \\
Justification Payment &  \textbf{0.7594} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{String Edit Distance Similarity Results}
\label{tab:string_edit_distance_results}
\begin{tabular}{lc}
\toprule
\textbf{Similarity Metric} & \textbf{Average Score} \\
\midrule
Outcome Justification Similarity & 0.1017 \\
Payment Justification Similarity & 0.7594 \\
Combined Justification Similarity & 0.4306 \\
\midrule
\multicolumn{2}{p{13cm}}{\textit{Note:} Similarity scores range from 0.0 (completely different) to 1.0 (identical). 
The scores measure the normalized Levenshtein edit distance between predicted and ground truth justifications.} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{String Edit Distance Similarity Interpretation}
\label{tab:string_edit_distance_interpretation}
\begin{tabular}{p{3cm}p{10cm}}
\toprule
\textbf{Similarity Score} & \textbf{Interpretation} \\
\midrule
$1.0$ & Perfect match (identical strings or both null/empty) \\
$0.8 - 0.99$ & Very high similarity (minor differences) \\
$0.6 - 0.79$ & Substantial similarity (some differences) \\
$0.4 - 0.59$ & Moderate similarity (significant differences) \\
$0.2 - 0.39$ & Low similarity (major differences) \\
$0.0 - 0.19$ & Very low similarity (almost completely different) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Confusion Matrix of Outcome Classifications}
\label{tab:confusion_matrix}
\begin{tabular}{lcccc|c}
\toprule
\multirow{2}{*}{\textbf{Actual Outcome}} & \multicolumn{4}{c}{\textbf{Predicted Outcome}} & \multirow{2}{*}{\textbf{Total}} \\
\cmidrule{2-5}
& \textbf{Yes} & \textbf{\begin{tabular}[c]{@{}c@{}}No - Unrelated\\event\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}No - condition(s)\\not met\end{tabular}} & \textbf{Maybe} & \\
\midrule
\textbf{Yes} & 2 & 0 & 3 & 0 & 5 \\
\textbf{No \- Unrelated event} & 2 & 25 & 1 & 1 & 29 \\
\textbf{No \- condition(s) not met} & 2 & 2 & 1 & 0 & 5 \\
\textbf{Maybe} & 0 & 1 & 0 & 0 & 1 \\
\midrule
\textbf{Total} & 6 & 28 & 5 & 1 & 40 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Performance Metrics by Outcome Category}
\label{tab:classification_metrics}
\begin{tabular}{lccccc}
\toprule
\textbf{Outcome Category} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} & \textbf{Accuracy} \\
\midrule
Yes & 0.3333 & 0.4000 & 0.3636 & 5 & \multirow{1}{*}{} \\
No \- Unrelated event & 0.8929 & 0.8621 & 0.8772 & 29 & \multirow{1}{*}{} \\
No \- condition(s) not met & 0.2000 & 0.2000 & 0.2000 & 5 & \multirow{1}{*}{} \\
Maybe & 0.0000 & 0.0000 & 0.0000 & 1 & \multirow{1}{*}{} \\
\midrule
\textbf{Weighted Average} & 0.7140 & 0.7000 & 0.7064 & 40 & 0.7000 \\
\bottomrule
\multicolumn{6}{p{14cm}}{\textit{Note:} Overall outcome classification accuracy: 0.7000 (70.00\%).} \\
\end{tabular}
\end{table}
