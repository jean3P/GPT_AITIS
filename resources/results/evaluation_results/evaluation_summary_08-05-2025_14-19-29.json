{
  "total_output_questions": 82,
  "total_evaluated_questions": 82,
  "outcome_classification": {
    "accuracy": 0.6463414634146342,
    "category_metrics": {
      "Yes": {
        "precision": 0.4166666666666667,
        "recall": 0.7142857142857143,
        "f1-score": 0.5263157894736842,
        "support": 7.0
      },
      "No - Unrelated event": {
        "precision": 0.8703703703703703,
        "recall": 0.8392857142857143,
        "f1-score": 0.8545454545454545,
        "support": 56.0
      },
      "No - condition(s) not met": {
        "precision": 0.0625,
        "recall": 0.16666666666666666,
        "f1-score": 0.09090909090909091,
        "support": 6.0
      },
      "Maybe": {
        "precision": 0.0,
        "recall": 0.0,
        "f1-score": 0.0,
        "support": 13.0
      }
    }
  },
  "exact_outcome_matches": 53,
  "exact_outcome_match_percentage": 64.63414634146342,
  "avg_justification_similarity": 0.08531512426001196,
  "avg_payment_similarity": 0.8464664161538219,
  "avg_combined_justification_similarity": 0.46589077020691694
}