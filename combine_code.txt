# Combined Python Code from: /cluster/home/jeanpool.pereyrap/repos/GPT_AITIS/src
# Excluded directories: scripts
# Total files combined: 39
# ================================================================================


# File 1/39: assistant_manager.py
# --------------------------------------------------------------------------------

# src/assistant_manager.py

import openai
import logging
from config import OPENAI_API_KEY, VECTOR_STORE_EXPIRATION_DAYS, RESPONSE_FORMAT_PATH
from typing import List, Any

from utils import load_response_schema

logger = logging.getLogger(__name__)

client = openai.OpenAI(api_key=OPENAI_API_KEY)

def create_vector_store(name: str, file_paths: List[str], check: bool =True) -> Any:
    """
    Create a vector store in OpenAI's API, upload PDF files to it, and index them.

    Args:
        name (str): Name of the vector store.
        file_paths (list[str]): List of file paths to upload.
        check (bool): For logging purposes

    Returns:
        OpenAI VectorStore object
    """

    logger.info(f"Creating vector store")
    vector_store = client.vector_stores.create(
        name=name,
        expires_after={"anchor": "last_active_at", "days": VECTOR_STORE_EXPIRATION_DAYS},
    )
    streams = [open(path, "rb") for path in file_paths]
    file_batch = client.vector_stores.file_batches.upload_and_poll(vector_store_id=vector_store.id, files=streams)
    if check:
        logger.info(f"  File Batch Status: {file_batch.status}")
        logger.info(f"  File Batch Counts: {file_batch.file_counts}")

    return vector_store

def create_assistant(name: str, sys_prompt: str, model: str = "gpt-4o") -> Any:
    """
    Create an OpenAI Assistant configured to perform file search using a given system prompt.

    Args:
        name (str): Name of the assistant.
        sys_prompt (str): System-level instructions for the assistant.
        model (str): Model to use for generating responses.

    Returns:
        OpenAI Assistant object
    """
    logger.info(f"Creating assistant")
    response_schema = load_response_schema(RESPONSE_FORMAT_PATH)

    return client.beta.assistants.create(
        name=name,
        instructions=sys_prompt,
        model=model,
        tools=[{"type": "file_search"}],
        response_format={"type": "json_schema", "json_schema": response_schema}
    )


def update_assistant_vector(assistant_id: str, vector_store_id: str) -> None:
    """
    Link an assistant to a vector store so it can use file search during conversation.

    Args:
        assistant_id (str): The ID of the assistant.
        vector_store_id (str): The ID of the vector store.
    """
    logger.info(f"Update assistant vector")
    client.beta.assistants.update(
        assistant_id=assistant_id,
        tool_resources={"file_search": {"vector_store_ids": [vector_store_id]}}
    )


# End of assistant_manager.py
# ================================================================================

# File 2/39: config.py
# --------------------------------------------------------------------------------

# src/config.py
import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()
base_dir = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Load environment and configuration variables
OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY")
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
MODEL_NAME: str = "gpt-4o"
EMBEDDING_MODEL: str = "all-MiniLM-L6-v2"  # Keep for backward compatibility
OPENROUTER_API_KEY: str = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_SITE_URL: str = os.getenv("OPENROUTER_SITE_URL", "")  # Optional
OPENROUTER_SITE_NAME: str = os.getenv("OPENROUTER_SITE_NAME", "")  # Optional

# Configure embedding model path for the downloaded sentence transformer model
EMBEDDING_MODEL_PATH = "/cluster/scratch/jeanpool.pereyrap/models/embeddings/sentence-transformers_all-MiniLM-L6-v2"

# Verify the model exists, otherwise fall back to remote
if os.path.exists(EMBEDDING_MODEL_PATH):
    print(f"Using local embedding model from: {EMBEDDING_MODEL_PATH}")
else:
    # Fall back to HuggingFace model if local copy not found
    print(f"Local model not found at {EMBEDDING_MODEL_PATH}")
    print(f"Falling back to remote model")
    EMBEDDING_MODEL_PATH = "sentence-transformers/all-MiniLM-L6-v2"

# HuggingFace cache settings
# Set environment variables if not already set
if "HF_HUB_CACHE" not in os.environ:
    os.environ["HF_HUB_CACHE"] = "/cluster/scratch/cache/huggingface/hub"
if "HF_ASSETS_CACHE" not in os.environ:
    os.environ["HF_ASSETS_CACHE"] = "/cluster/scratch/cache/huggingface/assets"

# Constants for reference
HF_HUB_CACHE: str = os.environ["HF_HUB_CACHE"]
HF_ASSETS_CACHE: str = os.environ["HF_ASSETS_CACHE"]

# =============================================================================
# MODEL CONFIGURATION AND PATH MANAGEMENT
# =============================================================================

# Base directory for locally downloaded models
MODELS_BASE_DIR = f"/cluster/scratch/{os.environ.get('USER', 'jeanpool.pereyrap')}/models"

# Model path mappings - maps HuggingFace model names to local directories
MODEL_PATHS = {
    # Phi models
    "microsoft/phi-4": os.path.join(MODELS_BASE_DIR, "phi-4"),

    # Qwen models
    "Qwen/Qwen2.5-32B": os.path.join(MODELS_BASE_DIR, "qwen2.5-32b"),
    "Qwen/Qwen2.5-7B": os.path.join(MODELS_BASE_DIR, "qwen2.5-7b"),
    # "Qwen/Qwen2.5-14B": os.path.join(MODELS_BASE_DIR, "qwen2.5-14b"),

    # Local name aliases for convenience
    "phi-4": os.path.join(MODELS_BASE_DIR, "phi-4"),
    "qwen2.5-32b": os.path.join(MODELS_BASE_DIR, "qwen2.5-32b"),
    "qwen2.5-7b": os.path.join(MODELS_BASE_DIR, "qwen2.5-7b"),
    # "qwen2.5-14b": os.path.join(MODELS_BASE_DIR, "qwen2.5-14b"),
}

OPENROUTER_MODELS = {
    # Qwen models via OpenRouter
    "qwen/qwen-2.5-72b-instruct": "qwen/qwen-2.5-72b-instruct",
    "qwen/qwen-2.5-32b-instruct": "qwen/qwen-2.5-32b-instruct",
    "qwen/qwen-2.5-14b-instruct": "qwen/qwen-2.5-14b-instruct",
    "qwen/qwen-2.5-7b-instruct": "qwen/qwen-2.5-7b-instruct",
    "qwen/qwen-2.5-3b-instruct": "qwen/qwen-2.5-3b-instruct",

    # Other popular models
    "anthropic/claude-3.5-sonnet": "anthropic/claude-3.5-sonnet",
    "openai/gpt-4o": "openai/gpt-4o",
    "openai/gpt-4o-mini": "openai/gpt-4o-mini",
    "meta-llama/llama-3.1-405b-instruct": "meta-llama/llama-3.1-405b-instruct",
    "meta-llama/llama-3.1-70b-instruct": "meta-llama/llama-3.1-70b-instruct",
    "meta-llama/llama-3.1-8b-instruct": "meta-llama/llama-3.1-8b-instruct",
}


def get_openrouter_model_name(model_name: str) -> str:
    """Get the OpenRouter model name, with fallback to the input name."""
    return OPENROUTER_MODELS.get(model_name, model_name)


def is_openrouter_model(model_name: str) -> bool:
    """Check if a model name corresponds to an OpenRouter model."""
    if model_name in OPENROUTER_MODELS:
        return True

    # Check common OpenRouter patterns
    openrouter_patterns = [
        "qwen/", "anthropic/", "meta-llama/", "google/",
        "cohere/", "mistral/", "01-ai/", "deepseek/"
    ]

    return any(model_name.startswith(pattern) for pattern in openrouter_patterns)

# Model-specific generation configurations
MODEL_CONFIGS = {
    "microsoft/phi-4": {
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 1020,
        "temperature": 0.1,
        "do_sample": False,
        "repetition_penalty": 1.05,
        "pad_token_id": None  # Will be set to eos_token_id
    },
    "Qwen/Qwen2.5-32B": {
         # Model loading parameters (required)
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,

        # Generation parameters (essential for speed)
        "max_new_tokens": 1024,
        # "temperature": 0.0,
        "do_sample": True,
        "repetition_penalty": 1.0,
        "pad_token_id": None
    },
    "Qwen/Qwen2.5-7B": {
        # Same config as above
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 1024,
        "temperature": 0.0,
        "do_sample": False,
        "repetition_penalty": 1.1,
        "pad_token_id": None,
        "eos_token_id": None,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "output_scores": False,
        "use_cache": True,
        "no_repeat_ngram_size": 3,
    },
    "Qwen/Qwen2.5-14B": {
        # Same config as above
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 2048,
        "temperature": 0.0,
        "do_sample": False,
        "repetition_penalty": 1.1,
        "pad_token_id": None,
        "eos_token_id": None,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "output_scores": False,
        "use_cache": True,
        "no_repeat_ngram_size": 3,
    },
    "Qwen/Qwen3-14B": {
        # Same config as above
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 1024,
        "temperature": 0.0,
        "do_sample": False,
        "repetition_penalty": 1.1,
        "pad_token_id": None,
        "eos_token_id": None,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "output_scores": False,
        "use_cache": True,
        "no_repeat_ngram_size": 3,
    },
    "Qwen/Qwen3-32B": {
        # Same config as above
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 1024,
        "temperature": 0.0,
        "do_sample": False,
        "repetition_penalty": 1.1,
        "pad_token_id": None,
        "eos_token_id": None,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "output_scores": False,
        "use_cache": True,
        "no_repeat_ngram_size": 3,
    },
}

# Copy configs for local name aliases
MODEL_CONFIGS["phi-4"] = MODEL_CONFIGS["microsoft/phi-4"]
MODEL_CONFIGS["qwen2.5-32b"] = MODEL_CONFIGS["Qwen/Qwen2.5-32B"]
MODEL_CONFIGS["qwen2.5-7b"] = MODEL_CONFIGS["Qwen/Qwen2.5-7B"]
MODEL_CONFIGS["qwen2.5-14b"] = MODEL_CONFIGS["Qwen/Qwen2.5-14B"]

def get_clean_model_name(model_name: str) -> str:
    """
    Extract a clean model name for directory naming.

    Examples:
    - "microsoft/phi-4" -> "phi-4"
    - "qwen/qwen-2.5-72b-instruct" -> "qwen-2.5-72b-instruct"
    - "gpt-4o" -> "gpt-4o"
    - "Qwen/Qwen2.5-32B" -> "qwen2.5-32b"
    """
    # Handle local paths
    if "/" in model_name and not model_name.startswith(("http://", "https://")):
        # For patterns like "microsoft/phi-4" or "qwen/qwen-2.5-72b"
        clean_name = model_name.split("/")[-1]
    else:
        clean_name = model_name

    # Normalize to lowercase and replace dots/spaces with hyphens
    clean_name = clean_name.lower()
    clean_name = clean_name.replace(".", "-").replace(" ", "-").replace("_", "-")

    # Remove common prefixes/suffixes
    clean_name = clean_name.replace("-instruct", "").replace("-chat", "")

    return clean_name

def get_local_model_path(model_name: str) -> str:
    """
    Get local path for a model, falling back to original name if not found locally.

    Args:
        model_name: HuggingFace model name or local identifier

    Returns:
        Local path if model exists locally, otherwise original model name
    """
    # Check if it's already a local path
    if os.path.exists(model_name) and os.path.isdir(model_name):
        print(f"Using provided local path: {model_name}")
        return model_name

    # Check our model mappings
    local_path = MODEL_PATHS.get(model_name)
    if local_path and os.path.exists(local_path) and os.path.isdir(local_path):
        # Verify it's a valid model directory
        if os.path.exists(os.path.join(local_path, "config.json")):
            print(f"Using local model from: {local_path}")
            return local_path
        else:
            print(f"Local model directory found but missing config.json: {local_path}")

    # Check for case-insensitive matches
    model_name_lower = model_name.lower()
    for key, path in MODEL_PATHS.items():
        if key.lower() == model_name_lower and os.path.exists(path) and os.path.isdir(path):
            if os.path.exists(os.path.join(path, "config.json")):
                print(f"Using local model from case-insensitive match: {path}")
                return path

    # Fall back to original name (will download from HuggingFace)
    print(f"Local model not found for '{model_name}', using remote download")
    return model_name


def get_model_config(model_name: str) -> dict:
    """
    Get model-specific configuration.

    Args:
        model_name: Model name or path

    Returns:
        Dictionary with model configuration parameters
    """
    # First try to get config by exact name
    config = MODEL_CONFIGS.get(model_name)
    if config:
        return config.copy()

    # Try case-insensitive match
    model_name_lower = model_name.lower()
    for key, config in MODEL_CONFIGS.items():
        if key.lower() == model_name_lower:
            return config.copy()

    # Check if it's a local path and try to infer from directory name
    if os.path.exists(model_name):
        dir_name = os.path.basename(model_name.rstrip('/'))
        config = MODEL_CONFIGS.get(dir_name)
        if config:
            return config.copy()

    # Default configuration for unknown models
    print(f"Using default configuration for unknown model: {model_name}")
    return {
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 2048,
        "temperature": 0.1,
        "do_sample": False,
        "repetition_penalty": 1.05,
        "pad_token_id": None
    }


def is_qwen_model(model_name: str) -> bool:
    """
    Check if a model is a Qwen model.

    Args:
        model_name: Model name or path

    Returns:
        True if it's a Qwen model, False otherwise
    """
    model_name_lower = model_name.lower()
    qwen_indicators = ["qwen", "qwen2", "qwen2.5"]

    # Check direct indicators
    for indicator in qwen_indicators:
        if indicator in model_name_lower:
            return True

    # Check if it's a local path pointing to a Qwen model
    if os.path.exists(model_name):
        dir_name = os.path.basename(model_name.rstrip('/')).lower()
        for indicator in qwen_indicators:
            if indicator in dir_name:
                return True

    return False


def is_phi_model(model_name: str) -> bool:
    """
    Check if a model is a Phi model.

    Args:
        model_name: Model name or path

    Returns:
        True if it's a Phi model, False otherwise
    """
    model_name_lower = model_name.lower()
    phi_indicators = ["phi", "microsoft/phi"]

    # Check direct indicators
    for indicator in phi_indicators:
        if indicator in model_name_lower:
            return True

    # Check if it's a local path pointing to a Phi model
    if os.path.exists(model_name):
        dir_name = os.path.basename(model_name.rstrip('/')).lower()
        if "phi" in dir_name:
            return True

    return False


# =============================================================================
# LEGACY COMPATIBILITY (keeping your existing variables)
# =============================================================================

# Keep your existing Qwen variables for backward compatibility
QWEN_MODEL_PATH = MODEL_PATHS.get("Qwen/Qwen2.5-32B", "/cluster/scratch/jeanpool.pereyrap/models/qwen2.5-32b")
QWEN_MODEL_NAME = "Qwen/Qwen2.5-32B"
QWEN_CONFIG = MODEL_CONFIGS["Qwen/Qwen2.5-32B"]

# =============================================================================
# APPLICATION PATHS AND SETTINGS
# =============================================================================

MAX_QUESTIONS = 1
DATASET_PATH: str = os.path.join(base_dir, "resources/questions/questions.xlsx")
DOCUMENT_DIR: str = os.path.join(base_dir, "resources/documents/policies/")
RESULT_PATH: str = os.path.join(base_dir, f"resources/results/run_output_{MAX_QUESTIONS}.tsv")
RESPONSE_FORMAT_PATH: str = os.path.join(base_dir, "resources/response_formats/travel_insurance_agent.json")
LOG_DIR: str = os.path.join(base_dir, "resources/results/logs")
JSON_PATH: str = os.path.join(base_dir, "resources/results/json_output")

RAW_GT_PATH: str = os.path.join(base_dir, "resources/raw_ground_truth/")
GT_PATH: str = os.path.join(base_dir, "resources/ground_truth/")

VECTOR_STORE_EXPIRATION_DAYS: int = 30
VECTOR_NAME_PREFIX: str = "AITIS_"
EVALUATION_RESULTS_PATH: str = os.path.join(base_dir, "resources/results/")
EVALUATION_RESULTS_FILES_PATH: str = os.path.join(base_dir, "resources/results/evaluation_results/")
DASHBOARD_PATH: str = os.path.join(base_dir, "resources/results/dashboard.html")

# New constants for embedding caching
EMBEDDINGS_DIR: str = os.path.join(base_dir, "resources/embeddings")
CACHE_EMBEDDINGS: bool = True  # Can be set to False to force re-embedding


# =============================================================================
# UTILITY FUNCTIONS FOR MODEL MANAGEMENT
# =============================================================================

def list_available_local_models() -> dict:
    """
    List all locally available models.

    Returns:
        Dictionary mapping model names to their local paths
    """
    available_models = {}

    for model_name, path in MODEL_PATHS.items():
        if os.path.exists(path) and os.path.isdir(path):
            # Check if it looks like a valid model directory
            if os.path.exists(os.path.join(path, "config.json")):
                available_models[model_name] = path

    return available_models


def get_model_info(model_name: str) -> dict:
    """
    Get comprehensive information about a model.

    Args:
        model_name: Model name or path

    Returns:
        Dictionary with model information
    """
    local_path = get_local_model_path(model_name)
    config = get_model_config(model_name)

    info = {
        "model_name": model_name,
        "local_path": local_path,
        "is_local": local_path != model_name and os.path.exists(local_path),
        "is_qwen": is_qwen_model(model_name),
        "is_phi": is_phi_model(model_name),
        "config": config
    }

    # Add size information if local
    if info["is_local"]:
        try:
            # Removed redundant 'import os' - os is already imported at module level
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(local_path):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        total_size += os.path.getsize(filepath)
            info["size_gb"] = round(total_size / (1024 ** 3), 2)
        except Exception as e:
            print(f"Warning: Could not calculate model size for {model_name}: {e}")
            info["size_gb"] = "unknown"

    return info




# End of config.py
# ================================================================================

# File 3/39: get_pdf_code.py
# --------------------------------------------------------------------------------

import fitz  # PyMuPDF
import re
import os
from typing import Dict, List, Tuple
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def extract_sections_from_pdf(pdf_path: str) -> Dict[str, str]:
    """
    Extract sections from a PDF file based on numbered headers.

    Args:
        pdf_path: Path to the PDF file

    Returns:
        Dictionary mapping section headers to their content
    """
    # Validate file exists
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF file not found: {pdf_path}")

    logging.info(f"Processing PDF: {pdf_path}")

    try:
        # Open the PDF
        doc = fitz.open(pdf_path)

        # Extract text with page numbers for debugging
        text_by_pages = []
        for i, page in enumerate(doc):
            page_text = page.get_text()
            text_by_pages.append(page_text)
            logging.info(f"Page {i + 1} extracted: {len(page_text)} characters")

        full_text = "\n".join(text_by_pages)
        logging.info(f"Total text extracted: {len(full_text)} characters")

        # More flexible regex pattern for section headers
        # This pattern looks for:
        # 1. Optional newlines
        # 2. One or more digits followed by a period
        # 3. One or more whitespace characters
        # 4. A word starting with uppercase letter followed by any characters (not just letters)
        # 5. The entire match must not exceed a reasonable length for a header
        pattern = r'(?:\n|\r\n?)?(\d+\.\s+[A-Z][^\n\r]{1,60})'

        # Find all matches
        matches = list(re.finditer(pattern, full_text))
        logging.info(f"Found {len(matches)} potential section headers")

        if not matches:
            # If no matches, try an alternative pattern without numbers
            alt_pattern = r'(?:\n|\r\n?)([A-Z][A-Z\s]{2,50}:?)'
            matches = list(re.finditer(alt_pattern, full_text))
            logging.info(f"Tried alternative pattern, found {len(matches)} potential headers")

        # Print found headers for debugging
        for match in matches:
            logging.info(f"Found header: '{match.group(1).strip()}'")

        # Group text into sections
        sections = {}
        for i, match in enumerate(matches):
            header = match.group(1).strip()
            start = match.end()

            # Determine end of this section (start of next section or end of document)
            end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)

            # Extract content
            content = full_text[start:end].strip()

            # Skip empty sections
            if not content:
                logging.warning(f"Empty content for section '{header}', skipping")
                continue

            sections[header] = content
            logging.info(f"Extracted section '{header}': {len(content)} characters")

        return sections

    except Exception as e:
        logging.error(f"Error processing PDF: {str(e)}")
        raise
    finally:
        # Close the document
        if 'doc' in locals():
            doc.close()


def print_sections(sections: Dict[str, str], preview_length: int = 300):
    """
    Print sections with preview of content

    Args:
        sections: Dictionary of sections
        preview_length: Number of characters to preview for each section
    """
    if not sections:
        print("No sections found!")
        return

    print(f"\nFound {len(sections)} sections:\n")

    for section_title, section_content in sections.items():
        print(f"\n{'=' * 80}")
        print(f"=== {section_title} ===")
        print(f"{'=' * 80}\n")

        preview = section_content[:preview_length]
        print(f"{preview}...")
        print(f"\n[Total: {len(section_content)} characters]")


def main():
    # Update this path to your PDF location
    from config import DOCUMENT_DIR
    pdf_path = os.path.join(DOCUMENT_DIR, "18_Nobis - Baggage loss EN.pdf")

    try:
        sections = extract_sections_from_pdf(pdf_path)
        print_sections(sections)

        # Optionally save to individual files
        # output_dir = "extracted_sections"
        # os.makedirs(output_dir, exist_ok=True)
        # for title, content in sections.items():
        #     safe_title = "".join(c if c.isalnum() or c.isspace() else "_" for c in title)
        #     with open(os.path.join(output_dir, f"{safe_title}.txt"), "w", encoding="utf-8") as f:
        #         f.write(content)

    except Exception as e:
        logging.error(f"Failed to process PDF: {str(e)}")


if __name__ == "__main__":
    main()


# End of get_pdf_code.py
# ================================================================================

# File 4/39: logging_utils.py
# --------------------------------------------------------------------------------

# src/logging_utils.py

import logging
from pathlib import Path
import datetime
from typing import Optional


def setup_logging(config, log_file: Optional[str] = None):
    """
    Set up logging configuration.

    Args:
        config: Configuration object.
        log_file: Optional specific log file name.
    """
    log_dir = Path(config["logging"]["log_dir"])
    log_level_str = config["logging"]["log_level"]
    log_level = getattr(logging, log_level_str.upper())

    # Create log directory if it doesn't exist
    if not log_dir.exists():
        log_dir.mkdir(parents=True)

    # Generate log file name if not provided
    if not log_file:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = f"swereflect_{timestamp}.log"

    # Ensure the parent directory of log_file exists
    log_path = log_dir / log_file
    log_path.parent.mkdir(parents=True, exist_ok=True)

    # Configure logging
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s [%(levelname)s] %(name)s (%(filename)s:%(lineno)d): %(message)s",
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler()
        ]
    )

    # Log initial message
    logging.info(f"Logging initialized at {log_path}")


# End of logging_utils.py
# ================================================================================

# File 5/39: main.py
# --------------------------------------------------------------------------------

# src/main.py

from config import LOG_DIR, OPENROUTER_API_KEY
from logging_utils import setup_logging
from rag_runner import run_rag, run_batch_rag
import argparse
import logging

logger = logging.getLogger(__name__)

if __name__ == "__main__":
    # Get available prompt names for the help message
    available_prompts = ", ".join([
        "standard", "detailed", "precise", "precise_v2", "precise_v3", "precise_v4",
        "precise_v2_1", "precise_v2_2", "precise_v2_qwen", "precise_v3_qwen", "precise_v4_qwen", "precise_v3_phi-4_v2",
        "precise_v5", "precise_v5_qwen"
    ])

    # Get available relevance filter prompts
    available_relevance_prompts = ", ".join([
        "relevance_filter_v1", "relevance_filter_v2",
    ])

    # Parse command-line arguments
    parser = argparse.ArgumentParser(description="Run RAG system for insurance policy analysis")
    parser.add_argument("--model", choices=["openai", "hf", "qwen", "openrouter"], default="hf",
                        help="Model provider (openai, hf, qwen, or openrouter)")
    parser.add_argument("--model-name", default="microsoft/phi-4",
                        help="Name of the model to use. For OpenRouter, use format like 'qwen/qwen-2.5-72b-instruct'")
    parser.add_argument("--batch", action="store_true", help="Process all policies in a single batch")
    parser.add_argument("--num-questions", type=int, default=None,
                        help="Number of questions to process (default: all available questions)")
    parser.add_argument("--output-dir", default=None,
                        help="Directory to save JSON output files (default: resources/results/json_output)")
    parser.add_argument("--prompt", default="standard",
                        help=f"Prompt template to use. Available: {available_prompts}")
    parser.add_argument("--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        default="INFO", help="Set the logging level")
    parser.add_argument("--persona", action="store_true", help="Use persona extraction for queries")
    parser.add_argument("--questions", type=str,
                        help="Comma-separated list of question IDs to process (e.g., '1,2,3,4')")
    parser.add_argument("--policy-id", type=str,
                        help="Process only a specific policy ID (e.g., '20')")
    parser.add_argument("--k", type=int, default=3,
                        help="Number of context chunks to retrieve from vector store (default: 3)")
    parser.add_argument("--filter-irrelevant", action="store_true",
                        help="Filter out obviously irrelevant queries before processing")
    parser.add_argument("--prompt-relevant", default="relevance_filter_v1",
                        help=f"Relevance filter prompt to use. Available: {available_relevance_prompts}")
    parser.add_argument("--rag-strategy", choices=["simple", "section", "smart_size", "semantic",
                                                   "graph", "semantic_graph", "hybrid"], default="simple",
                        help="RAG strategy to use (simple, section, smart_size, semantic)")
    parser.add_argument("--complete-policy", action="store_true",
                        help="Pass the complete policy document instead of using RAG retrieval")
    parser.add_argument("--verifier", action="store_true",
                        help="Enable verification to review and potentially correct results")
    parser.add_argument("--verifier-iterations", type=int, default=1,
                        help="Number of verification iterations to perform (default: 1)")

    args = parser.parse_args()

    # Set up logging with the specified log level
    setup_logging({
        "logging": {
            "log_dir": LOG_DIR,
            "log_level": args.log_level
        }
    })

    logger.info(f"Starting RAG pipeline with model: {args.model}/{args.model_name}")
    logger.info(f"Using prompt template: {args.prompt}")
    logger.info(f"Log level set to: {args.log_level}")


    # After parsing args, add logging for verification:
    if args.verifier:
        logger.info(f"Verification enabled with {args.verifier_iterations} iteration(s)")

    # Add OpenRouter-specific logging
    if args.model == "openrouter":
        logger.info("Using OpenRouter API for model access")
        if not OPENROUTER_API_KEY:
            logger.error("OPENROUTER_API_KEY environment variable not set!")
            logger.error("Please set your OpenRouter API key in the .env file")
            exit(1)

    if args.num_questions:
        logger.info(f"Processing {args.num_questions} questions")
    else:
        logger.info("Processing all available questions")

    # Parse the question_ids from the arguments
    question_ids = None
    if args.questions:
        question_ids = [q.strip() for q in args.questions.split(',')]
        logger.info(f"Will process specific questions with IDs: {', '.join(question_ids)}")

    if args.policy_id:
        logger.info(f"Will only process policy with ID: {args.policy_id}")

    # Run the appropriate RAG function with all parameters
    if args.batch:
        run_batch_rag(
            model_provider=args.model,
            model_name=args.model_name,
            max_questions=args.num_questions,
            output_dir=args.output_dir,
            prompt_name=args.prompt,
            use_persona=args.persona,
            question_ids=question_ids,
            policy_id=args.policy_id,
            k=args.k,
            filter_irrelevant=args.filter_irrelevant,
            relevance_prompt_name=args.prompt_relevant,
            rag_strategy=args.rag_strategy,
            complete_policy=args.complete_policy,
            use_verifier=args.verifier,
            verifier_iterations=args.verifier_iterations,
        )
    else:
        run_rag(
            model_provider=args.model,
            model_name=args.model_name,
            max_questions=args.num_questions,
            output_dir=args.output_dir,
            prompt_name=args.prompt,
            use_persona=args.persona,
            question_ids=question_ids,
            policy_id=args.policy_id,
            k=args.k,
            filter_irrelevant=args.filter_irrelevant,
            relevance_prompt_name=args.prompt_relevant,
            rag_strategy=args.rag_strategy,
            complete_policy=args.complete_policy,
            use_verifier=args.verifier,
            verifier_iterations=args.verifier_iterations,
        )

    logger.info("RAG pipeline completed successfully")



# End of main.py
# ================================================================================

# File 6/39: models/__init__.py
# --------------------------------------------------------------------------------



# End of models/__init__.py
# ================================================================================

# File 7/39: models/base.py
# --------------------------------------------------------------------------------

# src/models/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, List

class BaseModelClient(ABC):
    @abstractmethod
    def query(self, question: str, context_files: List[str]) -> Dict[str, Any]:
        pass



# End of models/base.py
# ================================================================================

# File 8/39: models/chunking/__init__.py
# --------------------------------------------------------------------------------

# models/chunking/__init__.py
"""
Chunking strategies for insurance policy analysis.
Provides a clean interface for different chunking approaches.
"""

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata
from .factory import ChunkingFactory
from .graph_chunker import GraphChunker
from .section_chunker import SectionChunker
from .simple_chunker import SimpleChunker
from .smart_size_chunker import SmartSizeChunker
from .semantic_chunker import SemanticChunker

__all__ = [
    'ChunkingStrategy',
    'ChunkResult',
    'ChunkMetadata',
    'SimpleChunker',
    'SectionChunker',
    'SmartSizeChunker',
    'SemanticChunker',
    'GraphChunker',
    'SemanticGraphChunker',
    'ChunkingFactory',
]


# End of models/chunking/__init__.py
# ================================================================================

# File 9/39: models/chunking/base.py
# --------------------------------------------------------------------------------

# models/chunking/base.py

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class ChunkMetadata:
    """Metadata for a single chunk."""
    chunk_id: str
    policy_id: Optional[str] = None
    chunk_type: str = "unknown"
    word_count: int = 0
    has_amounts: bool = False
    has_conditions: bool = False
    has_exclusions: bool = False
    section: Optional[str] = None
    coverage_type: str = "general"
    confidence_score: float = 1.0
    extra_data: Optional[Dict[str, Any]] = None


@dataclass
class ChunkResult:
    """Result of chunking operation."""
    text: str
    metadata: ChunkMetadata


class ChunkingStrategy(ABC):
    """
    Abstract base class for all chunking strategies.

    This interface ensures all chunking approaches have consistent behavior
    and can be easily swapped in the vector store.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the chunking strategy.

        Args:
            config: Optional configuration dictionary for the strategy
        """
        self.config = config or {}
        self.name = self.__class__.__name__.lower().replace('chunker', '')
        logger.info(f"Initialized {self.name} chunking strategy")

    @abstractmethod
    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Chunk the given text using this strategy.

        Args:
            text: The text to chunk
            policy_id: Optional policy identifier for metadata
            max_length: Maximum chunk length (strategy may ignore this)

        Returns:
            List of ChunkResult objects containing text and metadata
        """
        pass

    @abstractmethod
    def get_strategy_info(self) -> Dict[str, Any]:
        """
        Get information about this chunking strategy.

        Returns:
            Dictionary with strategy metadata (name, description, config, etc.)
        """
        pass

    def validate_config(self) -> bool:
        """
        Validate the strategy configuration.

        Returns:
            True if configuration is valid, False otherwise
        """
        return True

    def get_chunk_text_list(self, chunk_results: List[ChunkResult]) -> List[str]:
        """
        Extract just the text from chunk results for backward compatibility.

        Args:
            chunk_results: List of ChunkResult objects

        Returns:
            List of chunk texts
        """
        return [result.text for result in chunk_results]

    def get_metadata_list(self, chunk_results: List[ChunkResult]) -> List[ChunkMetadata]:
        """
        Extract just the metadata from chunk results.

        Args:
            chunk_results: List of ChunkResult objects

        Returns:
            List of ChunkMetadata objects
        """
        return [result.metadata for result in chunk_results]

    def _create_chunk_id(self, policy_id: Optional[str], chunk_index: int) -> str:
        """Create a unique chunk ID."""
        prefix = f"{policy_id}_" if policy_id else ""
        return f"{prefix}{self.name}_{chunk_index}"

    def _analyze_text_properties(self, text: str) -> Dict[str, bool]:
        """Analyze text for common properties."""
        import re

        return {
            'has_amounts': bool(re.search(r'â‚¬\s*\d+|CHF\s*\d+|\d+\s*EUR|maximum.*\d+', text)),
            'has_conditions': any(word in text.lower() for word in
                                  ['if', 'when', 'unless', 'provided that', 'subject to']),
            'has_exclusions': any(word in text.lower() for word in
                                  ['not covered', 'excluded', 'exception', 'does not apply'])
        }


class ChunkingError(Exception):
    """Custom exception for chunking-related errors."""
    pass


# End of models/chunking/base.py
# ================================================================================

# File 10/39: models/chunking/factory.py
# --------------------------------------------------------------------------------

# models/chunking/factory.py

from typing import Dict, Any, Optional, Type, List
import logging

from .graph_chunker import GraphChunker
from .semantic_graph_chunker import SemanticGraphChunker
from .simple_chunker import SimpleChunker
from .hybrid_chunker import HybridChunker
from .base import ChunkingStrategy, ChunkingError

logger = logging.getLogger(__name__)


class ChunkingFactory:
    """
    Factory for creating chunking strategies.

    Supports easy registration of new strategies and configuration-based creation.
    """

    _strategies: Dict[str, Type[ChunkingStrategy]] = {}
    _default_configs: Dict[str, Dict[str, Any]] = {}

    @classmethod
    def register_strategy(cls, name: str, strategy_class: Type[ChunkingStrategy],
                          default_config: Optional[Dict[str, Any]] = None):
        """
        Register a new chunking strategy.

        Args:
            name: Strategy name (used for selection)
            strategy_class: The strategy class
            default_config: Default configuration for this strategy
        """
        cls._strategies[name] = strategy_class
        cls._default_configs[name] = default_config or {}
        logger.info(f"Registered chunking strategy: {name}")

    @classmethod
    def create_strategy(cls, strategy_name: str,
                        config: Optional[Dict[str, Any]] = None) -> ChunkingStrategy:
        """
        Create a chunking strategy instance.

        Args:
            strategy_name: Name of the strategy to create
            config: Optional configuration to override defaults

        Returns:
            Configured ChunkingStrategy instance

        Raises:
            ChunkingError: If strategy is not found or creation fails
        """
        if strategy_name not in cls._strategies:
            available = list(cls._strategies.keys())
            raise ChunkingError(
                f"Unknown chunking strategy: {strategy_name}. "
                f"Available strategies: {available}"
            )

        strategy_class = cls._strategies[strategy_name]
        default_config = cls._default_configs[strategy_name].copy()

        # Merge provided config with defaults
        if config:
            default_config.update(config)

        try:
            strategy = strategy_class(default_config)

            # Validate configuration
            if not strategy.validate_config():
                raise ChunkingError(f"Invalid configuration for strategy: {strategy_name}")

            logger.info(f"Created {strategy_name} chunking strategy")
            return strategy

        except Exception as e:
            raise ChunkingError(f"Failed to create {strategy_name} strategy: {str(e)}")

    @classmethod
    def get_available_strategies(cls) -> List[str]:
        """Get list of available strategy names."""
        return list(cls._strategies.keys())

    @classmethod
    def get_strategy_info(cls, strategy_name: str) -> Dict[str, Any]:
        """
        Get information about a specific strategy.

        Args:
            strategy_name: Name of the strategy

        Returns:
            Dictionary with strategy information
        """
        if strategy_name not in cls._strategies:
            raise ChunkingError(f"Unknown strategy: {strategy_name}")

        strategy_class = cls._strategies[strategy_name]
        default_config = cls._default_configs[strategy_name]

        # Create temporary instance to get info
        try:
            temp_instance = strategy_class(default_config)
            strategy_info = temp_instance.get_strategy_info()
            strategy_info['default_config'] = default_config
            return strategy_info
        except Exception as e:
            return {
                'name': strategy_name,
                'class': strategy_class.__name__,
                'error': f"Could not get info: {str(e)}",
                'default_config': default_config
            }

    @classmethod
    def get_all_strategies_info(cls) -> Dict[str, Dict[str, Any]]:
        """Get information about all registered strategies."""
        return {
            name: cls.get_strategy_info(name)
            for name in cls._strategies.keys()
        }


# Configuration presets for common use cases
CHUNKING_PRESETS = {
    'fast': {
        'strategy': 'simple',
        'config': {'max_length': 256, 'overlap': 25}
    },
    'balanced': {
        'strategy': 'section',
        'config': {'max_section_length': 1500, 'preserve_subsections': True}
    },
    'comprehensive': {
        'strategy': 'section',
        'config': {
            'max_section_length': 2500,
            'preserve_subsections': True,
            'include_front_matter': True,
            'sentence_window_size': 5
        }
    },
    'adaptive': {
        'strategy': 'smart_size',
        'config': {
            'base_chunk_words': 80,
            'min_chunk_words': 25,
            'max_chunk_words': 180,
            'importance_multiplier': 1.4,
            'preserve_complete_clauses': True,
            'overlap_words': 5
        }
    },
    'semantic': {
        'strategy': 'semantic',
        'config': {
            'embedding_model': 'all-MiniLM-L6-v2',
            'breakpoint_threshold_type': 'percentile',
            'breakpoint_threshold_value': 75,
            'min_chunk_sentences': 2,
            'max_chunk_sentences': 15,
            'preserve_paragraph_boundaries': True
        }
    },
    'semantic_focused': {
        'strategy': 'semantic',
        'config': {
            'embedding_model': 'all-MiniLM-L6-v2',
            'breakpoint_threshold_type': 'percentile',
            'breakpoint_threshold_value': 85,  # Higher threshold = fewer breaks
            'min_chunk_sentences': 3,
            'max_chunk_sentences': 12,
            'preserve_paragraph_boundaries': True
        }
    },
    'semantic_comprehensive': {
        'strategy': 'semantic',
        'config': {
            'embedding_model': 'all-MiniLM-L6-v2',
            'breakpoint_threshold_type': 'percentile',
            'breakpoint_threshold_value': 65,  # Lower threshold = more breaks
            'min_chunk_sentences': 2,
            'max_chunk_sentences': 20,
            'preserve_paragraph_boundaries': True
        }
    },
    'insurance_semantic': {
        'strategy': 'semantic',
        'config': {
            'embedding_model': 'all-MiniLM-L6-v2',
            'breakpoint_threshold_type': 'percentile',
            'breakpoint_threshold_value': 70,
            'min_chunk_sentences': 2,
            'max_chunk_sentences': 15,
            'preserve_paragraph_boundaries': True,
            'device': 'cpu'
        }
    },
    'graph': {
        'strategy': 'graph',
        'config': {
            'max_chunk_size': 512,
            'community_size': 50,
            'enable_hierarchical': True
        }
    },
    'graph_simple': {
        'strategy': 'graph',
        'config': {
            'max_chunk_size': 256,
            'community_size': 30,
            'enable_hierarchical': False
        }
    },
    'graph_comprehensive': {
        'strategy': 'graph',
        'config': {
            'max_chunk_size': 1024,
            'community_size': 100,
            'enable_hierarchical': True
        }
    },
    'hybrid_balanced': {
        'strategy': 'hybrid',
        'config': {
            'max_chunk_words': 200,
            'min_chunk_words': 50,
            'overlap_words': 15,
            'semantic_threshold': 0.75,
            'include_cross_references': True
        }
    },
    'hybrid_comprehensive': {
        'strategy': 'hybrid',
        'config': {
            'max_chunk_words': 300,
            'min_chunk_words': 75,
            'overlap_words': 25,
            'semantic_threshold': 0.7,
            'include_cross_references': True,
            'preserve_tables': True
        }
    }
}


# Missing functions that vector_store.py is trying to import
def create_preset_strategy(preset_name: str,
                           config_overrides: Optional[Dict[str, Any]] = None) -> ChunkingStrategy:
    """
    Create a chunking strategy from a preset configuration.

    Args:
        preset_name: Name of the preset to use
        config_overrides: Optional configuration overrides

    Returns:
        Configured ChunkingStrategy instance

    Raises:
        ChunkingError: If preset is not found or strategy creation fails
    """
    if preset_name not in CHUNKING_PRESETS:
        available_presets = list(CHUNKING_PRESETS.keys())
        raise ChunkingError(
            f"Unknown preset: {preset_name}. "
            f"Available presets: {available_presets}"
        )

    preset_config = CHUNKING_PRESETS[preset_name]
    strategy_name = preset_config['strategy']
    strategy_config = preset_config['config'].copy()

    # Apply config overrides if provided
    if config_overrides:
        strategy_config.update(config_overrides)

    # Create the strategy using the factory
    return ChunkingFactory.create_strategy(strategy_name, strategy_config)


def get_available_presets() -> List[str]:
    """Get list of available preset names."""
    return list(CHUNKING_PRESETS.keys())


def get_preset_info(preset_name: str) -> Dict[str, Any]:
    """
    Get information about a specific preset.

    Args:
        preset_name: Name of the preset

    Returns:
        Dictionary with preset information
    """
    if preset_name not in CHUNKING_PRESETS:
        raise ChunkingError(f"Unknown preset: {preset_name}")

    preset_config = CHUNKING_PRESETS[preset_name]
    return {
        'name': preset_name,
        'strategy': preset_config['strategy'],
        'config': preset_config['config'],
        'description': f"Preset configuration for {preset_config['strategy']} strategy"
    }


def get_all_presets_info() -> Dict[str, Dict[str, Any]]:
    """Get information about all available presets."""
    return {
        name: get_preset_info(name)
        for name in CHUNKING_PRESETS.keys()
    }


# Auto-registration system
def auto_register_strategies():
    """
    Automatically register all available chunking strategies.
    This function is called when the module is imported.
    """
    try:
        # Register simple chunker (always available)
        ChunkingFactory.register_strategy(
            'simple',
            SimpleChunker,
            {'max_length': 512, 'overlap': 0, 'preserve_paragraphs': True}
        )
        logger.info("Successfully registered SimpleChunker strategy")

        # Register section chunker
        try:
            from .section_chunker import SectionChunker
            ChunkingFactory.register_strategy(
                'section',
                SectionChunker,
                {
                    'max_section_length': 2000,
                    'min_section_length': 50,
                    'preserve_subsections': True,
                    'include_front_matter': False,
                    'sentence_window_size': 3
                }
            )
            logger.info("Successfully registered SectionChunker strategy")
        except ImportError:
            logger.debug("SectionChunker not available")

        # Register smart size chunker
        try:
            from .smart_size_chunker import SmartSizeChunker
            ChunkingFactory.register_strategy(
                'smart_size',
                SmartSizeChunker,
                {
                    'base_chunk_words': 80,
                    'min_chunk_words': 20,
                    'max_chunk_words': 200,
                    'importance_multiplier': 1.5,
                    'coherence_threshold': 0.7,
                    'preserve_complete_clauses': True,
                    'overlap_words': 0
                }
            )
            logger.info("Successfully registered SmartSizeChunker strategy")
        except ImportError:
            logger.debug("SmartSizeChunker not available")

        # Register semantic chunker
        try:
            from .semantic_chunker import SemanticChunker
            ChunkingFactory.register_strategy(
                'semantic',
                SemanticChunker,
                {
                    'embedding_model': 'all-MiniLM-L6-v2',
                    'breakpoint_threshold_type': 'percentile',
                    'breakpoint_threshold_value': 75,
                    'min_chunk_sentences': 2,
                    'max_chunk_sentences': 15,
                    'buffer_size': 1,
                    'preserve_paragraph_boundaries': True,
                    'device': 'cpu'
                }
            )
            logger.info("Successfully registered SemanticChunker strategy")
        except ImportError:
            logger.debug("SemanticChunker not available")

        # NEW: Register graph chunker for PankRAG
        try:
            ChunkingFactory.register_strategy(
                'graph',
                GraphChunker,
                {
                    'max_chunk_size': 512,
                    'community_size': 50,
                    'enable_hierarchical': True
                }
            )
            logger.info("Successfully registered GraphChunker strategy")
        except ImportError:
            logger.debug("GraphChunker not available")

        try:
            ChunkingFactory.register_strategy(
                'semantic_graph',
                SemanticGraphChunker,
                {
                    'max_chunk_size': 512,
                    'similarity_threshold': 0.7,
                    'min_community_size': 3,
                    'enable_hierarchical': True,
                    'embedding_model': 'all-MiniLM-L6-v2',
                    'semantic_window': 5,
                    'entity_weight': 1.5,
                    'sequential_weight': 0.8
                }
            )
            logger.info("Successfully registered SemanticGraphChunker strategy")
        except ImportError as e:
            logger.debug(f"SemanticGraphChunker not available: {e}")

        try:
            ChunkingFactory.register_strategy(
                'hybrid',
                HybridChunker,
                {
                    'max_chunk_words': 250,
                    'min_chunk_words': 50,
                    'overlap_words': 20,
                    'semantic_threshold': 0.75,
                    'embedding_model': 'all-MiniLM-L6-v2',
                    'include_cross_references': True,
                    'preserve_tables': True
                }
            )
            logger.info("Successfully registered HybridChunker strategy")
        except ImportError:
            logger.debug("HybridChunker not available")

    except Exception as e:
        logger.error(f"Error during auto-registration: {e}")


# Auto-register strategies when module is imported
auto_register_strategies()


# End of models/chunking/factory.py
# ================================================================================

# File 11/39: models/chunking/graph_chunker.py
# --------------------------------------------------------------------------------

# models/chunking/graph_chunker.py

import re
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import networkx as nx

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata

logger = logging.getLogger(__name__)


@dataclass
class Entity:
    """Represents an entity extracted from text."""
    name: str
    type: str
    context: str
    position: int


@dataclass
class Relationship:
    """Represents a relationship between entities."""
    source: str
    target: str
    relation_type: str
    context: str


class GraphChunker(ChunkingStrategy):
    """
    Graph-based chunking strategy that builds knowledge graphs from insurance policies.
    Implements PankRAG's graph construction approach.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)

        # Configuration
        self.max_chunk_size = self.config.get('max_chunk_size', 512)
        self.community_size = self.config.get('community_size', 50)
        self.enable_hierarchical = self.config.get('enable_hierarchical', True)

        # Initialize entity extraction patterns for insurance domain
        self._compile_insurance_patterns()

        # Graph storage
        self.graph = nx.Graph()
        self.communities = []
        self.community_summaries = {}

        logger.info(f"GraphChunker initialized with max_chunk_size={self.max_chunk_size}")

    def _compile_insurance_patterns(self):
        """Compile regex patterns for insurance entity extraction."""

        # Entity patterns
        self.entity_patterns = {
            'coverage_type': [
                re.compile(r'\b(medical|baggage|cancellation|delay|assistance)\s+(?:coverage|insurance)\b',
                           re.IGNORECASE),
                re.compile(r'\b(trip|travel|flight)\s+(?:cancellation|interruption|delay)\b', re.IGNORECASE),
            ],
            'monetary_amount': [
                re.compile(r'(?:â‚¬|EUR|CHF|USD)\s*\d+(?:,\d{3})*(?:\.\d{2})?', re.IGNORECASE),
                re.compile(r'\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:â‚¬|EUR|CHF|USD)', re.IGNORECASE),
            ],
            'person_type': [
                re.compile(r'\b(?:insured|policyholder|beneficiary|traveler|companion)\b', re.IGNORECASE),
                re.compile(r'\b(?:spouse|children|family member|relative)\b', re.IGNORECASE),
            ],
            'condition': [
                re.compile(r'\b(?:if|when|provided that|subject to|in case of)\b', re.IGNORECASE),
                re.compile(r'\b(?:must|shall|required|mandatory)\b', re.IGNORECASE),
            ],
            'exclusion': [
                re.compile(r'\b(?:excluded|not covered|exception|limitation)\b', re.IGNORECASE),
                re.compile(r'\b(?:does not apply|not eligible)\b', re.IGNORECASE),
            ],
            'time_period': [
                re.compile(r'\b\d+\s*(?:hours?|days?|weeks?|months?)\b', re.IGNORECASE),
                re.compile(r'\b(?:within|after|before|during)\s*\d+\s*(?:hours?|days?)\b', re.IGNORECASE),
            ]
        }

        # Relationship patterns
        self.relationship_patterns = [
            (r'(\w+)\s+(?:covers?|includes?)\s+(\w+)', 'covers'),
            (r'(\w+)\s+(?:excludes?|does not cover)\s+(\w+)', 'excludes'),
            (r'(\w+)\s+(?:requires?|needs?)\s+(\w+)', 'requires'),
            (r'(\w+)\s+(?:applies? to|for)\s+(\w+)', 'applies_to'),
            (r'(\w+)\s+(?:limited to|up to)\s+(\w+)', 'limited_to'),
        ]

    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Build a knowledge graph from text and create chunks based on communities.
        """
        logger.info(f"Starting graph-based chunking for policy {policy_id}")

        # Step 1: Extract entities and relationships
        entities = self._extract_entities(text)
        relationships = self._extract_relationships(text, entities)

        # Step 2: Build graph
        self._build_graph(entities, relationships)

        # Step 3: Detect communities using Leiden algorithm
        communities = self._detect_communities()

        # Step 4: Generate community summaries
        community_chunks = self._create_community_chunks(communities, text, policy_id)

        # Step 5: If hierarchical, create higher-level summaries
        if self.enable_hierarchical:
            hierarchical_chunks = self._create_hierarchical_chunks(communities, text, policy_id)
            community_chunks.extend(hierarchical_chunks)

        logger.info(f"Created {len(community_chunks)} graph-based chunks")
        return community_chunks

    def _extract_entities(self, text: str) -> List[Entity]:
        """Extract entities from text using domain-specific patterns."""
        entities = []

        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    entity = Entity(
                        name=match.group(0),
                        type=entity_type,
                        context=text[max(0, match.start() - 50):match.end() + 50],
                        position=match.start()
                    )
                    entities.append(entity)

        # Deduplicate entities
        unique_entities = {}
        for entity in entities:
            key = (entity.name.lower(), entity.type)
            if key not in unique_entities:
                unique_entities[key] = entity

        return list(unique_entities.values())

    def _extract_relationships(self, text: str, entities: List[Entity]) -> List[Relationship]:
        """Extract relationships between entities."""
        relationships = []
        entity_names = {e.name.lower() for e in entities}

        for pattern_text, rel_type in self.relationship_patterns:
            pattern = re.compile(pattern_text, re.IGNORECASE)
            for match in pattern.finditer(text):
                source = match.group(1)
                target = match.group(2)

                # Check if both entities exist
                if source.lower() in entity_names and target.lower() in entity_names:
                    relationship = Relationship(
                        source=source,
                        target=target,
                        relation_type=rel_type,
                        context=text[max(0, match.start() - 50):match.end() + 50]
                    )
                    relationships.append(relationship)

        return relationships

    def _build_graph(self, entities: List[Entity], relationships: List[Relationship]):
        """Build NetworkX graph from entities and relationships."""
        # Add nodes
        for entity in entities:
            self.graph.add_node(
                entity.name,
                type=entity.type,
                context=entity.context,
                position=entity.position
            )

        # Add edges
        for rel in relationships:
            self.graph.add_edge(
                rel.source,
                rel.target,
                relation_type=rel.relation_type,
                context=rel.context
            )

    def _detect_communities(self) -> List[List[str]]:
        """Detect communities in the graph using Leiden algorithm."""
        # For now, use simple connected components
        # In production, use python-igraph for Leiden algorithm
        communities = []

        for component in nx.connected_components(self.graph):
            if len(component) >= 2:  # Minimum community size
                communities.append(list(component))

        return communities

    def _create_community_chunks(self, communities: List[List[str]],
                                 text: str, policy_id: str) -> List[ChunkResult]:
        """Create chunks based on detected communities."""
        chunks = []

        for i, community in enumerate(communities):
            # Extract text related to community entities
            community_text = self._extract_community_text(community, text)

            # Create chunk metadata
            metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(policy_id, i),
                policy_id=policy_id,
                chunk_type="graph_community",
                word_count=len(community_text.split()),
                has_amounts=self._has_amounts(community_text),
                has_conditions=self._has_conditions(community_text),
                has_exclusions=self._has_exclusions(community_text),
                section=f"Community_{i}",
                coverage_type=self._infer_coverage_type_from_entities(community),
                confidence_score=0.9,
                extra_data={
                    'entities': community,
                    'entity_count': len(community),
                    'graph_density': self._calculate_density(community),
                    'central_entity': self._find_central_entity(community)
                }
            )

            chunks.append(ChunkResult(text=community_text, metadata=metadata))

        return chunks

    def _extract_community_text(self, community: List[str], text: str) -> str:
        """Extract relevant text for a community of entities."""
        # Find all occurrences of community entities
        positions = []
        for entity in community:
            pattern = re.compile(re.escape(entity), re.IGNORECASE)
            for match in pattern.finditer(text):
                positions.append((match.start(), match.end()))

        if not positions:
            return ""

        # Sort positions
        positions.sort()

        # Merge overlapping or nearby positions
        merged_positions = []
        current_start, current_end = positions[0]

        for start, end in positions[1:]:
            if start - current_end < 100:  # Within 100 characters
                current_end = max(current_end, end)
            else:
                merged_positions.append((current_start, current_end))
                current_start, current_end = start, end

        merged_positions.append((current_start, current_end))

        # Extract text with context
        text_parts = []
        for start, end in merged_positions:
            context_start = max(0, start - 50)
            context_end = min(len(text), end + 50)
            text_parts.append(text[context_start:context_end])

        return "\n....\n".join(text_parts)

    def _create_hierarchical_chunks(self, communities: List[List[str]],
                                    text: str, policy_id: str) -> List[ChunkResult]:
        """Create higher-level summary chunks for groups of communities."""
        # Group small communities into larger clusters
        # This is a simplified version - in production, use hierarchical clustering

        chunks = []
        if len(communities) > 3:
            # Create a high-level summary chunk
            all_entities = [entity for community in communities for entity in community]
            summary_text = self._create_summary(all_entities, text)

            metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(policy_id, "summary"),
                policy_id=policy_id,
                chunk_type="graph_summary",
                word_count=len(summary_text.split()),
                has_amounts=self._has_amounts(summary_text),
                has_conditions=self._has_conditions(summary_text),
                has_exclusions=self._has_exclusions(summary_text),
                section="Policy_Summary",
                coverage_type="comprehensive",
                confidence_score=0.95,
                extra_data={
                    'summary_level': 'high',
                    'communities_covered': len(communities),
                    'total_entities': len(all_entities)
                }
            )

            chunks.append(ChunkResult(text=summary_text, metadata=metadata))

        return chunks

    def _create_summary(self, entities: List[str], text: str) -> str:
        """Create a summary for a group of entities."""
        # Extract key sentences mentioning multiple entities
        sentences = text.split('.')
        relevant_sentences = []

        for sentence in sentences:
            entity_count = sum(1 for entity in entities if entity.lower() in sentence.lower())
            if entity_count >= 2:  # Sentence mentions multiple entities
                relevant_sentences.append(sentence.strip())

        # Limit to most relevant sentences
        summary = '. '.join(relevant_sentences[:10])
        if summary and not summary.endswith('.'):
            summary += '.'

        return summary

    def _calculate_density(self, community: List[str]) -> float:
        """Calculate the density of connections within a community."""
        subgraph = self.graph.subgraph(community)
        if len(community) < 2:
            return 0.0

        possible_edges = len(community) * (len(community) - 1) / 2
        actual_edges = subgraph.number_of_edges()

        return actual_edges / possible_edges if possible_edges > 0 else 0.0

    def _find_central_entity(self, community: List[str]) -> str:
        """Find the most central entity in a community."""
        subgraph = self.graph.subgraph(community)

        if len(community) == 0:
            return ""

        # Calculate degree centrality
        centrality = nx.degree_centrality(subgraph)
        if centrality:
            return max(centrality, key=centrality.get)

        return community[0]

    def _infer_coverage_type_from_entities(self, entities: List[str]) -> str:
        """Infer coverage type based on entities in the community."""
        entity_text = ' '.join(entities).lower()

        coverage_keywords = {
            'medical': ['medical', 'hospital', 'doctor', 'treatment'],
            'baggage': ['baggage', 'luggage', 'suitcase', 'belongings'],
            'cancellation': ['cancellation', 'cancel', 'trip cancellation'],
            'delay': ['delay', 'late', 'postpone', 'flight delay'],
            'assistance': ['assistance', 'help', 'support', 'emergency']
        }

        for coverage_type, keywords in coverage_keywords.items():
            if any(keyword in entity_text for keyword in keywords):
                return coverage_type

        return 'general'

    def _has_amounts(self, text: str) -> bool:
        """Check if text contains monetary amounts."""
        return any(pattern.search(text) for patterns in [self.entity_patterns['monetary_amount']]
                   for pattern in patterns)

    def _has_conditions(self, text: str) -> bool:
        """Check if text contains conditions."""
        return any(pattern.search(text) for patterns in [self.entity_patterns['condition']]
                   for pattern in patterns)

    def _has_exclusions(self, text: str) -> bool:
        """Check if text contains exclusions."""
        return any(pattern.search(text) for patterns in [self.entity_patterns['exclusion']]
                   for pattern in patterns)

    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this chunking strategy."""
        return {
            "name": "graph",
            "description": "Graph-based chunking using entity extraction and community detection (PankRAG)",
            "type": "graph-based",
            "complexity": "very high",
            "performance": "slow",
            "config": self.config,
            "features": [
                "entity_extraction",
                "relationship_extraction",
                "community_detection",
                "hierarchical_summarization",
                "graph_analysis",
                "insurance_domain_optimization"
            ],
            "best_for": [
                "complex_insurance_policies",
                "multi_hop_reasoning",
                "entity_centric_queries",
                "relationship_analysis",
                "comprehensive_coverage_analysis"
            ],
            "expected_improvement": "25-30% improvement in complex query handling"
        }


# End of models/chunking/graph_chunker.py
# ================================================================================

# File 12/39: models/chunking/hybrid_chunker.py
# --------------------------------------------------------------------------------

# models/chunking/hybrid_chunker.py

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata

logger = logging.getLogger(__name__)


@dataclass
class HybridSection:
    """Represents a section with both structural and semantic information."""
    header: str
    content: str
    section_type: str
    start_position: int
    end_position: int
    embedding: Optional[np.ndarray] = None
    subsections: List['HybridSection'] = None

    def __post_init__(self):
        if self.subsections is None:
            self.subsections = []


class HybridChunker(ChunkingStrategy):
    """
    Advanced hybrid chunking strategy that combines:
    1. Structural awareness (sections, chapters, articles)
    2. Semantic coherence (embedding-based similarity)
    3. Cross-reference detection
    4. Coverage-specific optimization

    This strategy is specifically optimized for insurance policies.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)

        # Configuration
        self.max_chunk_words = self.config.get('max_chunk_words', 250)
        self.min_chunk_words = self.config.get('min_chunk_words', 50)
        self.overlap_words = self.config.get('overlap_words', 20)
        self.semantic_threshold = self.config.get('semantic_threshold', 0.75)
        self.embedding_model_name = self.config.get('embedding_model', 'all-MiniLM-L6-v2')
        self.include_cross_references = self.config.get('include_cross_references', True)
        self.preserve_tables = self.config.get('preserve_tables', True)

        # Initialize embedding model
        try:
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            logger.info(f"Loaded embedding model: {self.embedding_model_name}")
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            self.embedding_model = None

        # Compile patterns
        self._compile_patterns()

        # Coverage type keywords for insurance domain
        self.coverage_keywords = {
            'baggage': ['baggage', 'luggage', 'bagaglio', 'belongings', 'suitcase', 'personal effects'],
            'medical': ['medical', 'hospital', 'doctor', 'mediche', 'treatment', 'illness', 'injury', 'health'],
            'cancellation': ['cancellation', 'annullamento', 'cancel', 'refund', 'rinuncia', 'trip cancellation'],
            'delay': ['delay', 'ritardo', 'late', 'postpone', 'delayed', 'postponed'],
            'assistance': ['assistance', 'assistenza', 'help', 'support', 'emergency', '24/7', 'helpline'],
            'exclusion': ['excluded', 'not covered', 'escluso', 'exception', 'esclusioni', 'limitation'],
            'general': ['general', 'definitions', 'definizioni', 'terms', 'conditions']
        }

        logger.info(f"HybridChunker initialized with max_words={self.max_chunk_words}")

    def _compile_patterns(self):
        """Compile regex patterns for structural and content detection."""

        # Main section patterns (multilingual)
        self.section_patterns = [
            # English patterns
            (r'^SECTION\s+[A-Z](?:\s*[-â€“]\s*(.*))?$', 'section'),
            (r'^CHAPTER\s+\d+(?:\s*[-â€“]\s*(.*))?$', 'chapter'),
            (r'^ARTICLE?\s*\d+(?:\.\d+)?(?:\s*[-â€“]\s*(.*))?$', 'article'),

            # Italian patterns
            (r'^SEZIONE\s+[A-Z](?:\s*[-â€“]\s*(.*))?$', 'section'),
            (r'^CAPITOLO\s+\d+(?:\s*[-â€“]\s*(.*))?$', 'chapter'),
            (r'^ARTICOLO?\s*\d+(?:\.\d+)?(?:\s*[-â€“]\s*(.*))?$', 'article'),

            # Coverage-specific patterns
            (r'^(?:What is |Che cosa Ã¨ )?(?:insured|assicurato|covered|coperto).*\??$', 'coverage'),
            (r'^(?:What is NOT |Che cosa NON Ã¨ )?(?:insured|assicurato|covered|coperto).*\??$', 'exclusion'),
            (r'^(?:Exclusions?|Esclusioni?)(?:\s*[-â€“]\s*(.*))?$', 'exclusion'),
            (r'^(?:Definitions?|Definizioni?)(?:\s*[-â€“]\s*(.*))?$', 'definition'),

            # Subsection patterns
            (r'^\s*[a-z]\)\s+(.*)$', 'subsection'),
            (r'^\s*\d+\.\s+(.*)$', 'subsection'),
            (r'^\s*[A-Z]\.\s+(.*)$', 'subsection'),
            (r'^\s*[-â€¢]\s+(.*)$', 'subsection'),
        ]

        # Compile patterns
        self.compiled_patterns = [
            (re.compile(pattern, re.IGNORECASE | re.MULTILINE), type_)
            for pattern, type_ in self.section_patterns
        ]

        # Cross-reference patterns
        self.xref_patterns = [
            re.compile(r'(?:see|vedere|cfr\.?)\s+(?:article|articolo|section|sezione)\s+(\d+(?:\.\d+)?)',
                       re.IGNORECASE),
            re.compile(r'(?:as per|secondo|come da)\s+(?:article|articolo|section|sezione)\s+(\d+(?:\.\d+)?)',
                       re.IGNORECASE),
            re.compile(r'(?:refer to|riferimento a)\s+(?:article|articolo|section|sezione)\s+(\d+(?:\.\d+)?)',
                       re.IGNORECASE),
        ]

        # Table detection patterns
        self.table_patterns = [
            re.compile(r'^\s*(?:Option|Opzione)\s+\d+.*â‚¬\s*\d+', re.IGNORECASE | re.MULTILINE),
            re.compile(r'^\s*\|.*\|.*\|', re.MULTILINE),  # Markdown tables
            re.compile(r'(?:Amount|Importo|Limit|Limite).*\n.*â‚¬\s*\d+', re.IGNORECASE),
        ]

    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Create hybrid chunks that combine structural and semantic approaches.
        """
        logger.info(f"Starting hybrid chunking for policy {policy_id}")

        # Step 1: Extract hierarchical structure
        sections = self._extract_hierarchical_sections(text)
        logger.info(f"Extracted {len(sections)} top-level sections")

        # Step 2: Generate embeddings for sections if model available
        if self.embedding_model:
            self._generate_section_embeddings(sections)

        # Step 3: Create initial chunks from sections
        chunks = []
        for section_idx, section in enumerate(sections):
            section_chunks = self._process_section(section, policy_id, section_idx)
            chunks.extend(section_chunks)

        # Step 4: Add cross-reference chunks if enabled
        if self.include_cross_references:
            xref_chunks = self._create_cross_reference_chunks(sections, chunks, policy_id)
            chunks.extend(xref_chunks)

        # Step 5: Add overlap between chunks
        if self.overlap_words > 0:
            chunks = self._add_chunk_overlap(chunks)

        logger.info(f"Created {len(chunks)} hybrid chunks")
        return chunks

    def _extract_hierarchical_sections(self, text: str) -> List[HybridSection]:
        """Extract sections with hierarchical structure."""
        lines = text.split('\n')
        sections = []
        current_section = None
        current_content = []
        current_position = 0

        for line_num, line in enumerate(lines):
            line_stripped = line.strip()

            # Skip empty lines
            if not line_stripped:
                if current_section:
                    current_content.append(line)
                current_position += len(line) + 1
                continue

            # Check if this is a section header
            section_info = self._identify_section_header(line_stripped)

            if section_info:
                # Save previous section
                if current_section:
                    current_section.content = '\n'.join(current_content)
                    current_section.end_position = current_position
                    if self._is_valid_section(current_section):
                        sections.append(current_section)

                # Start new section
                header, section_type = section_info
                current_section = HybridSection(
                    header=header,
                    content='',
                    section_type=section_type,
                    start_position=current_position,
                    end_position=current_position
                )
                current_content = []
            else:
                current_content.append(line)

            current_position += len(line) + 1

        # Don't forget the last section
        if current_section:
            current_section.content = '\n'.join(current_content)
            current_section.end_position = current_position
            if self._is_valid_section(current_section):
                sections.append(current_section)

        # Extract subsections for each section
        for section in sections:
            section.subsections = self._extract_subsections(section)

        return sections

    def _identify_section_header(self, line: str) -> Optional[Tuple[str, str]]:
        """Check if a line is a section header and return (header, type)."""
        for pattern, section_type in self.compiled_patterns:
            match = pattern.match(line)
            if match:
                return line, section_type
        return None

    def _extract_subsections(self, section: HybridSection) -> List[HybridSection]:
        """Extract subsections from a section's content."""
        subsections = []
        lines = section.content.split('\n')
        current_subsection = None
        current_content = []

        for line in lines:
            # Check for subsection patterns
            for pattern, type_ in self.compiled_patterns:
                if type_ == 'subsection':
                    match = pattern.match(line)
                    if match:
                        # Save previous subsection
                        if current_subsection:
                            current_subsection.content = '\n'.join(current_content)
                            if len(current_subsection.content.strip()) > 20:
                                subsections.append(current_subsection)

                        # Start new subsection
                        current_subsection = HybridSection(
                            header=line.strip(),
                            content='',
                            section_type='subsection',
                            start_position=0,
                            end_position=0
                        )
                        current_content = []
                        break
            else:
                if current_subsection:
                    current_content.append(line)

        # Add last subsection
        if current_subsection and current_content:
            current_subsection.content = '\n'.join(current_content)
            if len(current_subsection.content.strip()) > 20:
                subsections.append(current_subsection)

        return subsections

    def _is_valid_section(self, section: HybridSection) -> bool:
        """Check if a section has enough content to be valid."""
        word_count = len(section.content.split())
        return word_count >= 10  # Minimum 10 words

    def _generate_section_embeddings(self, sections: List[HybridSection]):
        """Generate embeddings for all sections."""
        if not self.embedding_model:
            return

        for section in sections:
            # Generate embedding for section
            section_text = f"{section.header}\n{section.content[:500]}"  # Use header + first 500 chars
            section.embedding = self.embedding_model.encode(section_text)

            # Generate embeddings for subsections
            for subsection in section.subsections:
                subsection_text = f"{subsection.header}\n{subsection.content[:300]}"
                subsection.embedding = self.embedding_model.encode(subsection_text)

    def _process_section(self, section: HybridSection, policy_id: str,
                         section_idx: int) -> List[ChunkResult]:
        """Process a section into chunks."""
        chunks = []

        # Determine if section is small enough to be a single chunk
        word_count = len(section.content.split())

        if word_count <= self.max_chunk_words:
            # Create single chunk for the section
            chunk = self._create_section_chunk(
                section, policy_id, f"{section_idx}_0",
                include_header=True
            )
            chunks.append(chunk)
        else:
            # Split section into multiple chunks
            if section.subsections:
                # Use subsections as natural boundaries
                chunks.extend(self._chunk_by_subsections(section, policy_id, section_idx))
            else:
                # Use semantic chunking within the section
                chunks.extend(self._chunk_by_semantics(section, policy_id, section_idx))

        return chunks

    def _chunk_by_subsections(self, section: HybridSection, policy_id: str,
                              section_idx: int) -> List[ChunkResult]:
        """Create chunks based on subsections."""
        chunks = []

        # Create a chunk for the section introduction (before first subsection)
        intro_chunk = self._create_intro_chunk(section, policy_id, f"{section_idx}_intro")
        if intro_chunk:
            chunks.append(intro_chunk)

        # Group subsections into chunks
        current_chunk_subsections = []
        current_word_count = 0

        for i, subsection in enumerate(section.subsections):
            subsection_words = len(subsection.content.split())

            # Check if adding this subsection would exceed max size
            if current_word_count + subsection_words > self.max_chunk_words and current_chunk_subsections:
                # Create chunk from current subsections
                chunk = self._create_subsection_chunk(
                    section, current_chunk_subsections, policy_id,
                    f"{section_idx}_{len(chunks)}"
                )
                chunks.append(chunk)

                # Start new chunk
                current_chunk_subsections = [subsection]
                current_word_count = subsection_words
            else:
                # Add to current chunk
                current_chunk_subsections.append(subsection)
                current_word_count += subsection_words

        # Create final chunk
        if current_chunk_subsections:
            chunk = self._create_subsection_chunk(
                section, current_chunk_subsections, policy_id,
                f"{section_idx}_{len(chunks)}"
            )
            chunks.append(chunk)

        return chunks

    def _chunk_by_semantics(self, section: HybridSection, policy_id: str,
                            section_idx: int) -> List[ChunkResult]:
        """Create chunks using semantic similarity."""
        chunks = []

        # Split into sentences
        sentences = self._split_into_sentences(section.content)

        if not sentences:
            return chunks

        # Generate embeddings if available
        if self.embedding_model:
            embeddings = self.embedding_model.encode(sentences)
            chunks_data = self._create_semantic_chunks(
                sentences, embeddings, section.header
            )
        else:
            # Fallback to simple sentence grouping
            chunks_data = self._create_simple_chunks(sentences, section.header)

        # Convert to ChunkResult objects
        for i, chunk_data in enumerate(chunks_data):
            chunk_text = chunk_data['text']
            chunk = self._create_chunk_result(
                text=chunk_text,
                section=section,
                policy_id=policy_id,
                chunk_id=f"{section_idx}_{i}",
                metadata_extra={
                    'semantic_score': chunk_data.get('score', 0.0),
                    'sentence_count': chunk_data.get('sentence_count', 0)
                }
            )
            chunks.append(chunk)

        return chunks

    def _create_semantic_chunks(self, sentences: List[str], embeddings: np.ndarray,
                                section_header: str) -> List[Dict[str, Any]]:
        """Create chunks based on semantic similarity."""
        chunks_data = []
        current_chunk = []
        current_embedding = None

        for i, (sentence, embedding) in enumerate(zip(sentences, embeddings)):
            if not current_chunk:
                # Start new chunk
                current_chunk = [sentence]
                current_embedding = embedding
            else:
                # Calculate similarity with current chunk
                similarity = cosine_similarity(
                    [current_embedding],
                    [embedding]
                )[0][0]

                # Check if we should add to current chunk
                current_words = sum(len(s.split()) for s in current_chunk)
                sentence_words = len(sentence.split())

                if (similarity >= self.semantic_threshold and
                        current_words + sentence_words <= self.max_chunk_words):
                    # Add to current chunk
                    current_chunk.append(sentence)
                    # Update embedding (average)
                    current_embedding = (current_embedding * len(current_chunk) + embedding) / (len(current_chunk) + 1)
                else:
                    # Save current chunk and start new one
                    chunk_text = f"{section_header}\n\n{' '.join(current_chunk)}"
                    chunks_data.append({
                        'text': chunk_text,
                        'score': similarity,
                        'sentence_count': len(current_chunk)
                    })

                    current_chunk = [sentence]
                    current_embedding = embedding

        # Add final chunk
        if current_chunk:
            chunk_text = f"{section_header}\n\n{' '.join(current_chunk)}"
            chunks_data.append({
                'text': chunk_text,
                'score': 1.0,
                'sentence_count': len(current_chunk)
            })

        return chunks_data

    def _create_simple_chunks(self, sentences: List[str], section_header: str) -> List[Dict[str, Any]]:
        """Create chunks without embeddings."""
        chunks_data = []
        current_chunk = []
        current_words = 0

        for sentence in sentences:
            sentence_words = len(sentence.split())

            if current_words + sentence_words > self.max_chunk_words and current_chunk:
                # Save current chunk
                chunk_text = f"{section_header}\n\n{' '.join(current_chunk)}"
                chunks_data.append({
                    'text': chunk_text,
                    'score': 0.0,
                    'sentence_count': len(current_chunk)
                })

                current_chunk = [sentence]
                current_words = sentence_words
            else:
                current_chunk.append(sentence)
                current_words += sentence_words

        # Add final chunk
        if current_chunk:
            chunk_text = f"{section_header}\n\n{' '.join(current_chunk)}"
            chunks_data.append({
                'text': chunk_text,
                'score': 0.0,
                'sentence_count': len(current_chunk)
            })

        return chunks_data

    def _create_cross_reference_chunks(self, sections: List[HybridSection],
                                       existing_chunks: List[ChunkResult],
                                       policy_id: str) -> List[ChunkResult]:
        """Create chunks that link related sections."""
        xref_chunks = []

        # Find sections that reference each other
        references = self._find_cross_references(sections)

        # Create chunks for important cross-references
        for ref in references:
            if self._is_important_reference(ref):
                xref_chunk = self._create_xref_chunk(ref, policy_id, len(xref_chunks))
                if xref_chunk:
                    xref_chunks.append(xref_chunk)

        # Find coverage-exclusion pairs
        coverage_exclusion_pairs = self._find_coverage_exclusion_pairs(sections)

        for pair in coverage_exclusion_pairs:
            pair_chunk = self._create_coverage_exclusion_chunk(
                pair, policy_id, len(xref_chunks)
            )
            if pair_chunk:
                xref_chunks.append(pair_chunk)

        logger.info(f"Created {len(xref_chunks)} cross-reference chunks")
        return xref_chunks

    def _find_cross_references(self, sections: List[HybridSection]) -> List[Dict[str, Any]]:
        """Find cross-references between sections."""
        references = []

        for i, section in enumerate(sections):
            # Search for references in section content
            for pattern in self.xref_patterns:
                for match in pattern.finditer(section.content):
                    ref_target = match.group(1)

                    # Find target section
                    target_section = self._find_section_by_number(sections, ref_target)
                    if target_section:
                        references.append({
                            'source': section,
                            'target': target_section,
                            'reference': match.group(0),
                            'context': section.content[max(0, match.start() - 50):match.end() + 50]
                        })

        return references

    def _find_coverage_exclusion_pairs(self, sections: List[HybridSection]) -> List[
        Tuple[HybridSection, HybridSection]]:
        """Find related coverage and exclusion sections."""
        pairs = []

        coverage_sections = [s for s in sections if s.section_type in ['coverage', 'section']
                             and self._is_coverage_section(s)]
        exclusion_sections = [s for s in sections if s.section_type == 'exclusion'
                              or 'exclusion' in s.header.lower()]

        for coverage in coverage_sections:
            # Find related exclusions
            coverage_type = self._detect_coverage_type(coverage)

            for exclusion in exclusion_sections:
                if self._sections_are_related(coverage, exclusion, coverage_type):
                    pairs.append((coverage, exclusion))

        return pairs

    def _create_xref_chunk(self, reference: Dict[str, Any], policy_id: str,
                           chunk_idx: int) -> Optional[ChunkResult]:
        """Create a chunk for a cross-reference."""
        source = reference['source']
        target = reference['target']

        # Create combined text
        combined_text = (
            f"CROSS-REFERENCE: {source.header} â†’ {target.header}\n\n"
            f"FROM {source.header}:\n"
            f"{reference['context']}\n\n"
            f"REFERENCED {target.header}:\n"
            f"{target.content[:300]}..."
        )

        # Create metadata
        metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(policy_id, f"xref_{chunk_idx}"),
            policy_id=policy_id,
            chunk_type="cross_reference",
            word_count=len(combined_text.split()),
            has_amounts=self._has_amounts(combined_text),
            has_conditions=self._has_conditions(combined_text),
            has_exclusions=self._has_exclusions(combined_text),
            section=f"CrossRef: {source.header}",
            coverage_type=self._detect_coverage_type_from_text(combined_text),
            confidence_score=0.9,
            extra_data={
                'reference_type': 'cross_reference',
                'source_section': source.header,
                'target_section': target.header,
                'is_hybrid': True
            }
        )

        return ChunkResult(text=combined_text, metadata=metadata)

    def _create_coverage_exclusion_chunk(self, pair: Tuple[HybridSection, HybridSection],
                                         policy_id: str, chunk_idx: int) -> Optional[ChunkResult]:
        """Create a chunk linking coverage with its exclusions."""
        coverage, exclusion = pair
        coverage_type = self._detect_coverage_type(coverage)

        # Extract key parts
        coverage_summary = self._extract_key_sentences(coverage.content, max_sentences=3)
        exclusion_summary = self._extract_key_sentences(exclusion.content, max_sentences=3)

        combined_text = (
            f"COVERAGE & EXCLUSIONS: {coverage_type.upper()}\n\n"
            f"WHAT IS COVERED ({coverage.header}):\n"
            f"{coverage_summary}\n\n"
            f"WHAT IS NOT COVERED ({exclusion.header}):\n"
            f"{exclusion_summary}"
        )

        # Create metadata
        metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(policy_id, f"pair_{chunk_idx}"),
            policy_id=policy_id,
            chunk_type="coverage_exclusion_pair",
            word_count=len(combined_text.split()),
            has_amounts=self._has_amounts(combined_text),
            has_conditions=self._has_conditions(combined_text),
            has_exclusions=True,  # Always true for these chunks
            section=f"Coverage-Exclusion: {coverage_type}",
            coverage_type=coverage_type,
            confidence_score=0.95,
            extra_data={
                'reference_type': 'coverage_exclusion_pair',
                'coverage_section': coverage.header,
                'exclusion_section': exclusion.header,
                'is_complete_context': True,
                'is_hybrid': True
            }
        )

        return ChunkResult(text=combined_text, metadata=metadata)

    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences."""
        # Simple sentence splitting - could be enhanced with NLTK/spaCy
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]

    def _detect_coverage_type(self, section: HybridSection) -> str:
        """Detect the type of coverage from a section."""
        text = f"{section.header} {section.content}".lower()

        scores = {}
        for coverage_type, keywords in self.coverage_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > 0:
                scores[coverage_type] = score

        if scores:
            return max(scores, key=scores.get)
        return 'general'

    def _detect_coverage_type_from_text(self, text: str) -> str:
        """Detect coverage type from text."""
        text_lower = text.lower()

        scores = {}
        for coverage_type, keywords in self.coverage_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                scores[coverage_type] = score

        if scores:
            return max(scores, key=scores.get)
        return 'general'

    def _is_coverage_section(self, section: HybridSection) -> bool:
        """Check if a section describes coverage."""
        indicators = ['covered', 'insured', 'coperto', 'assicurato', 'guarantee', 'benefit']
        text_lower = f"{section.header} {section.content[:200]}".lower()
        return any(indicator in text_lower for indicator in indicators)

    def _sections_are_related(self, section1: HybridSection, section2: HybridSection,
                              coverage_type: str = None) -> bool:
        """Check if two sections are related."""
        # Check if they mention the same coverage type
        if coverage_type:
            keywords = self.coverage_keywords.get(coverage_type, [])
            section2_text = f"{section2.header} {section2.content}".lower()
            if any(keyword in section2_text for keyword in keywords):
                return True

        # Check for explicit references
        if any(section1.header in section2.content or section2.header in section1.content
               for _ in [1]):
            return True

        # Check embedding similarity if available
        if self.embedding_model and section1.embedding is not None and section2.embedding is not None:
            similarity = cosine_similarity([section1.embedding], [section2.embedding])[0][0]
            return similarity > 0.7

        return False

    def _extract_key_sentences(self, text: str, max_sentences: int = 3) -> str:
        """Extract the most important sentences from text."""
        sentences = self._split_into_sentences(text)

        if len(sentences) <= max_sentences:
            return ' '.join(sentences)

        # Simple importance scoring based on keywords
        important_keywords = ['must', 'shall', 'covered', 'not covered', 'excluded',
                              'required', 'maximum', 'limit', 'â‚¬', 'EUR', 'CHF']

        scored_sentences = []
        for sentence in sentences:
            score = sum(1 for keyword in important_keywords if keyword in sentence.lower())
            scored_sentences.append((score, sentence))

        # Sort by score and take top sentences
        scored_sentences.sort(key=lambda x: x[0], reverse=True)
        selected = [sent for _, sent in scored_sentences[:max_sentences]]

        # Return in original order
        return ' '.join(sent for sent in sentences if sent in selected)

    def _find_section_by_number(self, sections: List[HybridSection], number: str) -> Optional[HybridSection]:
        """Find a section by its number."""
        for section in sections:
            if number in section.header:
                return section
        return None

    def _is_important_reference(self, reference: Dict[str, Any]) -> bool:
        """Check if a cross-reference is important enough to create a chunk."""
        # References to exclusions or conditions are always important
        important_keywords = ['exclusion', 'condition', 'requirement', 'limit', 'deductible']
        context_lower = reference['context'].lower()
        return any(keyword in context_lower for keyword in important_keywords)

    def _add_chunk_overlap(self, chunks: List[ChunkResult]) -> List[ChunkResult]:
        """Add overlap between consecutive chunks."""
        if len(chunks) <= 1 or self.overlap_words <= 0:
            return chunks

        overlapped_chunks = []

        for i, chunk in enumerate(chunks):
            if i == 0:
                overlapped_chunks.append(chunk)
            else:
                # Get overlap from previous chunk
                prev_words = chunks[i - 1].text.split()
                if len(prev_words) > self.overlap_words:
                    overlap_text = ' '.join(prev_words[-self.overlap_words:])

                    # Add overlap to current chunk
                    new_text = f"{overlap_text}\n[...]\n{chunk.text}"

                    # Update metadata
                    new_chunk = ChunkResult(
                        text=new_text,
                        metadata=chunk.metadata
                    )
                    new_chunk.metadata.word_count = len(new_text.split())
                    new_chunk.metadata.extra_data['has_overlap'] = True
                    new_chunk.metadata.extra_data['overlap_words'] = self.overlap_words

                    overlapped_chunks.append(new_chunk)
                else:
                    overlapped_chunks.append(chunk)

        return overlapped_chunks

    def _create_section_chunk(self, section: HybridSection, policy_id: str,
                              chunk_id: str, include_header: bool = True) -> ChunkResult:
        """Create a chunk from a section."""
        if include_header:
            text = f"{section.header}\n\n{section.content}"
        else:
            text = section.content

        return self._create_chunk_result(
            text=text,
            section=section,
            policy_id=policy_id,
            chunk_id=chunk_id
        )

    def _create_intro_chunk(self, section: HybridSection, policy_id: str,
                            chunk_id: str) -> Optional[ChunkResult]:
        """Create a chunk for section introduction (before subsections)."""
        # Find content before first subsection
        if not section.subsections:
            return None

        first_subsection_pos = section.content.find(section.subsections[0].header)
        if first_subsection_pos <= 0:
            return None

        intro_text = section.content[:first_subsection_pos].strip()
        if len(intro_text.split()) < 20:  # Too short
            return None

        text = f"{section.header}\n\n{intro_text}"

        return self._create_chunk_result(
            text=text,
            section=section,
            policy_id=policy_id,
            chunk_id=chunk_id,
            metadata_extra={'chunk_part': 'introduction'}
        )

    def _create_subsection_chunk(self, section: HybridSection,
                                 subsections: List[HybridSection],
                                 policy_id: str, chunk_id: str) -> ChunkResult:
        """Create a chunk from subsections."""
        # Combine subsection texts
        subsection_texts = []
        for subsection in subsections:
            subsection_texts.append(f"{subsection.header}\n{subsection.content}")

        text = f"{section.header}\n\n" + "\n\n".join(subsection_texts)

        return self._create_chunk_result(
            text=text,
            section=section,
            policy_id=policy_id,
            chunk_id=chunk_id,
            metadata_extra={
                'subsection_count': len(subsections),
                'subsection_headers': [s.header for s in subsections]
            }
        )

    def _create_chunk_result(self, text: str, section: HybridSection,
                             policy_id: str, chunk_id: str,
                             metadata_extra: Dict[str, Any] = None) -> ChunkResult:
        """Create a ChunkResult with metadata."""
        # Analyze text properties
        has_amounts = self._has_amounts(text)
        has_conditions = self._has_conditions(text)
        has_exclusions = self._has_exclusions(text)

        # Create base metadata
        metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(policy_id, chunk_id),
            policy_id=policy_id,
            chunk_type=f"hybrid_{section.section_type}",
            word_count=len(text.split()),
            has_amounts=has_amounts,
            has_conditions=has_conditions,
            has_exclusions=has_exclusions,
            section=section.header,
            coverage_type=self._detect_coverage_type(section),
            confidence_score=self._calculate_confidence_score(section, text),
            extra_data={
                'section_type': section.section_type,
                'chunking_method': 'hybrid',
                'has_embedding': section.embedding is not None,
                'is_hybrid': True
            }
        )

        # Add extra metadata if provided
        if metadata_extra:
            metadata.extra_data.update(metadata_extra)

        return ChunkResult(text=text, metadata=metadata)

    def _has_amounts(self, text: str) -> bool:
        """Check if text contains monetary amounts."""
        amount_patterns = [
            r'â‚¬\s*\d+',
            r'EUR\s*\d+',
            r'CHF\s*\d+',
            r'\d+\s*(?:â‚¬|EUR|CHF)',
            r'(?:maximum|limit|up to).*\d+',
        ]
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in amount_patterns)

    def _has_conditions(self, text: str) -> bool:
        """Check if text contains conditions."""
        condition_keywords = [
            'if', 'when', 'provided that', 'subject to', 'must', 'shall',
            'required', 'only if', 'unless', 'condition', 'prerequisite'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in condition_keywords)

    def _has_exclusions(self, text: str) -> bool:
        """Check if text contains exclusions."""
        exclusion_keywords = [
            'not covered', 'excluded', 'exception', 'does not apply',
            'limitation', 'restriction', 'not eligible', 'exclusion'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in exclusion_keywords)

    def _calculate_confidence_score(self, section: HybridSection, text: str) -> float:
        """Calculate confidence score for the chunk."""
        score = 1.0

        # Higher confidence for well-structured sections
        if section.section_type in ['section', 'chapter', 'article']:
            score += 0.1

        # Higher confidence for coverage-specific sections
        if section.section_type in ['coverage', 'exclusion']:
            score += 0.15

        # Lower confidence for very short sections
        if len(text.split()) < 50:
            score -= 0.2

        # Higher confidence if section has subsections (well-structured)
        if section.subsections:
            score += 0.1

        # Higher confidence if embedding is available
        if section.embedding is not None:
            score += 0.05

        return max(0.0, min(1.0, score))

    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this chunking strategy."""
        return {
            "name": "hybrid",
            "description": "Advanced hybrid chunking combining structural and semantic approaches",
            "type": "hybrid",
            "complexity": "very high",
            "performance": "medium",
            "config": self.config,
            "features": [
                "hierarchical_section_extraction",
                "semantic_coherence_analysis",
                "cross_reference_detection",
                "coverage_exclusion_pairing",
                "multilingual_support",
                "table_preservation",
                "intelligent_overlap",
                "embedding_based_similarity"
            ],
            "supported_structures": [
                "sections_chapters_articles",
                "coverage_exclusion_pairs",
                "subsections_and_lists",
                "cross_references",
                "tables_and_amounts"
            ],
            "optimization_for": [
                "insurance_policies",
                "legal_documents",
                "structured_contracts",
                "multilingual_content"
            ],
            "expected_benefits": [
                "better_context_preservation",
                "improved_retrieval_accuracy",
                "reduced_false_negatives",
                "comprehensive_coverage_understanding"
            ]
        }

    def validate_config(self) -> bool:
        """Validate configuration."""
        if self.max_chunk_words <= 0:
            logger.error("max_chunk_words must be positive")
            return False

        if self.min_chunk_words <= 0:
            logger.error("min_chunk_words must be positive")
            return False

        if self.min_chunk_words >= self.max_chunk_words:
            logger.error("min_chunk_words must be less than max_chunk_words")
            return False

        if self.overlap_words < 0:
            logger.error("overlap_words cannot be negative")
            return False

        if self.semantic_threshold < 0 or self.semantic_threshold > 1:
            logger.error("semantic_threshold must be between 0 and 1")
            return False

        return True


# End of models/chunking/hybrid_chunker.py
# ================================================================================

# File 13/39: models/chunking/section_chunker.py
# --------------------------------------------------------------------------------

# models/chunking/section_chunker.py

import re
from typing import List, Dict, Any, Optional, Tuple
import logging

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata

logger = logging.getLogger(__name__)


class SectionChunker(ChunkingStrategy):
    """
    Section/Chapter-based chunking strategy for insurance policies.

    Recognizes structural boundaries like "SECTION A", "CHAPTER 1", etc.
    and creates chunks that preserve full legal context while filtering
    out irrelevant front matter.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize section chunker.

        Config options:
        - max_section_length: Maximum length for a section chunk (default: 2000)
        - min_section_length: Minimum length to consider valid section (default: 50)
        - preserve_subsections: Whether to keep subsections together (default: True)
        - include_front_matter: Whether to include document front matter (default: False)
        - sentence_window_size: Size for sentence-based fallback chunking (default: 3)
        """
        super().__init__(config)

        self.max_section_length = self.config.get('max_section_length', 2000)
        self.min_section_length = self.config.get('min_section_length', 50)
        self.preserve_subsections = self.config.get('preserve_subsections', True)
        self.include_front_matter = self.config.get('include_front_matter', False)
        self.sentence_window_size = self.config.get('sentence_window_size', 3)

        # Compile regex patterns for section detection
        self._compile_section_patterns()

        logger.info(f"SectionChunker configured: max_length={self.max_section_length}, "
                    f"preserve_subsections={self.preserve_subsections}")

    def _compile_section_patterns(self):
        """Compile regex patterns for different section types."""

        # Main section patterns (English)
        self.section_patterns = [
            # SECTION A, SECTION B, etc.
            r'^SECTION\s+[A-Z](?:\s*[-â€“]\s*.*)?$',
            # CHAPTER 1, CHAPTER 2, etc.
            r'^CHAPTER\s+\d+(?:\s*[-â€“]\s*.*)?$',
            # Article patterns
            r'^ART\.?\s*\d+(?:\.\d+)?(?:\s*[-â€“]\s*.*)?$',
            r'^ARTICLE\s+\d+(?:\.\d+)?(?:\s*[-â€“]\s*.*)?$',
        ]

        # Italian patterns
        self.italian_patterns = [
            # SEZIONE A, SEZIONE B
            r'^SEZIONE\s+[A-Z](?:\s*[-â€“]\s*.*)?$',
            # CAPITOLO 1, CAPITOLO 2
            r'^CAPITOLO\s+\d+(?:\s*[-â€“]\s*.*)?$',
            # Italian article patterns
            r'^ART\.?\s*\d+(?:\.\d+)?(?:\s*[-â€“]\s*.*)?$',
            r'^ARTICOLO\s+\d+(?:\.\d+)?(?:\s*[-â€“]\s*.*)?$',
        ]

        # Coverage-specific patterns (both languages)
        self.coverage_patterns = [
            # What is insured sections
            r'^(?:Che cosa Ã¨ assicurato\?|What is insured\?).*$',
            r'^(?:Che cosa NON Ã¨ assicurato\?|What is NOT insured\?).*$',

            # Common coverage types
            r'^(?:Assistenza in Viaggio|Travel Assistance).*$',
            r'^(?:Spese mediche|Medical Expenses).*$',
            r'^(?:Bagaglio|Baggage).*$',
            r'^(?:Annullamento|Cancellation).*$',
            r'^(?:Interruzione|Interruption).*$',
            r'^(?:Ritardo|Delay).*$',

            # Exclusions and limitations
            r'^(?:Esclusioni|Exclusions).*$',
            r'^(?:Limitazioni|Limitations).*$',
            r'^(?:Ci sono limiti|Are there limits).*$',
        ]

        # Compile all patterns
        self.compiled_patterns = []
        for pattern_list in [self.section_patterns, self.italian_patterns, self.coverage_patterns]:
            self.compiled_patterns.extend([re.compile(pattern, re.IGNORECASE | re.MULTILINE)
                                           for pattern in pattern_list])

        # Pattern for identifying front matter to skip
        self.front_matter_patterns = [
            re.compile(r'^(?:INDEX|INDICE)$', re.IGNORECASE),
            re.compile(r'^(?:The Information Set|Il presente documento)', re.IGNORECASE),
            re.compile(r'^(?:Before signing|Prima della sottoscrizione)', re.IGNORECASE),
            re.compile(r'^(?:Last update|Ultimo aggiornamento)', re.IGNORECASE),
        ]

    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Chunk text using section/chapter boundaries.
        """
        logger.info(f"Starting section-based chunking for policy {policy_id}")

        # Split text into lines for processing
        lines = text.split('\n')

        # Find section boundaries
        sections = self._identify_sections(lines)

        # Create chunks from sections
        chunk_results = []
        for i, section in enumerate(sections):
            section_chunks = self._process_section(section, policy_id, i)
            chunk_results.extend(section_chunks)

        logger.info(f"Created {len(chunk_results)} chunks from {len(sections)} sections")
        return chunk_results

    def _identify_sections(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Identify section boundaries in the text."""
        sections = []
        current_section = None
        section_content = []
        front_matter_ended = not self.include_front_matter

        for line_num, line in enumerate(lines):
            line_stripped = line.strip()

            # Skip empty lines
            if not line_stripped:
                if current_section:
                    section_content.append(line)
                continue

            # Check if this is a section header
            is_section_header, section_type, section_title = self._is_section_header(line_stripped)

            # Check if we're still in front matter
            if not front_matter_ended:
                if is_section_header or self._is_main_content_start(line_stripped):
                    front_matter_ended = True
                else:
                    continue  # Skip front matter

            if is_section_header:
                # Save previous section if it exists
                if current_section and section_content:
                    current_section['content'] = '\n'.join(section_content)
                    if len(current_section['content'].strip()) >= self.min_section_length:
                        sections.append(current_section)

                # Start new section
                current_section = {
                    'title': section_title,
                    'type': section_type,
                    'start_line': line_num,
                    'header': line_stripped,
                    'content': ''
                }
                section_content = []
            else:
                # Add to current section content
                if current_section:
                    section_content.append(line)
                elif front_matter_ended:
                    # Create a default section for content without clear headers
                    current_section = {
                        'title': 'General Content',
                        'type': 'general',
                        'start_line': line_num,
                        'header': 'General Content',
                        'content': ''
                    }
                    section_content = [line]

        # Don't forget the last section
        if current_section and section_content:
            current_section['content'] = '\n'.join(section_content)
            if len(current_section['content'].strip()) >= self.min_section_length:
                sections.append(current_section)

        return sections

    def _is_section_header(self, line: str) -> Tuple[bool, str, str]:
        """
        Check if a line is a section header.
        Returns: (is_header, section_type, clean_title)
        """
        for pattern in self.compiled_patterns:
            if pattern.match(line):
                # Determine section type based on pattern content
                line_lower = line.lower()

                if any(word in line_lower for word in ['section', 'sezione']):
                    section_type = 'section'
                elif any(word in line_lower for word in ['chapter', 'capitolo']):
                    section_type = 'chapter'
                elif any(word in line_lower for word in ['article', 'articolo', 'art']):
                    section_type = 'article'
                elif any(word in line_lower for word in ['esclusioni', 'exclusions']):
                    section_type = 'exclusions'
                elif any(word in line_lower for word in ['assicurato', 'insured', 'covered']):
                    section_type = 'coverage'
                elif any(word in line_lower for word in ['annullamento', 'cancellation']):
                    section_type = 'cancellation'
                elif any(word in line_lower for word in ['bagaglio', 'baggage']):
                    section_type = 'baggage'
                elif any(word in line_lower for word in ['mediche', 'medical']):
                    section_type = 'medical'
                elif any(word in line_lower for word in ['assistenza', 'assistance']):
                    section_type = 'assistance'
                else:
                    section_type = 'general'

                # Clean title (remove extra whitespace, normalize)
                clean_title = ' '.join(line.split())

                return True, section_type, clean_title

        return False, '', ''

    def _is_main_content_start(self, line: str) -> bool:
        """Check if line indicates start of main content (end of front matter)."""
        indicators = [
            'che cosa Ã¨ assicurato',
            'what is insured',
            'section a',
            'sezione a',
            'definitions',
            'definizioni'
        ]

        line_lower = line.lower()
        return any(indicator in line_lower for indicator in indicators)

    def _process_section(self, section: Dict[str, Any], policy_id: Optional[str],
                         section_index: int) -> List[ChunkResult]:
        """Process a single section into chunks."""

        section_text = section['content'].strip()
        section_header = section['header']
        section_type = section['type']

        # If section is small enough, return as single chunk
        if len(section_text) <= self.max_section_length:
            return [self._create_section_chunk(
                text=f"{section_header}\n\n{section_text}",
                section=section,
                policy_id=policy_id,
                chunk_index=0,
                total_chunks=1
            )]

        # Section is too large, need to split
        if self.preserve_subsections:
            # Try to split on subsection boundaries first
            subsection_chunks = self._split_by_subsections(section_text, section_header)
            if len(subsection_chunks) > 1 and all(len(chunk) <= self.max_section_length
                                                  for chunk in subsection_chunks):
                return [
                    self._create_section_chunk(
                        text=chunk_text,
                        section=section,
                        policy_id=policy_id,
                        chunk_index=i,
                        total_chunks=len(subsection_chunks)
                    )
                    for i, chunk_text in enumerate(subsection_chunks)
                ]

        # Fall back to sentence-based chunking
        sentence_chunks = self._split_by_sentences(section_text, section_header)
        return [
            self._create_section_chunk(
                text=chunk_text,
                section=section,
                policy_id=policy_id,
                chunk_index=i,
                total_chunks=len(sentence_chunks)
            )
            for i, chunk_text in enumerate(sentence_chunks)
        ]

    def _split_by_subsections(self, text: str, header: str) -> List[str]:
        """Split text by subsection markers without duplicating headers."""

        # Look for subsection patterns like "a)", "1.", bullet points, etc.
        subsection_patterns = [
            r'^[a-z]\)\s+',  # a) b) c)
            r'^\d+\.\s+',  # 1. 2. 3.
            r'^â€¢\s+',  # bullet points
            r'^-\s+',  # dashes
            r'^[A-Z]+\.\s+',  # A. B. C.
            r'^ARTICLE\s+\d+',  # ARTICLE 1.1, 1.2, etc.
            r'^ART\.?\s+\d+',  # ART. 1.1, 1.2, etc.
        ]

        lines = text.split('\n')
        chunks = []
        current_chunk_lines = []

        # First chunk gets the header
        first_chunk = True

        for line in lines:
            line_stripped = line.strip()

            # Check if this line starts a new subsection
            is_subsection = False
            for pattern in subsection_patterns:
                if re.match(pattern, line_stripped):
                    is_subsection = True
                    break

            if is_subsection and len('\n'.join(current_chunk_lines)) > 100:
                # Save current chunk
                if first_chunk:
                    # First chunk gets the header
                    chunk_text = f"{header}\n\n" + '\n'.join(current_chunk_lines)
                    first_chunk = False
                else:
                    # Subsequent chunks don't repeat the full header
                    chunk_text = '\n'.join(current_chunk_lines)

                chunks.append(chunk_text)
                current_chunk_lines = [line]
            else:
                current_chunk_lines.append(line)

        # Add final chunk
        if current_chunk_lines:
            if first_chunk:
                # If we never split, include the header
                chunk_text = f"{header}\n\n" + '\n'.join(current_chunk_lines)
            else:
                # This is a continuation chunk
                chunk_text = '\n'.join(current_chunk_lines)
            chunks.append(chunk_text)

        # If we didn't split successfully, return the original with header
        return chunks if len(chunks) > 1 else [f"{header}\n\n{text}"]

    def _split_by_sentences(self, text: str, header: str) -> List[str]:
        """Split text using sentence-based windowing without duplicating headers."""

        # Simple sentence splitting (could be enhanced with spaCy/NLTK)
        sentences = re.split(r'[.!?]+\s+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        chunks = []
        current_chunk_sentences = []
        current_length = 0
        first_chunk = True

        for sentence in sentences:
            sentence_length = len(sentence) + 1

            # Account for header length only in first chunk
            header_length = len(header) + 2 if first_chunk else 0
            total_length = current_length + sentence_length + header_length

            if total_length > self.max_section_length and len(current_chunk_sentences) > 0:
                # Save current chunk
                if first_chunk:
                    chunk_text = f"{header}\n\n" + '\n'.join(current_chunk_sentences)
                    first_chunk = False
                else:
                    chunk_text = '\n'.join(current_chunk_sentences)

                chunks.append(chunk_text)
                current_chunk_sentences = [sentence]
                current_length = sentence_length
            else:
                current_chunk_sentences.append(sentence)
                current_length += sentence_length

        # Add final chunk
        if current_chunk_sentences:
            if first_chunk:
                chunk_text = f"{header}\n\n" + '\n'.join(current_chunk_sentences)
            else:
                chunk_text = '\n'.join(current_chunk_sentences)
            chunks.append(chunk_text)

        return chunks if chunks else [f"{header}\n\n{text}"]

    def _create_section_chunk(self, text: str, section: Dict[str, Any],
                              policy_id: Optional[str], chunk_index: int,
                              total_chunks: int) -> ChunkResult:
        """Create a ChunkResult for a section."""

        # Analyze text properties
        text_properties = self._analyze_text_properties(text)

        # Create metadata
        metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(policy_id, chunk_index),
            policy_id=policy_id,
            chunk_type=f"section_{section['type']}",
            word_count=len(text.split()),
            has_amounts=text_properties['has_amounts'],
            has_conditions=text_properties['has_conditions'],
            has_exclusions=text_properties['has_exclusions'],
            section=section['title'],
            coverage_type=self._infer_coverage_type(section, text),
            confidence_score=self._calculate_confidence_score(section, text),
            extra_data={
                'section_type': section['type'],
                'section_title': section['title'],
                'chunk_index': chunk_index,
                'total_chunks': total_chunks,
                'section_header': section['header'],
                'original_section_length': len(section['content'])
            }
        )

        return ChunkResult(text=text, metadata=metadata)

    def _infer_coverage_type(self, section: Dict[str, Any], text: str) -> str:
        """Infer the type of coverage this section relates to."""

        section_title_lower = section['title'].lower()
        text_lower = text.lower()

        # Map keywords to coverage types
        coverage_mapping = {
            'medical': ['mediche', 'medical', 'spese mediche', 'health'],
            'baggage': ['bagaglio', 'baggage', 'luggage'],
            'cancellation': ['annullamento', 'cancellation', 'cancel'],
            'delay': ['ritardo', 'delay', 'flight delay'],
            'assistance': ['assistenza', 'assistance', 'help'],
            'exclusions': ['esclusioni', 'exclusions', 'not covered'],
            'definitions': ['definitions', 'definizioni', 'glossary'],
            'general_conditions': ['conditions', 'condizioni', 'terms']
        }

        # Check section title first
        for coverage_type, keywords in coverage_mapping.items():
            for keyword in keywords:
                if keyword in section_title_lower:
                    return coverage_type

        # Check text content
        for coverage_type, keywords in coverage_mapping.items():
            keyword_count = sum(1 for keyword in keywords if keyword in text_lower)
            if keyword_count >= 2:  # Multiple keyword matches
                return coverage_type

        return section['type'] if section['type'] != 'general' else 'general'

    def _calculate_confidence_score(self, section: Dict[str, Any], text: str) -> float:
        """Calculate confidence score for this chunk."""

        score = 1.0

        # Higher confidence for well-structured sections
        if section['type'] in ['section', 'chapter', 'article']:
            score += 0.1

        # Higher confidence for coverage-specific sections
        if any(keyword in section['title'].lower()
               for keyword in ['coverage', 'guarantee', 'benefit', 'assicur']):
            score += 0.1

        # Lower confidence for very short sections
        if len(text) < 100:
            score -= 0.2

        # Higher confidence for sections with clear structure
        if re.search(r'[a-z]\)\s|^\d+\.\s|^â€¢\s', text, re.MULTILINE):
            score += 0.1

        return max(0.0, min(1.0, score))

    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this chunking strategy."""
        return {
            "name": "section",
            "description": "Section/Chapter-based chunking for insurance policies with structural awareness",
            "type": "structural",
            "complexity": "medium",
            "performance": "medium",
            "config": self.config,
            "features": [
                "section_boundary_detection",
                "multi_language_support",
                "coverage_type_inference",
                "subsection_preservation",
                "front_matter_filtering",
                "sentence_fallback_chunking",
                "confidence_scoring"
            ],
            "supported_patterns": [
                "SECTION A/B/C",
                "CHAPTER 1/2/3",
                "ARTICLE patterns",
                "Coverage-specific headers",
                "Italian equivalents"
            ],
            "best_for": [
                "insurance_policies",
                "legal_documents",
                "structured_contracts",
                "multi_language_documents",
                "coverage_analysis",
                "eligibility_determination"
            ],
            "expected_improvement": "+10-15pp retrieval precision & justification IoU"
        }

    def validate_config(self) -> bool:
        """Validate configuration."""
        if self.max_section_length <= 0:
            logger.error("max_section_length must be positive")
            return False

        if self.min_section_length < 0:
            logger.error("min_section_length cannot be negative")
            return False

        if self.sentence_window_size <= 0:
            logger.error("sentence_window_size must be positive")
            return False

        if self.min_section_length >= self.max_section_length:
            logger.error("min_section_length must be less than max_section_length")
            return False

        return True


# End of models/chunking/section_chunker.py
# ================================================================================

# File 14/39: models/chunking/semantic_chunker.py
# --------------------------------------------------------------------------------

# models/chunking/semantic_chunker.py

import re
import numpy as np
from typing import List, Dict, Any, Optional
import logging
from dataclasses import dataclass
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata

logger = logging.getLogger(__name__)


@dataclass
class SemanticBreakpoint:
    """Information about a semantic breakpoint."""
    sentence_index: int
    distance: float
    threshold: float
    reason: str


class SemanticChunker(ChunkingStrategy):
    """
    Semantic chunking strategy that uses sentence embeddings to group semantically related content.

    This chunker:
    - Splits text into sentences
    - Generates embeddings for each sentence using a transformer model
    - Calculates cosine distances between adjacent sentence embeddings
    - Creates chunk boundaries when semantic distance exceeds a threshold
    - Maintains semantic coherence within chunks
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize semantic chunker.

        Config options:
        - embedding_model: Path/name of the embedding model (default: "all-MiniLM-L6-v2")
        - breakpoint_threshold_type: How to determine threshold ("percentile" or "fixed")
        - breakpoint_threshold_value: Threshold value (percentile 0-100 or fixed 0-1)
        - min_chunk_sentences: Minimum sentences per chunk (default: 2)
        - max_chunk_sentences: Maximum sentences per chunk (default: 20)
        - buffer_size: Number of sentences to look ahead for better boundaries (default: 1)
        - preserve_paragraph_boundaries: Respect paragraph breaks (default: True)
        - device: Device for embedding model ("cpu" or "cuda", default: "cpu")
        """
        super().__init__(config)

        # Configuration with insurance-focused defaults
        self.embedding_model_name = self.config.get('embedding_model', 'all-MiniLM-L6-v2')
        self.breakpoint_threshold_type = self.config.get('breakpoint_threshold_type', 'percentile')
        self.breakpoint_threshold_value = self.config.get('breakpoint_threshold_value', 75)
        self.min_chunk_sentences = self.config.get('min_chunk_sentences', 2)
        self.max_chunk_sentences = self.config.get('max_chunk_sentences', 20)
        self.buffer_size = self.config.get('buffer_size', 1)
        self.preserve_paragraph_boundaries = self.config.get('preserve_paragraph_boundaries', True)
        self.device = self.config.get('device', 'cpu')

        # Initialize embedding model
        self.embedding_model = None
        self._load_embedding_model()

        # Compile patterns for sentence analysis
        self._compile_analysis_patterns()

        logger.info(f"SemanticChunker configured: model={self.embedding_model_name}, "
                    f"threshold={self.breakpoint_threshold_type}@{self.breakpoint_threshold_value}, "
                    f"sentences=[{self.min_chunk_sentences}-{self.max_chunk_sentences}]")

    def _load_embedding_model(self):
        """Load the sentence transformer model."""
        try:
            self.embedding_model = SentenceTransformer(self.embedding_model_name, device=self.device)
            logger.info(f"Successfully loaded embedding model: {self.embedding_model_name}")
        except Exception as e:
            logger.error(f"Failed to load embedding model {self.embedding_model_name}: {e}")
            raise

    def _compile_analysis_patterns(self):
        """Compile regex patterns for insurance content analysis."""

        # Insurance-specific patterns for enhanced semantic understanding
        self.insurance_patterns = {
            'amounts': [
                re.compile(r'â‚¬\s*\d+(?:[.,]\d+)?', re.IGNORECASE),
                re.compile(r'CHF\s*\d+(?:[.,]\d+)?', re.IGNORECASE),
                re.compile(r'(?:up to|maximum|limit of|fino a)\s*â‚¬?\s*\d+', re.IGNORECASE),
            ],
            'coverage': [
                re.compile(r'\b(?:covered|insured|guarantee|benefit|indemnity)\b', re.IGNORECASE),
                re.compile(r'\b(?:assicurato|coperto|garanzia|beneficio)\b', re.IGNORECASE),
                re.compile(r'\b(?:what is (?:covered|insured))\b', re.IGNORECASE),
            ],
            'conditions': [
                re.compile(r'\b(?:if|when|unless|provided that|subject to)\b', re.IGNORECASE),
                re.compile(r'\b(?:must|shall|required|mandatory)\b', re.IGNORECASE),
                re.compile(r'\b(?:se|quando|purchÃ©|solo se)\b', re.IGNORECASE),
            ],
            'exclusions': [
                re.compile(r'\b(?:not covered|excluded|exception|does not apply)\b', re.IGNORECASE),
                re.compile(r'\b(?:limitation|restriction|not eligible)\b', re.IGNORECASE),
                re.compile(r'\b(?:non coperto|escluso|eccezione)\b', re.IGNORECASE),
            ],
            'definitions': [
                re.compile(r'\b(?:means|defined as|refers to|definition)\b', re.IGNORECASE),
                re.compile(r'\b(?:significa|definito come|si riferisce a)\b', re.IGNORECASE),
                re.compile(r'^[A-Z][A-Z\s]+:', re.MULTILINE),
            ]
        }

    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Chunk text using semantic similarity analysis.
        Note: max_length parameter is ignored as we use semantic boundaries.
        """
        logger.info(f"Starting semantic chunking for policy {policy_id}")

        # Split into sentences
        sentences = self._split_into_sentences(text)

        if len(sentences) < 2:
            # Not enough sentences for semantic analysis
            return [self._create_chunk_result(text, 0, policy_id, 1)]

        # Generate embeddings for all sentences
        embeddings = self._generate_embeddings(sentences)

        # Calculate semantic distances
        distances = self._calculate_semantic_distances(embeddings)

        # Determine breakpoints based on threshold
        breakpoints = self._find_semantic_breakpoints(distances, sentences)

        # Create chunks based on breakpoints
        chunks = self._create_semantic_chunks(sentences, breakpoints)

        # Convert to ChunkResult objects
        chunk_results = []
        for i, chunk_text in enumerate(chunks):
            chunk_results.append(self._create_chunk_result(
                chunk_text, i, policy_id, len(chunks)
            ))

        logger.info(f"Created {len(chunk_results)} semantic chunks from {len(sentences)} sentences")
        return chunk_results

    def _split_into_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences with improved handling for insurance documents.
        """
        # Handle paragraph boundaries if preserving them
        if self.preserve_paragraph_boundaries:
            paragraphs = text.split('\n\n')
            sentences = []

            for paragraph in paragraphs:
                paragraph_sentences = self._split_paragraph_into_sentences(paragraph)
                sentences.extend(paragraph_sentences)

                # Add paragraph boundary marker (will be handled in chunking)
                if paragraph_sentences:
                    sentences[-1] += "\n\n"

            return sentences
        else:
            return self._split_paragraph_into_sentences(text)

    def _split_paragraph_into_sentences(self, paragraph: str) -> List[str]:
        """Split a paragraph into sentences."""
        # Enhanced sentence splitting for insurance documents
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', paragraph)
        sentences = [s.strip() for s in sentences if s.strip()]

        # Handle common abbreviations in insurance documents
        merged_sentences = []
        i = 0
        while i < len(sentences):
            sentence = sentences[i]

            # Check for abbreviations that shouldn't split
            if i < len(sentences) - 1 and self._ends_with_abbreviation(sentence):
                sentence = sentence + ' ' + sentences[i + 1]
                i += 2
            else:
                i += 1

            merged_sentences.append(sentence)

        return merged_sentences

    def _ends_with_abbreviation(self, sentence: str) -> bool:
        """Check if sentence ends with insurance-related abbreviations."""
        insurance_abbrevs = [
            'art.', 'sec.', 'cap.', 'par.', 'e.g.', 'i.e.', 'vs.',
            'etc.', 'cf.', 'p.', 'pp.', 'vol.', 'no.', 'ltd.',
            'inc.', 'corp.', 'co.', 'llc.', 'spa.', 'srl.'
        ]
        sentence_lower = sentence.lower().strip()
        return any(sentence_lower.endswith(abbrev) for abbrev in insurance_abbrevs)

    def _generate_embeddings(self, sentences: List[str]) -> np.ndarray:
        """Generate embeddings for all sentences."""
        try:
            # Clean sentences for embedding (remove paragraph markers)
            clean_sentences = [s.replace('\n\n', ' ').strip() for s in sentences]

            # Generate embeddings
            embeddings = self.embedding_model.encode(clean_sentences, show_progress_bar=False)

            logger.debug(f"Generated embeddings for {len(sentences)} sentences")
            return embeddings

        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise

    def _calculate_semantic_distances(self, embeddings: np.ndarray) -> List[float]:
        """Calculate cosine distances between adjacent sentence embeddings."""
        distances = []

        for i in range(len(embeddings) - 1):
            # Calculate cosine similarity
            similarity = cosine_similarity(
                embeddings[i].reshape(1, -1),
                embeddings[i + 1].reshape(1, -1)
            )[0][0]

            # Convert to distance (1 - similarity)
            distance = 1 - similarity
            distances.append(distance)

        return distances

    def _find_semantic_breakpoints(self, distances: List[float],
                                   sentences: List[str]) -> List[SemanticBreakpoint]:
        """Find semantic breakpoints based on distance threshold."""

        if not distances:
            return []

        # Determine threshold
        if self.breakpoint_threshold_type == 'percentile':
            threshold = np.percentile(distances, self.breakpoint_threshold_value)
        else:  # fixed
            threshold = self.breakpoint_threshold_value

        breakpoints = []

        # Find points where distance exceeds threshold
        for i, distance in enumerate(distances):
            if distance > threshold:
                # Check if this is a valid breakpoint considering constraints
                if self._is_valid_breakpoint(i, sentences, breakpoints):
                    breakpoints.append(SemanticBreakpoint(
                        sentence_index=i + 1,  # Break after sentence i
                        distance=distance,
                        threshold=threshold,
                        reason=f"Semantic distance {distance:.3f} > threshold {threshold:.3f}"
                    ))

        # Ensure we don't have chunks that are too long
        breakpoints = self._enforce_max_chunk_length(breakpoints, sentences)

        logger.debug(f"Found {len(breakpoints)} semantic breakpoints with threshold {threshold:.3f}")
        return breakpoints

    def _is_valid_breakpoint(self, sentence_index: int, sentences: List[str],
                             existing_breakpoints: List[SemanticBreakpoint]) -> bool:
        """Check if a breakpoint is valid given constraints."""

        # Check minimum chunk size
        last_breakpoint = existing_breakpoints[-1].sentence_index if existing_breakpoints else 0
        sentences_since_last = sentence_index + 1 - last_breakpoint

        if sentences_since_last < self.min_chunk_sentences:
            return False

        # Check if respecting paragraph boundaries
        if self.preserve_paragraph_boundaries:
            # Only allow breaks at paragraph boundaries (sentences ending with \n\n)
            if not sentences[sentence_index].endswith('\n\n'):
                return False

        return True

    def _enforce_max_chunk_length(self, breakpoints: List[SemanticBreakpoint],
                                  sentences: List[str]) -> List[SemanticBreakpoint]:
        """Ensure no chunk exceeds maximum sentence count."""

        if not breakpoints:
            # No breakpoints, check if we need to force some
            if len(sentences) > self.max_chunk_sentences:
                # Force breakpoints every max_chunk_sentences
                forced_breakpoints = []
                for i in range(self.max_chunk_sentences, len(sentences), self.max_chunk_sentences):
                    forced_breakpoints.append(SemanticBreakpoint(
                        sentence_index=i,
                        distance=0.0,
                        threshold=0.0,
                        reason="Forced break: maximum chunk size exceeded"
                    ))
                return forced_breakpoints
            return breakpoints

        # Check existing breakpoints and add forced ones if needed
        enhanced_breakpoints = []
        last_breakpoint = 0

        for breakpoint in breakpoints:
            current_chunk_size = breakpoint.sentence_index - last_breakpoint

            # If chunk is too long, add intermediate breakpoints
            if current_chunk_size > self.max_chunk_sentences:
                # Add forced breakpoints
                for i in range(last_breakpoint + self.max_chunk_sentences,
                               breakpoint.sentence_index, self.max_chunk_sentences):
                    enhanced_breakpoints.append(SemanticBreakpoint(
                        sentence_index=i,
                        distance=0.0,
                        threshold=0.0,
                        reason="Forced break: maximum chunk size exceeded"
                    ))

            enhanced_breakpoints.append(breakpoint)
            last_breakpoint = breakpoint.sentence_index

        # Check final chunk
        final_chunk_size = len(sentences) - last_breakpoint
        if final_chunk_size > self.max_chunk_sentences:
            for i in range(last_breakpoint + self.max_chunk_sentences,
                           len(sentences), self.max_chunk_sentences):
                enhanced_breakpoints.append(SemanticBreakpoint(
                    sentence_index=i,
                    distance=0.0,
                    threshold=0.0,
                    reason="Forced break: maximum chunk size exceeded"
                ))

        return enhanced_breakpoints

    def _create_semantic_chunks(self, sentences: List[str],
                                breakpoints: List[SemanticBreakpoint]) -> List[str]:
        """Create chunks based on semantic breakpoints."""

        if not breakpoints:
            return [' '.join(sentences)]

        chunks = []
        start_idx = 0

        for breakpoint in breakpoints:
            # Create chunk from start_idx to breakpoint
            chunk_sentences = sentences[start_idx:breakpoint.sentence_index]
            if chunk_sentences:
                chunk_text = ' '.join(chunk_sentences)
                # Clean up paragraph markers
                chunk_text = re.sub(r'\n\n+', '\n\n', chunk_text)
                chunks.append(chunk_text)

            start_idx = breakpoint.sentence_index

        # Add final chunk
        if start_idx < len(sentences):
            final_chunk_sentences = sentences[start_idx:]
            if final_chunk_sentences:
                chunk_text = ' '.join(final_chunk_sentences)
                chunk_text = re.sub(r'\n\n+', '\n\n', chunk_text)
                chunks.append(chunk_text)

        return chunks

    def _create_chunk_result(self, text: str, chunk_index: int,
                             policy_id: Optional[str], total_chunks: int) -> ChunkResult:
        """Create a ChunkResult with semantic metadata."""

        # Analyze chunk content
        content_analysis = self._analyze_chunk_content(text)

        # Create metadata
        metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(policy_id, chunk_index),
            policy_id=policy_id,
            chunk_type="semantic",
            word_count=len(text.split()),
            has_amounts=content_analysis['has_amounts'],
            has_conditions=content_analysis['has_conditions'],
            has_exclusions=content_analysis['has_exclusions'],
            section=None,  # Could be enhanced with section detection
            coverage_type=content_analysis['coverage_type'],
            confidence_score=content_analysis['confidence_score'],
            extra_data={
                'chunk_method': 'semantic_embedding',
                'chunk_index': chunk_index,
                'total_chunks': total_chunks,
                'sentence_count': len(self._split_into_sentences(text)),
                'embedding_model': self.embedding_model_name,
                'semantic_coherence': content_analysis['semantic_coherence'],
                'insurance_content_density': content_analysis['insurance_content_density'],
                'threshold_type': self.breakpoint_threshold_type,
                'threshold_value': self.breakpoint_threshold_value
            }
        )

        return ChunkResult(text=text, metadata=metadata)

    def _analyze_chunk_content(self, text: str) -> Dict[str, Any]:
        """Analyze chunk content for insurance-specific patterns."""

        analysis = {
            'has_amounts': False,
            'has_conditions': False,
            'has_exclusions': False,
            'coverage_type': 'general',
            'confidence_score': 1.0,
            'semantic_coherence': 1.0,
            'insurance_content_density': 0.0
        }

        text_lower = text.lower()

        # Check for insurance patterns
        pattern_matches = {}
        for pattern_type, patterns in self.insurance_patterns.items():
            matches = sum(1 for pattern in patterns if pattern.search(text))
            pattern_matches[pattern_type] = matches

        # Set boolean flags
        analysis['has_amounts'] = pattern_matches['amounts'] > 0
        analysis['has_conditions'] = pattern_matches['conditions'] > 0
        analysis['has_exclusions'] = pattern_matches['exclusions'] > 0

        # Determine coverage type
        analysis['coverage_type'] = self._infer_coverage_type(text_lower, pattern_matches)

        # Calculate insurance content density
        total_matches = sum(pattern_matches.values())
        word_count = len(text.split())
        analysis['insurance_content_density'] = total_matches / max(word_count, 1) * 100

        # Estimate semantic coherence (could be enhanced with actual embedding analysis)
        analysis['semantic_coherence'] = self._estimate_semantic_coherence(text)

        # Calculate confidence score
        analysis['confidence_score'] = self._calculate_confidence_score(analysis)

        return analysis

    def _infer_coverage_type(self, text_lower: str, pattern_matches: Dict[str, int]) -> str:
        """Infer coverage type from content patterns."""

        coverage_keywords = {
            'medical': ['medical', 'mediche', 'hospital', 'doctor', 'treatment', 'spese mediche'],
            'baggage': ['baggage', 'bagaglio', 'luggage', 'suitcase', 'bagagli'],
            'cancellation': ['cancellation', 'annullamento', 'cancel', 'trip cancellation'],
            'delay': ['delay', 'ritardo', 'late', 'postpone', 'flight delay'],
            'assistance': ['assistance', 'assistenza', 'help', 'support', 'emergency'],
            'exclusions': ['exclusion', 'esclusione', 'not covered', 'except', 'excluded'],
            'definitions': ['definition', 'definizione', 'means', 'refers to', 'glossary']
        }

        # Score each coverage type
        coverage_scores = {}
        for coverage_type, keywords in coverage_keywords.items():
            score = sum(2 if keyword in text_lower else 0 for keyword in keywords)
            # Add pattern match bonus
            if coverage_type in pattern_matches:
                score += pattern_matches[coverage_type]
            coverage_scores[coverage_type] = score

        # Return highest scoring type
        if coverage_scores:
            best_type = max(coverage_scores, key=coverage_scores.get)
            if coverage_scores[best_type] > 0:
                return best_type

        return 'general'

    def _estimate_semantic_coherence(self, text: str) -> float:
        """Estimate semantic coherence of the chunk."""

        # Simple heuristic based on:
        # 1. Sentence length consistency
        # 2. Repeated key terms
        # 3. Logical flow indicators

        sentences = self._split_into_sentences(text)
        if len(sentences) < 2:
            return 1.0

        # Sentence length consistency
        sentence_lengths = [len(s.split()) for s in sentences]
        avg_length = sum(sentence_lengths) / len(sentence_lengths)
        length_variance = sum((l - avg_length) ** 2 for l in sentence_lengths) / len(sentence_lengths)
        length_consistency = 1.0 / (1.0 + length_variance / 100)

        # Key term repetition
        words = text.lower().split()
        word_counts = {}
        for word in words:
            if len(word) > 3:  # Skip short words
                word_counts[word] = word_counts.get(word, 0) + 1

        repeated_terms = sum(1 for count in word_counts.values() if count > 1)
        term_coherence = min(1.0, repeated_terms / max(len(word_counts), 1))

        # Logical flow indicators
        flow_indicators = ['therefore', 'however', 'furthermore', 'additionally', 'moreover']
        flow_score = sum(1 for indicator in flow_indicators if indicator in text.lower())
        flow_coherence = min(1.0, flow_score / max(len(sentences), 1))

        # Combined coherence score
        coherence = (length_consistency + term_coherence + flow_coherence) / 3
        return coherence

    def _calculate_confidence_score(self, analysis: Dict[str, Any]) -> float:
        """Calculate confidence score for the chunk."""

        confidence = 1.0

        # Higher confidence for insurance-specific content
        if analysis['insurance_content_density'] > 5:
            confidence += 0.1

        # Higher confidence for semantic coherence
        confidence += analysis['semantic_coherence'] * 0.1

        # Higher confidence for specific coverage types
        if analysis['coverage_type'] != 'general':
            confidence += 0.05

        return min(1.0, confidence)

    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this chunking strategy."""
        return {
            "name": "semantic",
            "description": "Semantic chunking using sentence embeddings and cosine similarity",
            "type": "semantic",
            "complexity": "high",
            "performance": "slow",
            "config": self.config,
            "features": [
                "sentence_embedding_analysis",
                "semantic_similarity_calculation",
                "adaptive_threshold_detection",
                "insurance_content_analysis",
                "paragraph_boundary_preservation",
                "configurable_breakpoint_strategies",
                "semantic_coherence_estimation"
            ],
            "embedding_model": self.embedding_model_name,
            "threshold_strategy": {
                "type": self.breakpoint_threshold_type,
                "value": self.breakpoint_threshold_value
            },
            "chunk_constraints": {
                "min_sentences": self.min_chunk_sentences,
                "max_sentences": self.max_chunk_sentences,
                "preserve_paragraphs": self.preserve_paragraph_boundaries
            },
            "best_for": [
                "insurance_policy_analysis",
                "semantic_coherence_preservation",
                "thematic_content_grouping",
                "question_answering_systems",
                "coverage_determination",
                "complex_document_understanding"
            ],
            "advantages": [
                "maintains_semantic_integrity",
                "improved_retrieval_accuracy",
                "context_aware_chunking",
                "insurance_domain_optimized",
                "flexible_threshold_strategies"
            ],
            "disadvantages": [
                "computationally_expensive",
                "requires_embedding_model",
                "slower_processing",
                "memory_intensive_for_large_documents"
            ],
            "expected_improvement": "20-25pp improvement in retrieval precision and semantic coherence"
        }

    def validate_config(self) -> bool:
        """Validate configuration."""

        if self.min_chunk_sentences <= 0:
            logger.error("min_chunk_sentences must be positive")
            return False

        if self.max_chunk_sentences <= self.min_chunk_sentences:
            logger.error("max_chunk_sentences must be greater than min_chunk_sentences")
            return False

        if self.breakpoint_threshold_type not in ['percentile', 'fixed']:
            logger.error("breakpoint_threshold_type must be 'percentile' or 'fixed'")
            return False

        if self.breakpoint_threshold_type == 'percentile':
            if not (0 <= self.breakpoint_threshold_value <= 100):
                logger.error("percentile threshold must be between 0 and 100")
                return False
        else:  # fixed
            if not (0 <= self.breakpoint_threshold_value <= 1):
                logger.error("fixed threshold must be between 0 and 1")
                return False

        if self.buffer_size < 0:
            logger.error("buffer_size cannot be negative")
            return False

        return True


# End of models/chunking/semantic_chunker.py
# ================================================================================

# File 15/39: models/chunking/semantic_graph_chunker.py
# --------------------------------------------------------------------------------

# models/chunking/semantic_graph_chunker.py

import re
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import networkx as nx
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata

logger = logging.getLogger(__name__)


@dataclass
class SemanticNode:
    """Represents a semantic node in the graph."""
    id: str
    text: str
    embedding: np.ndarray
    type: str  # 'sentence', 'entity', 'paragraph'
    position: int
    metadata: Dict[str, Any]


@dataclass
class SemanticEdge:
    """Represents a semantic edge between nodes."""
    source: str
    target: str
    weight: float  # Semantic similarity score
    relation_type: str  # 'semantic', 'sequential', 'entity_co-occurrence'


class SemanticGraphChunker(ChunkingStrategy):
    """
    Enhanced graph-based chunking that uses semantic embeddings to build
    a knowledge graph. Combines entity extraction with semantic similarity
    for more sophisticated graph construction.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)

        # Configuration
        self.max_chunk_size = self.config.get('max_chunk_size', 512)
        self.similarity_threshold = self.config.get('similarity_threshold', 0.7)
        self.min_community_size = self.config.get('min_community_size', 3)
        self.enable_hierarchical = self.config.get('enable_hierarchical', True)
        self.embedding_model_name = self.config.get('embedding_model', 'all-MiniLM-L6-v2')

        # Semantic graph specific parameters
        self.semantic_window = self.config.get('semantic_window', 5)  # How many sentences to consider for connections
        self.entity_weight = self.config.get('entity_weight', 1.5)  # Weight for entity-based connections
        self.sequential_weight = self.config.get('sequential_weight', 0.8)  # Weight for sequential connections

        # Initialize embedding model
        self.embedding_model = SentenceTransformer(self.embedding_model_name)

        # Initialize entity patterns (reuse from original)
        self._compile_insurance_patterns()

        # Graph storage
        self.graph = nx.Graph()
        self.nodes = {}  # id -> SemanticNode
        self.embeddings_cache = {}

        logger.info(f"SemanticGraphChunker initialized with model={self.embedding_model_name}")

    def _compile_insurance_patterns(self):
        """Compile regex patterns for insurance entity extraction."""
        # Entity patterns (same as original GraphChunker)
        self.entity_patterns = {
            'coverage_type': [
                re.compile(r'\b(medical|baggage|cancellation|delay|assistance)\s+(?:coverage|insurance)\b',
                           re.IGNORECASE),
                re.compile(r'\b(trip|travel|flight)\s+(?:cancellation|interruption|delay)\b', re.IGNORECASE),
            ],
            'monetary_amount': [
                re.compile(r'(?:â‚¬|EUR|CHF|USD)\s*\d+(?:,\d{3})*(?:\.\d{2})?', re.IGNORECASE),
                re.compile(r'\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:â‚¬|EUR|CHF|USD)', re.IGNORECASE),
            ],
            'condition': [
                re.compile(r'\b(?:if|when|provided that|subject to|in case of)\b', re.IGNORECASE),
            ],
            'exclusion': [
                re.compile(r'\b(?:excluded|not covered|exception|limitation)\b', re.IGNORECASE),
            ],
        }

    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Build a semantic knowledge graph from text and create chunks.
        """
        logger.info(f"Starting semantic graph-based chunking for policy {policy_id}")

        # Reset graph for new document
        self.graph.clear()
        self.nodes.clear()
        self.embeddings_cache.clear()

        # Step 1: Create semantic nodes (sentences + entities)
        nodes = self._create_semantic_nodes(text)

        # Step 2: Build semantic graph with weighted edges
        self._build_semantic_graph(nodes)

        # Step 3: Detect communities using semantic clustering
        communities = self._detect_semantic_communities()

        # Step 4: Create chunks from communities
        chunks = self._create_semantic_chunks(communities, text, policy_id)

        # Step 5: Add hierarchical summaries if enabled
        if self.enable_hierarchical:
            hierarchical_chunks = self._create_hierarchical_semantic_chunks(
                communities, chunks, policy_id
            )
            chunks.extend(hierarchical_chunks)

        logger.info(f"Created {len(chunks)} semantic graph-based chunks")
        return chunks

    def _create_semantic_nodes(self, text: str) -> List[SemanticNode]:
        """Create nodes from sentences and important entities."""
        nodes = []

        # Split into sentences
        sentences = self._split_into_sentences(text)

        # Create sentence nodes
        for i, sentence in enumerate(sentences):
            if len(sentence.strip()) < 10:  # Skip very short sentences
                continue

            # Get embedding
            embedding = self._get_embedding(sentence)

            # Extract entities in this sentence
            entities = self._extract_entities_from_text(sentence)

            node = SemanticNode(
                id=f"sent_{i}",
                text=sentence,
                embedding=embedding,
                type="sentence",
                position=i,
                metadata={
                    'entities': entities,
                    'has_amount': self._has_amounts(sentence),
                    'has_condition': self._has_conditions(sentence),
                    'has_exclusion': self._has_exclusions(sentence)
                }
            )
            nodes.append(node)
            self.nodes[node.id] = node

        # Create entity nodes for important entities
        entity_nodes = self._create_entity_nodes(text)
        nodes.extend(entity_nodes)

        return nodes

    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences."""
        # Simple sentence splitting - can be enhanced with NLTK or spaCy
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]

    def _extract_entities_from_text(self, text: str) -> List[Dict[str, str]]:
        """Extract entities from a piece of text."""
        entities = []

        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    entities.append({
                        'text': match.group(0),
                        'type': entity_type,
                        'start': match.start(),
                        'end': match.end()
                    })

        return entities

    def _create_entity_nodes(self, text: str) -> List[SemanticNode]:
        """Create nodes for important entities that appear multiple times."""
        entity_counts = {}
        entity_contexts = {}

        # Count entity occurrences
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    entity_text = match.group(0).lower()
                    entity_counts[entity_text] = entity_counts.get(entity_text, 0) + 1

                    # Collect context
                    context = text[max(0, match.start() - 100):match.end() + 100]
                    if entity_text not in entity_contexts:
                        entity_contexts[entity_text] = []
                    entity_contexts[entity_text].append(context)

        # Create nodes for frequent entities
        entity_nodes = []
        for entity_text, count in entity_counts.items():
            if count >= 2:  # Only include entities that appear multiple times
                # Combine contexts for embedding
                combined_context = ' '.join(entity_contexts[entity_text][:3])  # Use first 3 contexts
                embedding = self._get_embedding(combined_context)

                node = SemanticNode(
                    id=f"entity_{len(entity_nodes)}",
                    text=entity_text,
                    embedding=embedding,
                    type="entity",
                    position=-1,  # Entities don't have sequential position
                    metadata={
                        'occurrences': count,
                        'contexts': entity_contexts[entity_text][:3]
                    }
                )
                entity_nodes.append(node)
                self.nodes[node.id] = node

        return entity_nodes

    def _get_embedding(self, text: str) -> np.ndarray:
        """Get embedding for text, with caching."""
        if text in self.embeddings_cache:
            return self.embeddings_cache[text]

        embedding = self.embedding_model.encode(text, convert_to_numpy=True)
        self.embeddings_cache[text] = embedding
        return embedding

    def _build_semantic_graph(self, nodes: List[SemanticNode]):
        """Build graph with semantic similarity edges."""
        # Add nodes to graph
        for node in nodes:
            self.graph.add_node(
                node.id,
                text=node.text,
                type=node.type,
                position=node.position,
                metadata=node.metadata
            )

        # Create edges based on semantic similarity
        sentence_nodes = [n for n in nodes if n.type == "sentence"]
        entity_nodes = [n for n in nodes if n.type == "entity"]

        # Connect semantically similar sentences
        self._connect_similar_sentences(sentence_nodes)

        # Connect sentences that share entities
        self._connect_sentences_with_shared_entities(sentence_nodes)

        # Connect entity nodes to sentences containing them
        self._connect_entities_to_sentences(entity_nodes, sentence_nodes)

        # Add sequential connections with lower weight
        self._add_sequential_connections(sentence_nodes)

    def _connect_similar_sentences(self, sentence_nodes: List[SemanticNode]):
        """Connect sentences based on semantic similarity."""
        if len(sentence_nodes) < 2:
            return

        # Get all embeddings
        embeddings = np.array([node.embedding for node in sentence_nodes])

        # Compute pairwise similarities
        similarities = cosine_similarity(embeddings)

        # Add edges for similar sentences
        for i in range(len(sentence_nodes)):
            for j in range(i + 1, len(sentence_nodes)):
                similarity = similarities[i, j]

                # Only connect if similarity is above threshold
                if similarity > self.similarity_threshold:
                    # Consider distance penalty (sentences far apart get lower weight)
                    distance = abs(sentence_nodes[i].position - sentence_nodes[j].position)
                    distance_penalty = 1.0 / (1.0 + distance / 10.0)

                    weight = similarity * distance_penalty

                    self.graph.add_edge(
                        sentence_nodes[i].id,
                        sentence_nodes[j].id,
                        weight=weight,
                        relation_type='semantic'
                    )

    def _connect_sentences_with_shared_entities(self, sentence_nodes: List[SemanticNode]):
        """Connect sentences that mention the same entities."""
        entity_to_sentences = {}

        # Build mapping of entities to sentences
        for node in sentence_nodes:
            entities = node.metadata.get('entities', [])
            for entity in entities:
                entity_key = entity['text'].lower()
                if entity_key not in entity_to_sentences:
                    entity_to_sentences[entity_key] = []
                entity_to_sentences[entity_key].append(node.id)

        # Connect sentences that share entities
        for entity, sentence_ids in entity_to_sentences.items():
            if len(sentence_ids) >= 2:
                for i in range(len(sentence_ids)):
                    for j in range(i + 1, len(sentence_ids)):
                        # Check if edge already exists
                        if self.graph.has_edge(sentence_ids[i], sentence_ids[j]):
                            # Increase weight
                            current_weight = self.graph[sentence_ids[i]][sentence_ids[j]]['weight']
                            self.graph[sentence_ids[i]][sentence_ids[j]]['weight'] = current_weight + 0.2
                        else:
                            self.graph.add_edge(
                                sentence_ids[i],
                                sentence_ids[j],
                                weight=self.entity_weight,
                                relation_type='entity_co-occurrence'
                            )

    def _connect_entities_to_sentences(self, entity_nodes: List[SemanticNode],
                                       sentence_nodes: List[SemanticNode]):
        """Connect entity nodes to sentences that contain them."""
        for entity_node in entity_nodes:
            entity_text = entity_node.text.lower()

            for sent_node in sentence_nodes:
                if entity_text in sent_node.text.lower():
                    # Compute semantic similarity between entity context and sentence
                    similarity = cosine_similarity(
                        [entity_node.embedding],
                        [sent_node.embedding]
                    )[0, 0]

                    if similarity > 0.5:  # Lower threshold for entity connections
                        self.graph.add_edge(
                            entity_node.id,
                            sent_node.id,
                            weight=similarity * self.entity_weight,
                            relation_type='entity_mention'
                        )

    def _add_sequential_connections(self, sentence_nodes: List[SemanticNode]):
        """Add connections between sequential sentences."""
        sorted_nodes = sorted(sentence_nodes, key=lambda n: n.position)

        for i in range(len(sorted_nodes) - 1):
            if sorted_nodes[i].position + 1 == sorted_nodes[i + 1].position:
                # Check if edge already exists
                if self.graph.has_edge(sorted_nodes[i].id, sorted_nodes[i + 1].id):
                    # Increase weight slightly
                    current_weight = self.graph[sorted_nodes[i].id][sorted_nodes[i + 1].id]['weight']
                    self.graph[sorted_nodes[i].id][sorted_nodes[i + 1].id]['weight'] = max(
                        current_weight, self.sequential_weight
                    )
                else:
                    self.graph.add_edge(
                        sorted_nodes[i].id,
                        sorted_nodes[i + 1].id,
                        weight=self.sequential_weight,
                        relation_type='sequential'
                    )

    def _detect_semantic_communities(self) -> List[List[str]]:
        """Detect communities using graph clustering algorithms."""
        if len(self.graph.nodes) == 0:
            return []

        communities = []

        try:
            # Use Louvain community detection (works well for weighted graphs)
            import community as community_louvain

            # Get the partition
            partition = community_louvain.best_partition(
                self.graph,
                weight='weight',
                resolution=1.0  # Can be tuned for different granularity
            )

            # Convert partition to list of communities
            community_dict = {}
            for node_id, comm_id in partition.items():
                if comm_id not in community_dict:
                    community_dict[comm_id] = []
                community_dict[comm_id].append(node_id)

            # Filter communities by minimum size
            communities = [
                nodes for nodes in community_dict.values()
                if len(nodes) >= self.min_community_size
            ]

        except ImportError:
            logger.warning("python-louvain not installed, falling back to connected components")
            # Fallback to connected components
            for component in nx.connected_components(self.graph):
                if len(component) >= self.min_community_size:
                    communities.append(list(component))

        return communities

    def _create_semantic_chunks(self, communities: List[List[str]],
                                text: str, policy_id: str) -> List[ChunkResult]:
        """Create chunks from semantic communities."""
        chunks = []

        for i, community in enumerate(communities):
            # Get all text from community nodes
            community_texts = []
            entities = set()
            positions = []

            for node_id in community:
                node = self.nodes[node_id]

                if node.type == "sentence":
                    community_texts.append(node.text)
                    positions.append(node.position)
                    # Collect entities
                    for entity in node.metadata.get('entities', []):
                        entities.add(entity['text'])

                elif node.type == "entity":
                    entities.add(node.text)

            # Sort texts by position
            if positions:
                sorted_texts = [x for _, x in sorted(zip(positions, community_texts))]
                chunk_text = ' '.join(sorted_texts)
            else:
                chunk_text = ' '.join(community_texts)

            # Ensure chunk doesn't exceed max length
            if len(chunk_text.split()) > self.max_chunk_size:
                chunk_text = ' '.join(chunk_text.split()[:self.max_chunk_size])

            # Calculate community metrics
            subgraph = self.graph.subgraph(community)
            avg_weight = np.mean([d['weight'] for _, _, d in subgraph.edges(data=True)])

            # Create metadata
            metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(policy_id, i),
                policy_id=policy_id,
                chunk_type="semantic_graph_community",
                word_count=len(chunk_text.split()),
                has_amounts=self._has_amounts(chunk_text),
                has_conditions=self._has_conditions(chunk_text),
                has_exclusions=self._has_exclusions(chunk_text),
                section=f"Semantic_Community_{i}",
                coverage_type=self._infer_coverage_type(chunk_text),
                confidence_score=min(avg_weight, 1.0),  # Use average edge weight as confidence
                extra_data={
                    'node_count': len(community),
                    'sentence_count': len([n for n in community if n.startswith('sent_')]),
                    'entity_count': len(entities),
                    'entities': list(entities)[:10],  # Top 10 entities
                    'graph_density': nx.density(subgraph),
                    'average_similarity': avg_weight
                }
            )

            chunks.append(ChunkResult(text=chunk_text, metadata=metadata))

        return chunks

    def _create_hierarchical_semantic_chunks(self, communities: List[List[str]],
                                             base_chunks: List[ChunkResult],
                                             policy_id: str) -> List[ChunkResult]:
        """Create higher-level semantic summaries."""
        if len(communities) <= 3:
            return []

        chunks = []

        # Group similar communities based on their chunk embeddings
        community_embeddings = []
        for chunk in base_chunks:
            if chunk.metadata.chunk_type == "semantic_graph_community":
                embedding = self._get_embedding(chunk.text[:500])  # Use first 500 chars
                community_embeddings.append(embedding)

        if len(community_embeddings) < 4:
            return []

        # Cluster communities
        embeddings_array = np.array(community_embeddings)
        clustering = DBSCAN(eps=0.3, min_samples=2).fit(embeddings_array)

        # Create summary for each cluster
        cluster_labels = set(clustering.labels_)
        cluster_labels.discard(-1)  # Remove noise label

        for cluster_id in cluster_labels:
            cluster_indices = np.where(clustering.labels_ == cluster_id)[0]

            # Combine texts from clustered communities
            cluster_texts = []
            all_entities = set()

            for idx in cluster_indices:
                chunk = base_chunks[idx]
                cluster_texts.append(chunk.text)
                all_entities.update(chunk.metadata.extra_data.get('entities', []))

            # Create summary
            summary_text = self._create_semantic_summary(cluster_texts)

            metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(policy_id, f"semantic_summary_{cluster_id}"),
                policy_id=policy_id,
                chunk_type="semantic_graph_summary",
                word_count=len(summary_text.split()),
                has_amounts=self._has_amounts(summary_text),
                has_conditions=self._has_conditions(summary_text),
                has_exclusions=self._has_exclusions(summary_text),
                section=f"Semantic_Summary_{cluster_id}",
                coverage_type="comprehensive",
                confidence_score=0.95,
                extra_data={
                    'summary_level': 'high',
                    'communities_included': len(cluster_indices),
                    'total_entities': len(all_entities),
                    'key_entities': list(all_entities)[:15]
                }
            )

            chunks.append(ChunkResult(text=summary_text, metadata=metadata))

        return chunks

    def _create_semantic_summary(self, texts: List[str]) -> str:
        """Create a summary from multiple texts using semantic importance."""
        # Combine all texts
        combined_text = ' '.join(texts)
        sentences = self._split_into_sentences(combined_text)

        if len(sentences) <= 10:
            return combined_text

        # Get embeddings for all sentences
        embeddings = np.array([self._get_embedding(sent) for sent in sentences])

        # Compute centroid
        centroid = np.mean(embeddings, axis=0)

        # Find sentences closest to centroid (most representative)
        similarities = cosine_similarity([centroid], embeddings)[0]
        top_indices = np.argsort(similarities)[-10:][::-1]  # Top 10 most similar

        # Sort by original order to maintain coherence
        top_indices = sorted(top_indices)

        summary_sentences = [sentences[i] for i in top_indices]
        return ' '.join(summary_sentences)

    def _infer_coverage_type(self, text: str) -> str:
        """Infer coverage type from text content."""
        text_lower = text.lower()

        coverage_keywords = {
            'medical': ['medical', 'hospital', 'doctor', 'treatment', 'health'],
            'baggage': ['baggage', 'luggage', 'belongings', 'personal effects'],
            'cancellation': ['cancellation', 'cancel', 'trip cancellation', 'refund'],
            'delay': ['delay', 'late', 'postpone', 'missed connection'],
            'assistance': ['assistance', 'help', 'support', 'emergency', '24/7']
        }

        scores = {}
        for coverage_type, keywords in coverage_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                scores[coverage_type] = score

        if scores:
            return max(scores, key=scores.get)
        return 'general'

    def _has_amounts(self, text: str) -> bool:
        """Check if text contains monetary amounts."""
        return any(pattern.search(text) for patterns in [self.entity_patterns['monetary_amount']]
                   for pattern in patterns)

    def _has_conditions(self, text: str) -> bool:
        """Check if text contains conditions."""
        return any(pattern.search(text) for patterns in [self.entity_patterns['condition']]
                   for pattern in patterns)

    def _has_exclusions(self, text: str) -> bool:
        """Check if text contains exclusions."""
        return any(pattern.search(text) for patterns in [self.entity_patterns['exclusion']]
                   for pattern in patterns)

    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this chunking strategy."""
        return {
            "name": "semantic_graph",
            "description": "Semantic graph-based chunking using embeddings and graph clustering",
            "type": "semantic-graph-based",
            "complexity": "very high",
            "performance": "slow",
            "config": self.config,
            "features": [
                "semantic_embeddings",
                "graph_construction",
                "weighted_edges",
                "community_detection",
                "hierarchical_clustering",
                "entity_integration",
                "semantic_similarity"
            ],
            "best_for": [
                "complex_documents",
                "semantic_coherence",
                "topic_modeling",
                "multi_hop_reasoning",
                "comprehensive_analysis"
            ],
            "expected_improvement": "30-40% improvement in semantic coherence and retrieval accuracy"
        }


# End of models/chunking/semantic_graph_chunker.py
# ================================================================================

# File 16/39: models/chunking/simple_chunker.py
# --------------------------------------------------------------------------------

# models/chunking/simple_chunker.py

from typing import List, Dict, Any, Optional
import logging

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata

logger = logging.getLogger(__name__)


class SimpleChunker(ChunkingStrategy):
    """
    Simple paragraph-based chunking strategy.

    This implements your current LocalVectorStore chunking approach adapted
    to follow the ChunkingStrategy interface.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize simple chunker.

        Config options:
        - max_length: Maximum chunk length in characters (default: 512)
        - overlap: Number of characters to overlap between chunks (default: 0)
        - preserve_paragraphs: Whether to prefer paragraph boundaries (default: True)
        """
        super().__init__(config)

        # Configuration with defaults matching your LocalVectorStore
        self.max_length = self.config.get('max_length', 512)
        self.overlap = self.config.get('overlap', 0)
        self.preserve_paragraphs = self.config.get('preserve_paragraphs', True)

        logger.info(f"SimpleChunker configured: max_length={self.max_length}, "
                    f"overlap={self.overlap}, preserve_paragraphs={self.preserve_paragraphs}")

    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Chunk text using your current LocalVectorStore approach.

        This replicates your existing LocalVectorStore.chunk_text() method
        but returns ChunkResult objects instead of just strings.
        """
        # Use configured max_length unless overridden
        chunk_max_length = self.config.get('max_length', max_length)

        # Apply your exact chunking logic
        chunks = self._chunk_text_simple(text, chunk_max_length)

        # Convert to ChunkResult objects
        chunk_results = []
        for i, chunk_text in enumerate(chunks):
            # Create metadata for each chunk
            metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(policy_id, i),
                policy_id=policy_id,
                chunk_type="simple_paragraph",
                word_count=len(chunk_text.split()),
                has_amounts=self._detect_amounts(chunk_text),
                has_conditions=self._detect_conditions(chunk_text),
                has_exclusions=self._detect_exclusions(chunk_text),
                section=None,  # Simple chunker doesn't identify sections
                coverage_type="general",  # Simple chunker doesn't classify coverage
                confidence_score=1.0,
                extra_data={
                    'chunk_method': 'paragraph_based',
                    'chunk_index': i,
                    'original_length': len(chunk_text)
                }
            )

            chunk_results.append(ChunkResult(
                text=chunk_text,
                metadata=metadata
            ))

        logger.info(f"SimpleChunker created {len(chunk_results)} chunks for policy {policy_id}")
        return chunk_results

    def _chunk_text_simple(self, text: str, max_length: int) -> List[str]:
        """
        Your exact chunking logic from LocalVectorStore.chunk_text()
        """
        if self.preserve_paragraphs:
            return self._paragraph_based_chunking(text, max_length)
        else:
            return self._word_based_chunking(text, max_length)

    def _paragraph_based_chunking(self, text: str, max_length: int) -> List[str]:
        """
        Exact implementation from your LocalVectorStore.
        """
        # Simple paragraph-based chunking
        paragraphs = text.split('\n\n')
        chunks = []

        current_chunk = ""
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) < max_length:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"

        if current_chunk:
            chunks.append(current_chunk.strip())

        # Fallback to word-based chunking if no paragraphs
        if not chunks:
            chunks = self._word_based_chunking(text, max_length)

        return chunks

    def _word_based_chunking(self, text: str, max_length: int) -> List[str]:
        """
        Fallback word-based chunking from your LocalVectorStore.
        """
        words = text.split()
        chunks = []

        current_chunk_words = []
        current_length = 0

        for word in words:
            word_length = len(word) + 1  # +1 for space

            if current_length + word_length <= max_length:
                current_chunk_words.append(word)
                current_length += word_length
            else:
                if current_chunk_words:
                    chunks.append(" ".join(current_chunk_words))
                current_chunk_words = [word]
                current_length = word_length

        if current_chunk_words:
            chunks.append(" ".join(current_chunk_words))

        return chunks if chunks else [text[:max_length]]

    def _detect_amounts(self, text: str) -> bool:
        """Detect if text contains monetary amounts or numbers."""
        import re
        # Look for currency symbols, numbers with currency codes, or percentage
        amount_patterns = [
            r'â‚¬\s*\d+',  # Euro amounts
            r'CHF\s*\d+',  # Swiss Franc
            r'\d+\s*EUR',  # EUR suffix
            r'\d+\s*CHF',  # CHF suffix
            r'maximum.*\d+',  # Maximum amounts
            r'\d+%',  # Percentages
            r'\d+\.\d+',  # Decimal numbers
        ]
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in amount_patterns)

    def _detect_conditions(self, text: str) -> bool:
        """Detect if text contains conditional language."""
        condition_words = [
            'if', 'when', 'unless', 'provided that', 'subject to',
            'conditional', 'depends on', 'only if', 'in case of',
            'se', 'quando', 'solo se', 'purchÃ©'  # Italian equivalents
        ]
        text_lower = text.lower()
        return any(word in text_lower for word in condition_words)

    def _detect_exclusions(self, text: str) -> bool:
        """Detect if text contains exclusion language."""
        exclusion_words = [
            'not covered', 'excluded', 'exception', 'does not apply',
            'limitation', 'restriction', 'not eligible', 'shall not',
            'non coperto', 'escluso', 'eccezione', 'limitazione'  # Italian
        ]
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in exclusion_words)

    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this chunking strategy."""
        return {
            "name": "simple",
            "description": "Simple paragraph-based chunking adapted from LocalVectorStore",
            "type": "rule_based",
            "complexity": "low",
            "performance": "fast",
            "config": self.config,
            "features": [
                "paragraph_splitting",
                "configurable_max_length",
                "word_based_fallback",
                "basic_content_detection"
            ],
            "best_for": [
                "quick_testing",
                "baseline_comparison",
                "simple_documents",
                "fast_processing",
                "compatibility_with_existing_system"
            ],
            "matches_original": "LocalVectorStore.chunk_text()"
        }

    def validate_config(self) -> bool:
        """Validate configuration."""
        if self.max_length <= 0:
            logger.error("max_length must be positive")
            return False

        if self.overlap < 0:
            logger.error("overlap cannot be negative")
            return False

        if self.overlap >= self.max_length:
            logger.error("overlap must be less than max_length")
            return False

        return True


# End of models/chunking/simple_chunker.py
# ================================================================================

# File 17/39: models/chunking/smart_size_chunker.py
# --------------------------------------------------------------------------------

# models/chunking/smart_size_chunker.py

import re
from typing import List, Dict, Any, Optional
import logging
from dataclasses import dataclass

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata

logger = logging.getLogger(__name__)


@dataclass
class ContentSignal:
    """Signals that indicate content importance for chunk sizing."""
    has_amount: bool = False
    has_condition: bool = False
    has_exclusion: bool = False
    has_coverage_statement: bool = False
    has_definition: bool = False
    sentence_count: int = 0
    complexity_score: float = 0.0
    confidence_score: float = 1.0


class SmartSizeChunker(ChunkingStrategy):
    """
    Smart size chunking strategy that adapts chunk size based on content density.

    Key principles:
    - Important content (amounts, conditions, coverage statements) gets larger chunks
    - Simple content gets smaller chunks for efficiency
    - Semantic coherence is prioritized over fixed size limits
    - Preserves complete legal/coverage contexts
    - Uses word-based sizing for consistency with other strategies
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize smart size chunker.

        Config options:
        - base_chunk_words: Base target chunk size in words (default: 80)
        - min_chunk_words: Minimum allowed chunk size in words (default: 20)
        - max_chunk_words: Maximum allowed chunk size in words (default: 200)
        - importance_multiplier: How much to expand important content (default: 1.5)
        - coherence_threshold: Minimum coherence score to maintain (default: 0.7)
        - preserve_complete_clauses: Keep legal clauses intact (default: True)
        - overlap_words: Number of words to overlap between chunks (default: 0)
        """
        super().__init__(config)

        # Use word-based sizing for consistency with other strategies
        self.base_chunk_words = self.config.get('base_chunk_words', 80)
        self.min_chunk_words = self.config.get('min_chunk_words', 20)
        self.max_chunk_words = self.config.get('max_chunk_words', 200)
        self.importance_multiplier = self.config.get('importance_multiplier', 1.5)
        self.coherence_threshold = self.config.get('coherence_threshold', 0.7)
        self.preserve_complete_clauses = self.config.get('preserve_complete_clauses', True)
        self.overlap_words = self.config.get('overlap_words', 0)

        # Compile patterns for content analysis
        self._compile_content_patterns()

        logger.info(f"SmartSizeChunker configured: base_words={self.base_chunk_words}, "
                    f"range=[{self.min_chunk_words}, {self.max_chunk_words}], "
                    f"overlap={self.overlap_words}")

    def _compile_content_patterns(self):
        """Compile regex patterns for content analysis."""

        # Amount patterns (high importance)
        self.amount_patterns = [
            re.compile(r'â‚¬\s*\d+(?:[.,]\d+)?', re.IGNORECASE),
            re.compile(r'CHF\s*\d+(?:[.,]\d+)?', re.IGNORECASE),
            re.compile(r'\d+(?:[.,]\d+)?\s*(?:â‚¬|EUR|CHF)', re.IGNORECASE),
            re.compile(r'(?:up to|maximum|limit of|fino a)\s*â‚¬?\s*\d+', re.IGNORECASE),
            re.compile(r'option\s+\d+\s*â‚¬\s*\d+', re.IGNORECASE),
        ]

        # Condition patterns (high importance)
        self.condition_patterns = [
            re.compile(r'\b(?:if|when|unless|provided that|subject to|conditional)\b', re.IGNORECASE),
            re.compile(r'\b(?:se|quando|purchÃ©|solo se)\b', re.IGNORECASE),
            re.compile(r'\b(?:only if|in case of|depends on)\b', re.IGNORECASE),
            re.compile(r'\b(?:must|shall|required|mandatory)\b', re.IGNORECASE),
        ]

        # Exclusion patterns (high importance)
        self.exclusion_patterns = [
            re.compile(r'\b(?:not covered|excluded|exception|does not apply)\b', re.IGNORECASE),
            re.compile(r'\b(?:limitation|restriction|not eligible|shall not)\b', re.IGNORECASE),
            re.compile(r'\b(?:non coperto|escluso|eccezione|limitazione)\b', re.IGNORECASE),
            re.compile(r'\b(?:exclude|except|but not|other than)\b', re.IGNORECASE),
        ]

        # Coverage statement patterns (high importance)
        self.coverage_patterns = [
            re.compile(r'\b(?:covered|insured|guarantee|benefit|indemnity)\b', re.IGNORECASE),
            re.compile(r'\b(?:assicurato|coperto|garanzia|beneficio)\b', re.IGNORECASE),
            re.compile(r'\b(?:pays|reimburse|compensate|indemnify)\b', re.IGNORECASE),
            re.compile(r'\b(?:what is (?:covered|insured)|che cosa Ã¨ assicurato)\b', re.IGNORECASE),
        ]

        # Definition patterns (medium importance)
        self.definition_patterns = [
            re.compile(r'\b(?:means|defined as|refers to|definition)\b', re.IGNORECASE),
            re.compile(r'\b(?:significa|definito come|si riferisce a)\b', re.IGNORECASE),
            re.compile(r'^[A-Z][A-Z\s]+:', re.MULTILINE),  # ALL CAPS definitions
        ]

        # Clause boundary patterns (fixed word boundaries)
        self.clause_boundary_patterns = [
            re.compile(r'^\s*[a-z]\)\s+', re.MULTILINE),  # a) b) c)
            re.compile(r'^\s*\d+\.\s+', re.MULTILINE),  # 1. 2. 3.
            re.compile(r'^\s*[A-Z]\.\s+', re.MULTILINE),  # A. B. C.
            re.compile(r'^\s*\bArticle\s+\d+\b', re.MULTILINE | re.IGNORECASE),
            re.compile(r'^\s*\bSection\s+[A-Z]\b', re.MULTILINE | re.IGNORECASE),
        ]

    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Chunk text using smart size adaptation.
        Note: max_length parameter is ignored as we use word-based sizing.
        """
        logger.info(f"Starting smart size chunking for policy {policy_id}")

        # Split into sentences for analysis
        sentences = self._split_into_sentences(text)

        # Analyze each sentence for content signals
        sentence_signals = [self._analyze_sentence(sentence) for sentence in sentences]

        # Create chunks based on content importance and coherence
        chunks = self._create_smart_chunks(sentences, sentence_signals)

        # Add overlap if configured
        if self.overlap_words > 0:
            chunks = self._add_overlap(chunks)

        # Convert to ChunkResult objects
        chunk_results = []
        for i, chunk_text in enumerate(chunks):
            chunk_results.append(self._create_chunk_result(
                chunk_text, i, policy_id, len(chunks)
            ))

        logger.info(f"Created {len(chunk_results)} smart-sized chunks")
        return chunk_results

    def _split_into_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences with improved handling of abbreviations.
        TODO: Consider using nltk.tokenize.sent_tokenize or spaCy for better accuracy.
        """
        # Simple sentence splitting with some abbreviation handling
        # This is a basic implementation - could be enhanced with nltk/spaCy
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        # Handle common abbreviations that might cause false splits
        merged_sentences = []
        i = 0
        while i < len(sentences):
            sentence = sentences[i]

            # Check if this sentence ends with a common abbreviation
            if i < len(sentences) - 1 and self._ends_with_abbreviation(sentence):
                # Merge with next sentence
                sentence = sentence + ' ' + sentences[i + 1]
                i += 2
            else:
                i += 1

            merged_sentences.append(sentence)

        return merged_sentences

    def _ends_with_abbreviation(self, sentence: str) -> bool:
        """Check if sentence ends with common abbreviations."""
        common_abbrevs = ['e.g.', 'i.e.', 'art.', 'sec.', 'etc.', 'vs.', 'cf.']
        sentence_lower = sentence.lower().strip()
        return any(sentence_lower.endswith(abbrev) for abbrev in common_abbrevs)

    def _analyze_sentence(self, sentence: str) -> ContentSignal:
        """Analyze a sentence for content importance signals."""
        signal = ContentSignal()

        # Check for amounts
        signal.has_amount = any(pattern.search(sentence) for pattern in self.amount_patterns)

        # Check for conditions
        signal.has_condition = any(pattern.search(sentence) for pattern in self.condition_patterns)

        # Check for exclusions
        signal.has_exclusion = any(pattern.search(sentence) for pattern in self.exclusion_patterns)

        # Check for coverage statements
        signal.has_coverage_statement = any(pattern.search(sentence) for pattern in self.coverage_patterns)

        # Check for definitions
        signal.has_definition = any(pattern.search(sentence) for pattern in self.definition_patterns)

        # Count sentences (for now, just 1)
        signal.sentence_count = 1

        # Calculate complexity and confidence scores
        signal.complexity_score = self._calculate_complexity_score(sentence, signal)
        signal.confidence_score = self._calculate_confidence_score(sentence, signal)

        return signal

    def _calculate_complexity_score(self, text: str, signal: ContentSignal) -> float:
        """Calculate complexity score for text (can be sentence or chunk)."""
        score = 0.0

        # Base score from content signals
        if signal.has_amount:
            score += 0.3
        if signal.has_condition:
            score += 0.2
        if signal.has_exclusion:
            score += 0.25
        if signal.has_coverage_statement:
            score += 0.2
        if signal.has_definition:
            score += 0.15

        # Length-based complexity (normalize by sentence count for multi-sentence chunks)
        word_count = len(text.split())
        avg_words_per_sentence = word_count / max(1, signal.sentence_count)

        if avg_words_per_sentence > 30:
            score += 0.1
        elif avg_words_per_sentence < 10:
            score -= 0.1

        # Punctuation complexity (normalize by sentence count)
        complex_punct = text.count(',') + text.count('(') + text.count(';')
        punct_per_sentence = complex_punct / max(1, signal.sentence_count)

        if punct_per_sentence > 3:
            score += 0.1

        return min(1.0, max(0.0, score))

    def _calculate_confidence_score(self, text: str, signal: ContentSignal) -> float:
        """Calculate confidence score (different from complexity)."""
        confidence = 1.0

        # Higher confidence for structured content
        if signal.has_amount or signal.has_coverage_statement:
            confidence += 0.1

        # Lower confidence for very short or very long sentences
        word_count = len(text.split())
        if word_count < 5:
            confidence -= 0.2
        elif word_count > 50:
            confidence -= 0.1

        # Higher confidence for clear structure
        if any(pattern.search(text) for pattern in self.clause_boundary_patterns):
            confidence += 0.05

        return max(0.0, min(1.0, confidence))

    def _create_smart_chunks(self, sentences: List[str], signals: List[ContentSignal]) -> List[str]:
        """Create chunks with smart size adaptation."""
        chunks = []
        current_chunk_sentences = []
        current_chunk_words = 0
        current_importance = 0.0

        for i, (sentence, signal) in enumerate(zip(sentences, signals)):
            sentence_words = len(sentence.split())

            # Calculate target chunk size based on importance
            target_words = self._calculate_target_chunk_words(current_importance, signal)

            # Check if we should start a new chunk
            should_split = self._should_split_chunk(
                current_chunk_words, sentence_words, target_words,
                current_chunk_sentences, sentence, signal
            )

            if should_split and current_chunk_sentences:
                # Save current chunk
                chunks.append(' '.join(current_chunk_sentences))
                current_chunk_sentences = []
                current_chunk_words = 0
                current_importance = 0.0

            # Add sentence to current chunk
            current_chunk_sentences.append(sentence)
            current_chunk_words += sentence_words
            current_importance = max(current_importance, signal.complexity_score)

        # Add final chunk
        if current_chunk_sentences:
            chunks.append(' '.join(current_chunk_sentences))

        return chunks

    def _calculate_target_chunk_words(self, current_importance: float,
                                      new_signal: ContentSignal) -> int:
        """Calculate target chunk size in words based on content importance."""
        # Use the higher importance between current and new content
        max_importance = max(current_importance, new_signal.complexity_score)

        # Scale base size by importance
        if max_importance > 0.6:  # High importance
            target_words = int(self.base_chunk_words * self.importance_multiplier)
        elif max_importance > 0.3:  # Medium importance
            target_words = int(self.base_chunk_words * 1.2)
        else:  # Low importance
            target_words = int(self.base_chunk_words * 0.8)

        # Ensure within bounds and above minimum for embedding quality
        target_words = max(self.min_chunk_words, min(self.max_chunk_words, target_words))

        return target_words

    def _should_split_chunk(self, current_words: int, sentence_words: int,
                            target_words: int, current_sentences: List[str],
                            new_sentence: str, new_signal: ContentSignal) -> bool:
        """Determine if we should split the current chunk."""

        # If we haven't reached minimum size, don't split
        if current_words < self.min_chunk_words:
            return False

        # If adding this sentence would exceed max size, split
        if current_words + sentence_words > self.max_chunk_words:
            return True

        # If we're over target size, consider splitting
        if current_words > target_words:
            # Don't split if this would break clause coherence
            if self.preserve_complete_clauses:
                if self._would_break_clause_coherence(current_sentences, new_sentence):
                    return False
            return True

        # Check for natural boundaries (section breaks, etc.)
        if self._is_natural_boundary(new_sentence):
            return current_words > self.min_chunk_words

        return False

    def _would_break_clause_coherence(self, current_sentences: List[str],
                                      new_sentence: str) -> bool:
        """Check if splitting would break clause coherence."""
        if not current_sentences:
            return False

        # Check if current chunk ends with incomplete clause indicators
        last_sentence = current_sentences[-1]

        # Indicators that the clause continues
        continuation_indicators = [
            'and', 'or', 'but', 'however', 'provided that',
            'subject to', 'unless', 'if', 'when', 'where'
        ]

        # Strip punctuation for better matching
        last_sentence_clean = re.sub(r'[.,;:()]+\s*$', '', last_sentence.lower().strip())

        # If last sentence ends with continuation indicators, don't split
        for indicator in continuation_indicators:
            if last_sentence_clean.endswith(indicator):
                return True

        # Check if new sentence starts with continuation words
        new_sentence_lower = new_sentence.lower().strip()
        continuation_starts = ['and', 'or', 'but', 'however', 'therefore', 'thus']

        for start in continuation_starts:
            if new_sentence_lower.startswith(start):
                return True

        return False

    def _is_natural_boundary(self, sentence: str) -> bool:
        """Check if sentence represents a natural boundary."""
        # Check for clause boundary patterns
        for pattern in self.clause_boundary_patterns:
            if pattern.match(sentence):
                return True

        # Check for section/article headers with word boundaries
        if re.search(r'\b(?:SECTION|ARTICLE|CHAPTER)\b', sentence, re.IGNORECASE):
            return True

        return False

    def _add_overlap(self, chunks: List[str]) -> List[str]:
        """Add word-based overlap between chunks."""
        if len(chunks) <= 1 or self.overlap_words <= 0:
            return chunks

        overlapped_chunks = []

        for i, chunk in enumerate(chunks):
            if i == 0:
                # First chunk - no prefix overlap
                overlapped_chunks.append(chunk)
            else:
                # Get overlap from previous chunk
                prev_chunk_words = chunks[i - 1].split()
                overlap_words = prev_chunk_words[-self.overlap_words:]

                # Add overlap to current chunk
                current_chunk_words = chunk.split()
                overlapped_chunk = ' '.join(overlap_words + current_chunk_words)
                overlapped_chunks.append(overlapped_chunk)

        return overlapped_chunks

    def _create_chunk_result(self, text: str, chunk_index: int, policy_id: Optional[str],
                             total_chunks: int) -> ChunkResult:
        """Create a ChunkResult with smart size metadata."""

        # Analyze the chunk for properties
        chunk_signal = self._analyze_chunk(text)

        # Create metadata
        metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(policy_id, chunk_index),
            policy_id=policy_id,
            chunk_type="smart_size",
            word_count=len(text.split()),
            has_amounts=chunk_signal.has_amount,
            has_conditions=chunk_signal.has_condition,
            has_exclusions=chunk_signal.has_exclusion,
            section=None,  # Could be enhanced to detect sections
            coverage_type=self._infer_coverage_type(text),
            confidence_score=chunk_signal.confidence_score,
            extra_data={
                'chunk_method': 'smart_size',
                'chunk_index': chunk_index,
                'total_chunks': total_chunks,
                'target_words': self._calculate_target_chunk_words(0, chunk_signal),
                'actual_words': len(text.split()),
                'complexity_score': chunk_signal.complexity_score,
                'has_coverage_statement': chunk_signal.has_coverage_statement,
                'has_definition': chunk_signal.has_definition,
                'sentence_count': chunk_signal.sentence_count,
                'overlap_words': self.overlap_words
            }
        )

        return ChunkResult(text=text, metadata=metadata)

    def _analyze_chunk(self, text: str) -> ContentSignal:
        """Analyze an entire chunk for content signals."""
        signal = ContentSignal()

        # Check for amounts
        signal.has_amount = any(pattern.search(text) for pattern in self.amount_patterns)

        # Check for conditions
        signal.has_condition = any(pattern.search(text) for pattern in self.condition_patterns)

        # Check for exclusions
        signal.has_exclusion = any(pattern.search(text) for pattern in self.exclusion_patterns)

        # Check for coverage statements
        signal.has_coverage_statement = any(pattern.search(text) for pattern in self.coverage_patterns)

        # Check for definitions
        signal.has_definition = any(pattern.search(text) for pattern in self.definition_patterns)

        # Count sentences
        signal.sentence_count = len(self._split_into_sentences(text))

        # Calculate complexity and confidence scores
        signal.complexity_score = self._calculate_complexity_score(text, signal)
        signal.confidence_score = self._calculate_confidence_score(text, signal)

        return signal

    def _infer_coverage_type(self, text: str) -> str:
        """Infer coverage type from chunk content."""
        text_lower = text.lower()

        # Coverage type mapping
        coverage_keywords = {
            'medical': ['medical', 'mediche', 'hospital', 'doctor', 'treatment'],
            'baggage': ['baggage', 'bagaglio', 'luggage', 'suitcase'],
            'cancellation': ['cancellation', 'annullamento', 'cancel', 'trip'],
            'delay': ['delay', 'ritardo', 'late', 'postpone'],
            'assistance': ['assistance', 'assistenza', 'help', 'support'],
            'exclusions': ['exclusion', 'esclusione', 'not covered', 'except'],
            'definitions': ['definition', 'definizione', 'means', 'refers to']
        }

        for coverage_type, keywords in coverage_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                return coverage_type

        return 'general'

    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this chunking strategy."""
        return {
            "name": "smart_size",
            "description": "Adaptive word-based chunking that adjusts size based on content importance and coherence",
            "type": "adaptive",
            "complexity": "high",
            "performance": "medium",
            "config": self.config,
            "features": [
                "content_importance_analysis",
                "adaptive_word_based_sizing",
                "clause_coherence_preservation",
                "natural_boundary_detection",
                "multi_signal_analysis",
                "complexity_and_confidence_scoring",
                "optional_overlap_support"
            ],
            "content_signals": [
                "monetary_amounts",
                "conditions_and_prerequisites",
                "exclusions_and_limitations",
                "coverage_statements",
                "definitions_and_terms",
                "clause_boundaries"
            ],
            "size_adaptation": {
                "base_words": self.base_chunk_words,
                "word_range": [self.min_chunk_words, self.max_chunk_words],
                "importance_multiplier": self.importance_multiplier,
                "overlap_words": self.overlap_words
            },
            "best_for": [
                "insurance_policy_analysis",
                "legal_document_processing",
                "coverage_determination",
                "amount_extraction",
                "condition_analysis"
            ],
            "improvements_over_v1": [
                "word_based_sizing_consistency",
                "fixed_clause_coherence_detection",
                "improved_sentence_splitting",
                "separate_complexity_confidence_scores",
                "optional_overlap_support",
                "better_natural_boundary_detection"
            ],
            "expected_improvement": "Enhanced retrieval precision for complex queries with better context preservation"
        }

    def validate_config(self) -> bool:
        """Validate configuration."""
        if self.min_chunk_words <= 0:
            logger.error("min_chunk_words must be positive")
            return False

        if self.max_chunk_words <= self.min_chunk_words:
            logger.error("max_chunk_words must be greater than min_chunk_words")
            return False

        if self.base_chunk_words < self.min_chunk_words or self.base_chunk_words > self.max_chunk_words:
            logger.error("base_chunk_words must be between min and max chunk words")
            return False

        if self.importance_multiplier <= 0:
            logger.error("importance_multiplier must be positive")
            return False

        if self.overlap_words < 0:
            logger.error("overlap_words cannot be negative")
            return False

        if self.overlap_words >= self.min_chunk_words:
            logger.error("overlap_words must be less than min_chunk_words")
            return False

        return True


# End of models/chunking/smart_size_chunker.py
# ================================================================================

# File 18/39: models/factory.py
# --------------------------------------------------------------------------------

# src/models/factory.py

from config import DOCUMENT_DIR, is_qwen_model, is_phi_model, is_openrouter_model
from models.hf_model import HuggingFaceModelClient
from models.openai_model import OpenAIModelClient
from models.qwen_model import QwenModelClient
from models.openrouter_model import OpenRouterModelClient
from models.shared_client import SharedModelClient
from utils import list_policy_paths


def get_model_client(provider: str, model_name: str, sys_prompt: str):
    """
    Create appropriate model client based on provider and model name.

    Args:
        provider: Model provider ("openai", "hf", "qwen", "openrouter")
        model_name: Name of the model to use
        sys_prompt: System prompt for the model

    Returns:
        Appropriate model client instance
    """
    file_paths = list_policy_paths(DOCUMENT_DIR)

    if provider == "openai":
        return OpenAIModelClient(model_name, sys_prompt, file_paths)
    elif provider == "openrouter":
        return OpenRouterModelClient(model_name, sys_prompt)
    elif provider in ["hf", "qwen"]:
        # Auto-detect model type and use appropriate client
        if is_openrouter_model(model_name):
            # If user specified hf/qwen but model looks like OpenRouter, suggest correction
            print(f"Warning: Model '{model_name}' appears to be an OpenRouter model. Consider using --model openrouter")
            return OpenRouterModelClient(model_name, sys_prompt)
        elif is_qwen_model(model_name):
            return QwenModelClient(model_name, sys_prompt)
        elif is_phi_model(model_name):
            return HuggingFaceModelClient(model_name, sys_prompt)
        else:
            return HuggingFaceModelClient(model_name, sys_prompt)
    else:
        raise ValueError(f"Unknown provider: {provider}. Supported: openai, hf, qwen, openrouter")


def get_shared_relevance_client(base_client, relevance_prompt: str):
    """Create a shared model client for relevance filtering."""
    return SharedModelClient(base_client, relevance_prompt)



# End of models/factory.py
# ================================================================================

# File 19/39: models/hf_model.py
# --------------------------------------------------------------------------------

# src/models/hf_model.py

"""
HuggingFace model client implementation for insurance policy analysis.
"""
import logging
import os
from typing import List, Dict, Any

from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from config import HUGGINGFACE_TOKEN, get_local_model_path, get_model_config
from models.base import BaseModelClient
from models.persona.extractor import PersonaExtractor
from models.json_utils.extractors import JSONExtractor

# Login with your token
login(token=HUGGINGFACE_TOKEN)

logger = logging.getLogger(__name__)


class HuggingFaceModelClient(BaseModelClient):
    """
    Client for using HuggingFace models to analyze insurance policies.
    Handles model initialization, inference, and result formatting.
    """

    def __init__(self, model_name: str, sys_prompt: str):
        """
        Initialize a HuggingFace model client.

        Args:
            model_name: Name of the model to use
            sys_prompt: System prompt for the model
        """
        logger.info(f"Loading HuggingFace model: {model_name}")
        self._log_cache_locations()
        model_name = self._check_model_in_scratch(model_name)

        self.pipe = self._initialize_pipeline(model_name)
        self.base_prompt = sys_prompt.strip()
        self.json_format = self._get_json_format_template()

        # Initialize extractors
        self.persona_extractor = PersonaExtractor(self.pipe)
        self.json_extractor = JSONExtractor()

    def _log_cache_locations(self) -> None:
        """Log HuggingFace cache locations to verify they're set."""
        logger.info(f"HF_HUB_CACHE: {os.environ.get('HF_HUB_CACHE', 'Not set')}")
        logger.info(f"HF_ASSETS_CACHE: {os.environ.get('HF_ASSETS_CACHE', 'Not set')}")

    def _check_model_in_scratch(self, model_name: str) -> str:
        """Check if the model exists locally and return appropriate path."""
        return get_local_model_path(model_name)

    def _initialize_pipeline(self, model_name: str):
        """Initialize the HuggingFace pipeline with the specified model."""
        try:
            logger.info(f"Initializing pipeline with model: {model_name}")
            model_config = get_model_config(model_name)

            tokenizer = self._load_tokenizer(model_name)
            model = self._load_model(model_name, model_config)

            # Create pipeline with loaded model and tokenizer
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer
            )

            logger.info("Pipeline successfully initialized")
            return pipe

        except Exception as e:
            logger.error(f"Error initializing pipeline: {e}")
            raise

    def _load_tokenizer(self, model_name: str):
        """Load the tokenizer for the specified model."""
        return AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )

    def _load_model(self, model_name: str, model_config: dict):
        """Load the model with configuration from config.py."""
        return AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=model_config["torch_dtype"],
            device_map=model_config["device_map"],
            trust_remote_code=model_config["trust_remote_code"],
            low_cpu_mem_usage=model_config["low_cpu_mem_usage"]
        )

    def _get_json_format_template(self) -> str:
        """Return the JSON format template for model output."""
        return """
            Format the output EXACTLY in the following JSON schema WITHOUT ANY ADDITIONAL TEXT BEFORE OR AFTER:
            {
              "answer": {
                "eligibility": "Yes | No - Unrelated event | No - condition(s) not met",
                "outcome_justification": "Quoted text from policy",
                "payment_justification": "Amount like '1000 CHF' or null"
              }
            }
            """

    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """Create a formatted error response."""
        return {
            "answer": {
                "eligibility": "Error",
                "outcome_justification": error_message,  # NEW field name
                "payment_justification": None  # NEW field name
            }
        }

    def _format_context_text(self, context_files: List[str]) -> str:
        """
        Format context information from policy files.

        Args:
            context_files: List of context file contents

        Returns:
            Formatted context text
        """
        if not context_files or len(context_files) == 0:
            return ""

        context_text = "\n\nRelevant policy information:\n\n"
        context_text += ("""
                        WARNING: Some of the following context may contain "SECTION DEFINITIONS" of terms or general "
                        "information that does not directly indicate coverage. Please carefully distinguish "
                        "between definitions, exclusions and actual coverage provisions.\n\n
                        """)

        for i, ctx in enumerate(context_files, 1):
            context_text += f"Policy context {i}:\n{ctx.strip()}\n\n"

        return context_text

    def _build_prompt(self, question: str, context_text: str, persona_text: str) -> str:
        """
        Build the full prompt using model-specific format.
        - Phi-4: Simple concatenated format (original working version)
        - Qwen: Chat template format
        """
        # Get the model path to determine which format to use
        model_path = getattr(self.pipe.model, 'name_or_path', '')
        model_name_lower = model_path.lower()

        # Check if this is a Qwen model
        is_qwen = any(indicator in model_name_lower for indicator in ["qwen", "qwen2", "qwen2.5"])

        if is_qwen:
            # QWEN FORMAT: Use chat template
            return self._build_qwen_chat_prompt(question, context_text, persona_text)
        else:
            # PHI-4 FORMAT: Use original simple format that was working
            return self._build_phi_simple_prompt(question, context_text, persona_text)

    def _build_phi_simple_prompt(self, question: str, context_text: str, persona_text: str) -> str:
        """
        Build the full prompt for Phi-4 using the original simple format that was working.
        """
        # Replace placeholders in base_prompt with actual values
        prompt_with_replacements = self.base_prompt.replace("{{RETRIEVED_POLICY_TEXT}}", context_text.strip())
        prompt_with_replacements = prompt_with_replacements.replace("{{USER_QUESTION}}", question)

        prompt_final = "\nThen the JSON Solution is:\n\n"

        if persona_text:
            full_prompt = f"{prompt_with_replacements}\n\n{persona_text}\n\n{prompt_final}"
        else:
            full_prompt = f"{prompt_with_replacements}\n\n{prompt_final}"

        logger.debug(f"Phi-4 prompt: {full_prompt}")
        return full_prompt

    def _build_qwen_chat_prompt(self, question: str, context_text: str, persona_text: str) -> str:
        """
        Build the full prompt for Qwen using chat template format.
        """
        # Replace placeholders in base_prompt with actual values
        prompt_with_replacements = self.base_prompt.replace("{{RETRIEVED_POLICY_TEXT}}", context_text.strip())
        prompt_with_replacements = prompt_with_replacements.replace("{{USER_QUESTION}}", question)

        if persona_text:
            user_content = f"{prompt_with_replacements}\n\n{persona_text}"
        else:
            user_content = f"{prompt_with_replacements}"

        # Use Qwen chat template format
        full_prompt = user_content
        logger.debug(f"Qwen chat template prompt: {full_prompt}")
        return full_prompt

    def _get_appropriate_json_format(self) -> str:
        """
        Get the appropriate JSON format based on the current prompt type.

        Returns:
            The appropriate JSON format string
        """
        # Check if this is a relevance filtering prompt
        if "INSURANCE-POLICY RELEVANCE FILTER" in self.base_prompt:
            return """
                Return exactly this JSON:
                {
                  "is_relevant": true/false,
                  "reason": "Brief explanation (â‰¤ 25 words)"
                }
                """
        else:
            # Default to insurance analysis format
            return self.json_format

    def _generate_model_output(self, prompt: str) -> Dict[str, Any]:
        """
        Generate output from the model.

        Args:
            prompt: The prompt to send to the model

        Returns:
            Parsed JSON response or default "No - condition(s) not met" response
        """
        try:
            model_config = get_model_config(self.pipe.model.name_or_path)
            # Generate text with appropriate parameters for JSON output
            outputs = self.pipe(
                prompt,
                max_new_tokens=model_config["max_new_tokens"],
                do_sample=model_config["do_sample"],
                num_return_sequences=1,
                return_full_text=False
            )

            # Get the generated text
            generated = outputs[0]["generated_text"]
            logger.debug(f"Model output: {generated}")

            # Try to extract JSON from the output
            extracted_json = self.json_extractor.extract_json(generated)

            if extracted_json:
                return extracted_json

            # If all extraction methods fail, return default "No - condition(s) not met" response
            logger.error("All JSON extraction methods failed - returning default 'No - condition(s) not met' response")
            return self._create_default_no_conditions_response()

        except Exception as e:
            error_msg = f"Error during model inference: {str(e)}"
            logger.error(error_msg)
            return self._create_default_no_conditions_response()

    def _create_default_no_conditions_response(self) -> Dict[str, Any]:
        """Create a default response when JSON extraction fails."""
        return {
            "answer": {
                "eligibility": "",
                "outcome_justification": "",  # NEW field name
                "payment_justification": ""  # NEW field name
            }
        }

    def query(self, question: str, context_files: List[str], use_persona: bool = False) -> Dict[str, Any]:
        """
        Process a query and return the model's response.

        Args:
            question: The question to answer
            context_files: List of relevant policy files
            use_persona: Whether to extract and use persona information (default: False)

        Returns:
            Dictionary containing the answer
        """
        logger.info(f"Querying model with question: {question}")

        # Extract personas from the question if use_persona is True
        persona_text = ""
        if use_persona:
            personas_info = self.persona_extractor.extract_personas(question)
            logger.info(f"Extracted personas: {personas_info}")
            # Format the persona information
            persona_text = self.persona_extractor.format_persona_text(personas_info)
            logger.info("Using persona information in prompt")
        else:
            logger.info("Skipping persona extraction (--persona flag not used)")

        # Format context information
        context_text = self._format_context_text(context_files)

        # Build the full prompt
        full_prompt = self._build_prompt(question, context_text, persona_text)

        # Generate and process the model output
        return self._generate_model_output(full_prompt)


# End of models/hf_model.py
# ================================================================================

# File 20/39: models/json_utils/__init__.py
# --------------------------------------------------------------------------------



# End of models/json_utils/__init__.py
# ================================================================================

# File 21/39: models/json_utils/extractors.py
# --------------------------------------------------------------------------------

# src/models/json_utils/extractors.py
"""
JSON extraction utilities.
Functions for extracting valid JSON from model outputs.
"""
import json
import re
import logging
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class JSONExtractor:
    """
    Extracts and validates JSON from model outputs.
    Supports multiple extraction methods and validation.
    """

    def extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from text, handling Qwen thinking tags and malformed JSON.
        """
        # Remove thinking tags that interfere with JSON extraction
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

        # Remove any text before the first { and after the last }
        # This helps with models that add explanatory text
        first_brace = text.find('{')
        last_brace = text.rfind('}')

        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            text = text[first_brace:last_brace + 1]

        # QUICK FIX: Replace escaped quotes in policy text with single quotes
        text = re.sub(r'\\"([^"]*)\\"', r"'\1'", text)

        # Try multiple extraction methods
        extracted = self._try_all_json_extraction_methods(text)
        if extracted:
            normalized = self._normalize_json_fields(extracted)
            logger.info(f"Successfully extracted and normalized JSON: {normalized}")
            return normalized
        return None

    def _normalize_json_fields(self, json_obj: Dict[str, Any]) -> Dict[str, Any]:
        """
        Normalize model output JSON to the standard structure:
        {
          "answer": {
            "eligibility": <str>,
            "outcome_justification": <str>,
            "payment_justification": <str|null>
          }
        }
        """
        if "answer" in json_obj and isinstance(json_obj["answer"], dict):
            answer = json_obj["answer"]
            normalized_answer = {}

            # 1. Normalize eligibility
            eligibility = ""
            for key in ["eligibility", "elgibility", "eligiblity", "eligible"]:
                if key in answer and isinstance(answer[key], str):
                    eligibility = answer[key].strip()
                    break
            normalized_answer["eligibility"] = eligibility

            # 2. Normalize outcome_justification
            outcome = ""
            for key in ["outcome_justification", "eligibility_policy", "justification", "text", "description"]:
                if key in answer:
                    val = answer[key]
                    if isinstance(val, str):
                        outcome = val.strip()
                    elif isinstance(val, dict):
                        # flatten if nested keys contain single string values
                        outcome = "; ".join(str(v) for v in val.values() if isinstance(v, str))
                    elif isinstance(val, list):
                        outcome = "; ".join(str(v) for v in val if isinstance(v, str))
                    break
            normalized_answer["outcome_justification"] = outcome

            # 3. Normalize payment_justification
            payment = None
            for key in ["payment_justification", "amount_policy", "amount", "coverage_amount", "payment"]:
                if key in answer:
                    val = answer[key]
                    if isinstance(val, str) and val.strip():
                        payment = val.strip()
                    else:
                        payment = None
                    break
            normalized_answer["payment_justification"] = payment

            json_obj["answer"] = normalized_answer

        return json_obj

    def _try_all_json_extraction_methods(self, generated: str) -> Optional[Dict[str, Any]]:
        """
        Try multiple JSON extraction methods in sequence.
        Args:
            generated: The generated text to extract JSON from
        Returns:
            Extracted JSON or None if all methods failed
        """
        # Try solution block extraction (specific to Phi pattern)
        solution_json = self._extract_json_from_solution_block(generated)
        if solution_json:
            logger.info("Successfully parsed JSON from solution block format")
            return solution_json

        # Try code block extraction
        code_block_json = self._extract_json_from_code_block(generated)
        if code_block_json:
            logger.info("Successfully parsed JSON from code block")
            return code_block_json

        # Try generic JSON extraction
        generic_json = self._extract_json_from_text(generated)
        if generic_json:
            logger.info("Successfully parsed JSON using generic extraction")
            return generic_json

        return None

    def _extract_json_from_solution_block(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from Phi's "Solution 1" block format.
        Args:
            text: Generated text containing the solution block
        Returns:
            Parsed JSON dictionary or None if not found
        """
        solution_pattern = r'## Solution 1:.*?```json\s*(.*?)\s*```'
        match = re.search(solution_pattern, text, re.DOTALL)
        if match:
            json_str = match.group(1).strip()
            try:
                parsed = json.loads(json_str)
                logger.debug(f"Successfully extracted JSON from solution block: {json_str}")
                return parsed
            except json.JSONDecodeError as e:
                logger.warning(f"Found solution block but JSON is invalid: {e}")
        return None

    def _extract_json_from_code_block(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from markdown code blocks.
        Args:
            text: Generated text that might contain code blocks
        Returns:
            Parsed JSON dictionary or None if not found
        """
        code_block_pattern = r'```(?:json)?\s*(.*?)\s*```'
        matches = re.findall(code_block_pattern, text, re.DOTALL)
        for block in matches:
            try:
                parsed = json.loads(block.strip())
                if self._is_valid_answer_json_flexible(parsed):
                    return parsed
            except json.JSONDecodeError:
                continue
        return None

    def _extract_json_from_text(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract valid JSON from text that might contain additional content.
        Args:
            text: The generated text that should contain JSON
        Returns:
            The parsed JSON dictionary or None if not found
        """
        json_candidates = self._extract_json_candidate_strings(text)
        if not json_candidates:
            logger.warning("No JSON-like patterns found in the text")
            return None

        for json_candidate in json_candidates:
            json_obj = self._parse_json_candidate(json_candidate)
            if json_obj:
                return json_obj

        logger.warning("No valid JSON with expected structure found")
        return None

    def _extract_json_candidate_strings(self, text: str) -> List[str]:
        """
        Enhanced JSON candidate extraction.
        """
        if not text:
            logger.warning("Empty text provided for JSON extraction")
            return []

        # First, try to find JSON objects with flexible key matching
        patterns = [
            # Standard JSON pattern
            r'\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\}',
            # More flexible pattern for malformed JSON
            r'\{[^{}]*["\'](?:answer|eligibility|eligible)["\'][^{}]*\}',
        ]

        all_matches = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            all_matches.extend(matches)

        # Remove duplicates while preserving order
        seen = set()
        unique_matches = []
        for match in all_matches:
            if match not in seen:
                seen.add(match)
                unique_matches.append(match)

        # Sort by length (longest first)
        unique_matches.sort(key=len, reverse=True)
        return unique_matches

    def _parse_json_candidate(self, json_candidate: str) -> Optional[Dict[str, Any]]:
        """
        Try to parse a JSON candidate string.
        Args:
            json_candidate: The string to parse as JSON
        Returns:
            Parsed JSON object or None if parsing failed
        """
        try:
            # Attempt to parse the candidate
            json_obj = json.loads(json_candidate)
            if self._is_valid_answer_json_flexible(json_obj):
                return json_obj
        except json.JSONDecodeError:
            # Try to fix common JSON issues and try again
            try:
                # Replace single quotes with double quotes
                fixed_json = json_candidate.replace("'", '"')
                json_obj = json.loads(fixed_json)
                if self._is_valid_answer_json_flexible(json_obj):
                    logger.info("Successfully parsed JSON after fixing quotes")
                    return json_obj
            except json.JSONDecodeError:
                pass
        return None

    def _is_valid_answer_json_flexible(self, json_obj: Dict[str, Any]) -> bool:
        """
        Very flexible validation - accepts almost any JSON with recognizable structure.
        Args:
            json_obj: The JSON object to check
        Returns:
            True if the JSON has some recognizable structure, False otherwise
        """
        # Check for the answer structure (very flexible)
        if "answer" in json_obj and isinstance(json_obj["answer"], dict):
            return True

        # Check for the personas structure for persona extraction
        if "personas" in json_obj and isinstance(json_obj["personas"], dict):
            return True

        # Check for relevance filtering structure
        if "is_relevant" in json_obj and "reason" in json_obj:
            return True

        # If it has at least one of the key fields we care about
        if isinstance(json_obj, dict):
            answer_keys = ["eligibility", "elgibility", "eligiblity", "eligibilty"]
            if any(key in json_obj for key in answer_keys):
                return True

        return False

    def _is_valid_answer_json(self, json_obj: Dict[str, Any]) -> bool:
        """
        Legacy method - kept for compatibility but now calls the flexible version
        """
        return self._is_valid_answer_json_flexible(json_obj)


# End of models/json_utils/extractors.py
# ================================================================================

# File 22/39: models/openai_model.py
# --------------------------------------------------------------------------------

# src/models/openai_model.py
from typing import List, Dict, Any

from .base import BaseModelClient
from assistant_manager import client, create_vector_store, create_assistant, update_assistant_vector
import json, time, logging

logger = logging.getLogger(__name__)

class OpenAIModelClient(BaseModelClient):
    def __init__(self, model_name: str, sys_prompt: str, file_paths: List[str]):
        self.vector_store = create_vector_store("RAG_VectorStore", file_paths)
        self.assistant = create_assistant("Insurance Expert Assistant", sys_prompt, model_name)
        update_assistant_vector(self.assistant.id, self.vector_store.id)
        self.thread = client.beta.threads.create()

    def query(self, question: str, context_files: List[str]) -> Dict[str, Any]:
        prompt = f"Question: \"{question}\". Give a Yes/No, quote the supporting text, and mention the amount if relevant."
        client.beta.threads.messages.create(thread_id=self.thread.id, role="user", content=prompt)
        run = client.beta.threads.runs.create(thread_id=self.thread.id, assistant_id=self.assistant.id)

        while client.beta.threads.runs.retrieve(thread_id=self.thread.id, run_id=run.id).status != "completed":
            time.sleep(1)

        messages = client.beta.threads.messages.list(thread_id=self.thread.id)
        for msg in reversed(messages.data):
            if msg.role == "assistant":
                return json.loads(msg.content[0].text.value.strip())
        return {}



# End of models/openai_model.py
# ================================================================================

# File 23/39: models/openrouter_model.py
# --------------------------------------------------------------------------------

# =============================================================================
# Enhanced OpenRouter Model with Rate Limiting and Complete Prompt/Output Logging
# =============================================================================

"""
OpenRouter model client implementation for insurance policy analysis.
Uses OpenRouter's API to access Qwen and other models through a unified endpoint.
Enhanced with comprehensive logging and intelligent rate limiting.
"""
import logging
import json
import time
from typing import List, Dict, Any
from datetime import datetime

from openai import OpenAI
import httpx

from config import OPENROUTER_API_KEY, OPENROUTER_SITE_URL, OPENROUTER_SITE_NAME
from models.base import BaseModelClient
from models.json_utils.extractors import JSONExtractor

logger = logging.getLogger(__name__)


class OpenRouterModelClient(BaseModelClient):
    """
    Client for using OpenRouter API to access various models including Qwen.
    Handles model initialization, inference, and result formatting.
    Enhanced with comprehensive logging and intelligent rate limiting.
    """

    def __init__(self, model_name: str, sys_prompt: str, requests_per_minute: int = 12):
        """
        Initialize an OpenRouter model client.

        Args:
            model_name: Name of the model to use (e.g., "qwen/qwen-2.5-72b-instruct")
            sys_prompt: System prompt for the model
            requests_per_minute: Maximum requests per minute (default: 12, conservative)
        """
        logger.info(f"Loading OpenRouter model: {model_name}")
        logger.info(f"Rate limit: {requests_per_minute} requests per minute")

        # Log the system prompt being used
        logger.info(f"System prompt: {sys_prompt}")

        # Initialize OpenAI client with OpenRouter endpoint
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=OPENROUTER_API_KEY,
        )

        self.model_name = model_name
        self.base_prompt = sys_prompt.strip()

        # Initialize JSON extractor
        self.json_extractor = JSONExtractor()

        # Rate limiting
        self.requests_per_minute = requests_per_minute
        self.min_delay = 60.0 / requests_per_minute  # Minimum delay between requests
        self.last_request_time = 0
        self.rate_limit_reset_time = None
        self.remaining_requests = None

        logger.info(f"OpenRouter client initialized successfully for model: {model_name}")
        logger.info(f"Rate limiting: {self.min_delay:.2f}s minimum delay between requests")

    def _wait_for_rate_limit(self):
        """Wait if necessary to respect rate limits."""
        current_time = time.time()

        # Check if we need to wait for rate limit reset
        if self.rate_limit_reset_time and current_time < self.rate_limit_reset_time:
            wait_time = self.rate_limit_reset_time - current_time
            logger.warning(f"Rate limit exceeded. Waiting {wait_time:.1f}s for reset...")
            time.sleep(wait_time)
            # Reset tracking after waiting
            self.rate_limit_reset_time = None
            self.remaining_requests = None

        # Ensure minimum delay between requests
        time_since_last = current_time - self.last_request_time
        if time_since_last < self.min_delay:
            sleep_time = self.min_delay - time_since_last
            logger.info(f"Rate limiting: waiting {sleep_time:.2f}s before next request")
            time.sleep(sleep_time)

        self.last_request_time = time.time()

    def _handle_rate_limit_headers(self, response_headers):
        """Extract and handle rate limit information from response headers."""
        try:
            if 'x-ratelimit-remaining' in response_headers:
                self.remaining_requests = int(response_headers['x-ratelimit-remaining'])
                logger.debug(f"Remaining requests: {self.remaining_requests}")

            if 'x-ratelimit-reset' in response_headers:
                reset_timestamp = int(response_headers['x-ratelimit-reset']) / 1000  # Convert from ms
                self.rate_limit_reset_time = reset_timestamp
                reset_time = datetime.fromtimestamp(reset_timestamp)
                logger.debug(f"Rate limit resets at: {reset_time}")

            # If we're running low on requests, slow down
            if self.remaining_requests is not None and self.remaining_requests < 3:
                logger.warning(f"Low on requests ({self.remaining_requests} remaining). Increasing delay.")
                self.min_delay = max(self.min_delay * 2, 5.0)  # At least 5s delay

        except (ValueError, TypeError) as e:
            logger.debug(f"Error parsing rate limit headers: {e}")

    def _make_api_call_with_retry(self, messages: List[Dict[str, str]], max_retries: int = 3) -> str:
        """
        Make API call with intelligent retry logic and rate limiting.
        """
        for attempt in range(max_retries):
            try:
                # Wait if necessary for rate limiting
                self._wait_for_rate_limit()

                # Create extra headers for OpenRouter
                extra_headers = {}
                if OPENROUTER_SITE_URL:
                    extra_headers["HTTP-Referer"] = OPENROUTER_SITE_URL
                if OPENROUTER_SITE_NAME:
                    extra_headers["X-Title"] = OPENROUTER_SITE_NAME

                logger.info(f"Making API call to model: {self.model_name} (attempt {attempt + 1})")
                logger.debug(f"Extra headers: {extra_headers}")

                response = self.client.chat.completions.create(
                    extra_headers=extra_headers,
                    model=self.model_name,
                    messages=messages,
                    max_tokens=1200,
                    temperature=0.1,
                    top_p=0.9
                )

                # Success! Handle rate limit headers from response
                if hasattr(response, 'response') and hasattr(response.response, 'headers'):
                    self._handle_rate_limit_headers(response.response.headers)

                generated = response.choices[0].message.content
                logger.info("API call successful")

                # Log token usage if available
                if hasattr(response, 'usage'):
                    logger.info(f"Token usage: {response.usage}")

                return generated

            except httpx.HTTPStatusError as e:
                if e.response.status_code == 429:
                    # Rate limit exceeded
                    logger.warning(f"Rate limit exceeded on attempt {attempt + 1}")

                    # Parse rate limit headers from error response
                    self._handle_rate_limit_headers(e.response.headers)

                    if attempt < max_retries - 1:
                        # Calculate wait time with exponential backoff
                        base_wait = 2 ** (attempt + 1)  # 2, 4, 8 seconds

                        # If we have reset time, use it
                        if self.rate_limit_reset_time:
                            wait_time = max(base_wait, self.rate_limit_reset_time - time.time())
                        else:
                            wait_time = base_wait

                        logger.info(f"Waiting {wait_time:.1f}s before retry...")
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.error("Max retries reached for rate limit")
                        raise

                else:
                    # Other HTTP error
                    logger.error(f"HTTP error {e.response.status_code}: {e}")
                    raise

            except Exception as e:
                logger.error(f"Unexpected error on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    wait_time = 2 ** (attempt + 1)
                    logger.info(f"Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                    continue
                else:
                    raise

        raise Exception("Max retries reached")

    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """Create a formatted error response."""
        return {
            "answer": {
                "eligibility": "Error",
                "outcome_justification": error_message,  # NEW field name
                "payment_justification": None  # NEW field name
            }
        }

    def _create_default_no_conditions_response(self) -> Dict[str, Any]:
        """Create a default response when JSON extraction fails."""
        default_response = {
            "answer": {
                "eligibility": "",
                "outcome_justification": "",  # NEW field name
                "payment_justification": ""  # NEW field name
            }
        }
        logger.warning(f"Using default response: {default_response}")
        return default_response

    def _format_context_text(self, context_files: List[str]) -> str:
        """Format context information from policy files."""
        if not context_files or len(context_files) == 0:
            return ""

        context_text = "\n\nRelevant policy information:\n\n"
        context_text += """
                        WARNING: Some of the following context may contain "SECTION DEFINITIONS" of terms or general 
                        information that does not directly indicate coverage. Please carefully distinguish 
                        between definitions, exclusions and actual coverage provisions.\n\n
                        """

        for i, ctx in enumerate(context_files, 1):
            context_text += f"Policy context {i}:\n{ctx.strip()}\n\n"

        # Log the formatted context
        logger.debug(f"Formatted context text (length: {len(context_text)} chars)")
        logger.debug(f"Context preview: {context_text}...")

        return context_text

    def _extract_persona_via_api(self, question: str) -> str:
        """Extract persona information using the API model."""
        try:
            persona_prompt = f"""
            Analyze this insurance query and extract ONLY information about the people involved and their locations:

            Query: "{question}"

            1. Who is making the insurance claim (the primary policy user/policyholder)?
            2. Who actually experienced the event, accident, health issue, loss, or damage?
            3. WHERE was the affected person when the event occurred?
            4. What is the relationship between the policyholder and the affected person?
            5. Is the affected person covered by the policy?
            6. Who else is mentioned but NOT making the claim?
            7. Total number of people in the scenario?
            8. Number of people actually covered by the insurance?

            Return JSON format:
            {{
              "personas": {{
                "policy_user": "Who is making the claim/policyholder",
                "affected_person": "Who experienced the event",
                "location": "Where the event occurred",
                "is_abroad": true or false,
                "relationship_to_policyholder": "Relationship",
                "is_affected_covered": true or false,
                "mentioned_people": "Others mentioned",
                "total_count": number,
                "claimant_count": number,
                "relationship": "Relationships between people"
              }}
            }}
            """

            # Log the persona extraction prompt
            logger.debug("=== PERSONA EXTRACTION PROMPT ===")
            logger.debug(persona_prompt)

            messages = [
                {"role": "system", "content": "You extract persona information from insurance queries."},
                {"role": "user", "content": persona_prompt}
            ]

            persona_response = self._make_api_call_with_retry(messages)

            logger.debug(f"=== PERSONA EXTRACTION RESPONSE ===")
            logger.debug(f"Raw persona response: {persona_response}")

            extracted_json = self.json_extractor.extract_json(persona_response)

            if extracted_json and "personas" in extracted_json:
                try:
                    from models.persona.formatters import format_persona_text
                    formatted_persona = format_persona_text(extracted_json)
                    logger.info(f"Successfully extracted persona: {formatted_persona}")
                    return formatted_persona
                except ImportError:
                    logger.warning("Persona formatters not available, using raw JSON")
                    return json.dumps(extracted_json, indent=2)
            else:
                logger.warning("Failed to extract valid persona JSON from API response")
                return ""

        except Exception as e:
            logger.warning(f"API persona extraction failed: {str(e)}")
            return ""

    def _build_messages(self, question: str, context_text: str, persona_text: str) -> List[Dict[str, str]]:
        """Build the message array for the OpenRouter API call."""

        # Format the system prompt with actual values
        formatted_system_prompt = self.base_prompt.replace(
            "{RETRIEVED_POLICY_TEXT}", context_text
        ).replace(
            "{USER_QUESTION}", question
        )

        # If using persona, include it in the user message
        if persona_text:
            user_content = f"Additional context:\n{persona_text}"
        else:
            user_content = "Please analyze the policy context and question provided above."

        messages = [
            {"role": "system", "content": formatted_system_prompt},
            {"role": "user", "content": user_content}
        ]

        return messages

    def _generate_model_output(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """Generate output from the OpenRouter API with rate limiting."""
        try:
            generated = self._make_api_call_with_retry(messages)

            # Log the complete raw output
            logger.info("=== MODEL OUTPUT ===")
            logger.info(f"Raw model response: {generated}")

            extracted_json = self.json_extractor.extract_json(generated)

            if extracted_json:
                logger.info("=== EXTRACTED JSON ===")
                logger.info(f"Parsed JSON: {json.dumps(extracted_json, indent=2)}")
                return extracted_json

            logger.error("JSON extraction failed - returning default response")
            logger.error(f"Failed to extract JSON from: {generated}")
            return self._create_default_no_conditions_response()

        except Exception as e:
            error_msg = f"Error during OpenRouter API call: {str(e)}"
            logger.error(error_msg)
            return self._create_error_response(error_msg)

    def query(self, question: str, context_files: List[str], use_persona: bool = False) -> Dict[str, Any]:
        """Process a query and return the model's response."""
        logger.info("=" * 60)
        logger.info(f"NEW QUERY STARTED")
        logger.info(f"Model: {self.model_name}")
        logger.info(f"Question: {question}")
        logger.info(f"Context files count: {len(context_files) if context_files else 0}")
        logger.info(f"Use persona: {use_persona}")
        logger.info("=" * 60)

        persona_text = ""
        if use_persona:
            logger.info("Starting persona extraction...")
            persona_text = self._extract_persona_via_api(question)
            if persona_text:
                logger.info("Successfully extracted persona information")
                logger.debug(f"Persona text: {persona_text}")
            else:
                logger.info("No persona information extracted")
        else:
            logger.info("Skipping persona extraction")

        context_text = self._format_context_text(context_files)
        messages = self._build_messages(question, context_text, persona_text)

        logger.info("=== PROMPT FINAL ===")
        logger.info(f"Final prompt: {json.dumps(messages, indent=2)}")
        result = self._generate_model_output(messages)


        logger.info("=== FINAL RESULT ===")
        logger.info(f"Final result: {json.dumps(result, indent=2)}")
        logger.info("=" * 60)

        return result

    def get_rate_limit_status(self) -> Dict[str, Any]:
        """Get current rate limit status information."""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time

        status = {
            "requests_per_minute": self.requests_per_minute,
            "min_delay": self.min_delay,
            "last_request_time": self.last_request_time,
            "time_since_last_request": time_since_last,
            "remaining_requests": self.remaining_requests,
            "rate_limit_reset_time": self.rate_limit_reset_time,
            "ready_for_next_request": time_since_last >= self.min_delay
        }

        return status


# =============================================================================
# LOGGING CONFIGURATION HELPER
# =============================================================================

def setup_detailed_logging(log_level: str = "INFO", log_file: str = "openrouter_model.log"):
    """
    Setup logging configuration to see all the prompt and output details.
    Call this at the start of your application.

    Args:
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR)
        log_file: Path to log file (optional)
    """
    # Convert string log level to logging constant
    numeric_level = getattr(logging, log_level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f'Invalid log level: {log_level}')

    # Configure logging
    handlers = [logging.StreamHandler()]  # Console output

    if log_file:
        handlers.append(logging.FileHandler(log_file))  # File output

    logging.basicConfig(
        level=numeric_level,
        format='%(asctime)s [%(levelname)s] %(name)s (%(filename)s:%(lineno)d): %(message)s',
        handlers=handlers
    )

    # Reduce noise from external libraries
    logging.getLogger('openai').setLevel(logging.WARNING)
    logging.getLogger('httpx').setLevel(logging.WARNING)
    logging.getLogger('httpcore').setLevel(logging.WARNING)

    logger.info(f"Logging configured with level: {log_level}")
    if log_file:
        logger.info(f"Log file: {log_file}")


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def calculate_wait_time_from_reset(reset_timestamp_ms: int) -> float:
    """
    Calculate how long to wait based on OpenRouter's rate limit reset timestamp.

    Args:
        reset_timestamp_ms: Reset timestamp in milliseconds

    Returns:
        Wait time in seconds
    """
    current_time = time.time()
    reset_time = reset_timestamp_ms / 1000.0  # Convert to seconds
    wait_time = max(0, reset_time - current_time)
    return wait_time


def get_recommended_delay(model_name: str) -> float:
    """
    Get recommended delay between requests based on model type.

    Args:
        model_name: OpenRouter model name

    Returns:
        Recommended delay in seconds
    """
    if "free" in model_name.lower():
        return 6.0  # 10 requests per minute for free models
    elif "claude" in model_name.lower():
        return 3.0  # 20 requests per minute for Claude models
    elif "gpt" in model_name.lower():
        return 2.0  # 30 requests per minute for GPT models
    else:
        return 4.0  # 15 requests per minute as default



# End of models/openrouter_model.py
# ================================================================================

# File 24/39: models/persona/__init__.py
# --------------------------------------------------------------------------------



# End of models/persona/__init__.py
# ================================================================================

# File 25/39: models/persona/extractor.py
# --------------------------------------------------------------------------------

# src/models/persona/extractor.py

"""
Main persona extraction functionality.
Coordinates rule-based and LLM-based extraction methods.
"""
import logging
from typing import Dict, Any

from models.persona.rule_based import RuleBasedExtractor
from models.persona.llm_based import LLMBasedExtractor
from models.persona.formatters import format_persona_text

logger = logging.getLogger(__name__)


class PersonaExtractor:
    """
    Extracts persona information from insurance queries.
    Combines rule-based and LLM-based approaches.
    """

    def __init__(self, pipe):
        """
        Initialize persona extractors.

        Args:
            pipe: HuggingFace pipeline for LLM-based extraction
        """
        self.rule_based_extractor = RuleBasedExtractor()
        self.llm_based_extractor = LLMBasedExtractor(pipe)

    def extract_personas(self, question: str) -> Dict[str, Any]:
        """
        Extract persona information from a question.

        Args:
            question: The insurance query to analyze

        Returns:
            Dictionary with persona information
        """
        logger.info(f"Extracting personas from question: {question}")

        # First try rule-based extraction
        rule_based_result = self.rule_based_extractor.extract(question)

        # Try LLM-based approach as a backup
        llm_result = self.llm_based_extractor.extract(question)

        # Use LLM result if available, otherwise use rule-based
        if llm_result:
            logger.info("Using LLM-extracted persona information")
            return llm_result
        else:
            logger.info("Using rule-based persona extraction")
            return rule_based_result

    def format_persona_text(self, personas_info: Dict[str, Any]) -> str:
        """
        Format persona information for inclusion in a prompt.

        Args:
            personas_info: Dictionary with persona information

        Returns:
            Formatted persona text
        """
        return format_persona_text(personas_info)

# End of models/persona/extractor.py
# ================================================================================

# File 26/39: models/persona/formatters.py
# --------------------------------------------------------------------------------

# src/models/persona/formatters.py

"""
Formatting functions for persona information.
Converts extracted persona information into text for prompts.
"""
from typing import Dict, Any


def format_persona_text(personas_info: Dict[str, Any]) -> str:
    """
    Format persona information for inclusion in the prompt, with enhanced focus
    on the affected person and their location during the event.

    Args:
        personas_info: Dictionary with persona information

    Returns:
        Formatted persona text
    """
    persona_text = "IMPORTANT PERSONA INFORMATION FROM THE QUESTION:\n"

    try:
        # Extract persona details
        policy_user = personas_info["personas"]["policy_user"]
        affected_person = personas_info["personas"].get("affected_person", f"{policy_user} (inferred)")
        location = personas_info["personas"].get("location", "unspecified location")
        is_abroad = personas_info["personas"].get("is_abroad", False)
        relationship_to_policyholder = personas_info["personas"].get("relationship_to_policyholder",
                                                                     "self (inferred)")
        is_affected_covered = personas_info["personas"].get("is_affected_covered", True)
        mentioned_people = personas_info["personas"]["mentioned_people"]
        total_count = personas_info["personas"]["total_count"]
        claimant_count = personas_info["personas"]["claimant_count"]
        relationship = personas_info["personas"]["relationship"]

        # Build detailed persona text
        persona_text += f"- Primary policy user/claimant: {policy_user}\n"
        persona_text += f"- Person who experienced the event/accident: {affected_person}\n"
        persona_text += f"- Location where the event occurred: {location}\n"
        persona_text += f"- Event occurred abroad: {'Yes' if is_abroad else 'No'}\n"
        persona_text += f"- Relationship to policyholder: {relationship_to_policyholder}\n"

        # Add coverage information for affected person
        if is_affected_covered:
            persona_text += f"- The affected person is likely covered under this policy\n"
        else:
            persona_text += f"- The affected person may NOT be covered under this policy\n"

        persona_text += f"- Other people mentioned (not policy users): {mentioned_people}\n"
        persona_text += f"- Total number of people mentioned: {total_count}\n"
        persona_text += f"- Number of people actually claiming/covered: {claimant_count}\n"
        persona_text += f"- Relationships between all people: {relationship}\n\n"

        # Add location-specific guidance
        if is_abroad:
            persona_text += "When determining coverage, check if the policy covers events occurring abroad and any special conditions or exclusions for international coverage.\n"

        if "airport" in location.lower():
            persona_text += "When determining coverage, focus on travel-related provisions, especially those related to baggage, delays, or airport incidents.\n"

        elif "hotel" in location.lower() or "resort" in location.lower():
            persona_text += "When determining coverage, check if accommodation-related incidents are covered and under what conditions.\n"

        elif "hospital" in location.lower() or "medical" in location.lower():
            persona_text += "When determining coverage, focus on medical coverage provisions, including emergency treatment and hospitalization.\n"

        elif "home" in location.lower() or "domestic" in location.lower():
            persona_text += "When determining coverage, verify if domestic incidents are covered, as some travel policies only apply when traveling.\n"

        # Add person-specific guidance
        if relationship_to_policyholder == "self":
            persona_text += "Also check provisions that apply to the policyholder directly.\n"
        elif relationship_to_policyholder in ["spouse/partner", "child/dependent"]:
            persona_text += "Also verify if family members/dependents are covered and under what conditions.\n"
        else:
            persona_text += "Also carefully verify if non-family members are covered under this policy.\n"

        persona_text += "Take into account all of these factors when assessing eligibility for coverage.\n"
    except (KeyError, TypeError):
        persona_text += "Unable to extract detailed persona information. Consider who is actually making the claim vs. who experienced the event vs. who is just mentioned and where the event occurred.\n"

    return persona_text


# End of models/persona/formatters.py
# ================================================================================

# File 27/39: models/persona/llm_based.py
# --------------------------------------------------------------------------------

# src/models/persona/llm_based.py

"""
LLM-based persona extraction.
Uses large language models to extract persona information.
"""
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


class LLMBasedExtractor:
    """
    LLM-based extractor for persona information.
    Uses language models to extract information.
    """

    def __init__(self, pipe):
        """
        Initialize the LLM-based extractor.

        Args:
            pipe: HuggingFace pipeline for inference
        """
        self.pipe = pipe

    def extract(self, question: str) -> Optional[Dict[str, Any]]:
        """
        Extract persona information using LLM-based approach.

        Args:
            question: The insurance query to analyze

        Returns:
            Dictionary with persona information or None if extraction failed
        """
        try:
            # Construct a persona-focused prompt
            persona_prompt = self._create_persona_prompt(question)

            # Use a shorter context window
            outputs = self.pipe(
                persona_prompt,
                max_new_tokens=200,
                do_sample=False,
                num_return_sequences=1,
                return_full_text=False
            )

            # Get the generated text
            generated = outputs[0]["generated_text"]
            logger.debug(f"Persona extraction output: {generated}")

            # Try to extract JSON from the output
            from models.json_utils.extractors import JSONExtractor
            json_extractor = JSONExtractor()
            llm_result = json_extractor.extract_json(generated)

            # Return the result if valid
            if llm_result and "personas" in llm_result:
                # Validate and fix values if needed
                llm_result = self._validate_llm_persona_result(llm_result)
                logger.info("Using LLM-extracted persona information")
                return llm_result

            return None

        except Exception as e:
            logger.warning(f"LLM persona extraction failed: {str(e)}")
            return None

    def _create_persona_prompt(self, question: str) -> str:
        """
        Create a prompt for persona extraction with enhanced focus on affected person
        and their location during the event.

        Args:
            question: The insurance query to analyze

        Returns:
            Formatted prompt for the LLM
        """
        return f"""
        Analyze this insurance query and extract ONLY information about the people involved and their locations:

        Query: "{question}"

        1. Who is making the insurance claim (the primary policy user/policyholder)?
        2. Who actually experienced the event, accident, health issue, loss, or damage that is the subject of the claim?
        3. WHERE was the affected person when the event occurred? (e.g., at home, abroad, at the airport, in a hotel)
        4. What is the relationship between the policyholder and the affected person?
        5. Is the affected person covered by the policy? (Usually yes if they are the policyholder, spouse, or dependent)
        6. Who else is mentioned but NOT making the claim or experiencing the event?
        7. Total number of people in the scenario?
        8. Number of people actually covered by the insurance or making claims?

        IMPORTANT: Carefully distinguish between these roles:
        - The POLICYHOLDER (who owns the policy and usually makes the claim)
        - The AFFECTED PERSON (who actually experienced the event, accident, illness, loss, or damage)
        - OTHER PEOPLE who are merely mentioned but not directly involved

        Pay special attention to LOCATION information, which is often critical for insurance claims:
        - Was the event domestic or international?
        - Was the person in transit (airport, train station, etc.)?
        - Was the person at a specific venue (hotel, resort, hospital)?
        - Was the person in their home country or abroad?

        Examples to consider:
        - "At the airport my baggage was lost" â†’ Location: airport
        - "During our vacation in Spain my daughter got sick" â†’ Location: abroad (Spain)
        - "My house was damaged by a storm" â†’ Location: home/domestic
        - "While staying at the hotel, my wallet was stolen" â†’ Location: hotel

        IMPORTANT: The answer must ONLY be valid JSON in this EXACT format:
        {{
          "personas": {{
            "policy_user": "Who is making the claim/policyholder",
            "affected_person": "Who experienced the event/accident/illness/loss/damage",
            "location": "Where the affected person was when the event occurred",
            "is_abroad": true or false (whether the event occurred outside home country),
            "relationship_to_policyholder": "Relationship between affected person and policyholder (self, spouse, child, etc.)",
            "is_affected_covered": true or false (whether the affected person is likely covered),
            "mentioned_people": "Who else is mentioned but not a claimant or affected",
            "total_count": number of ALL people mentioned,
            "claimant_count": number of people actually claiming/using the insurance,
            "relationship": "Relationships between all mentioned people"
          }}
        }}
        """

    def _validate_llm_persona_result(self, llm_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate and fix persona extraction results, including location information.

        Args:
            llm_result: The LLM extraction result to validate

        Returns:
            Validated and corrected result
        """
        # Ensure the personas key exists
        if "personas" not in llm_result:
            llm_result["personas"] = {}

        # VALIDATION: Ensure the counts make sense
        if "total_count" in llm_result["personas"]:
            # Get the count from LLM
            llm_count = llm_result["personas"]["total_count"]
            # Ensure it's reasonable (1-5) for most queries
            if not isinstance(llm_count, int) or llm_count < 1 or llm_count > 5:
                # If unreasonable, use a default count
                llm_result["personas"]["total_count"] = 1
                logger.warning(f"Adjusted implausible LLM total_count from {llm_count} to 1")

        # Make sure claimant_count is not greater than total_count
        if "claimant_count" in llm_result["personas"] and "total_count" in llm_result["personas"]:
            if llm_result["personas"]["claimant_count"] > llm_result["personas"]["total_count"]:
                llm_result["personas"]["claimant_count"] = llm_result["personas"]["total_count"]
                logger.warning("Adjusted claimant_count to not exceed total_count")

        # Ensure claimant_count exists
        if "claimant_count" not in llm_result["personas"]:
            llm_result["personas"]["claimant_count"] = 1
            logger.warning(f"Added missing claimant_count: 1")

        # Ensure policy_user exists
        if "policy_user" not in llm_result["personas"]:
            llm_result["personas"]["policy_user"] = "policyholder (individual)"
            logger.warning("Added missing policy_user with default value")

        # If affected_person is missing, use policy_user as a fallback
        if "affected_person" not in llm_result["personas"] and "policy_user" in llm_result["personas"]:
            llm_result["personas"]["affected_person"] = f"{llm_result['personas']['policy_user']} (inferred)"
            logger.warning("Added missing affected_person, inferred from policy_user")

        # If location is missing, infer from context or set to unknown
        if "location" not in llm_result["personas"]:
            # Try to infer from other fields
            affected_person_desc = llm_result["personas"].get("affected_person", "").lower()
            context = affected_person_desc + " " + llm_result["personas"].get("relationship", "").lower()

            if "airport" in context:
                llm_result["personas"]["location"] = "airport"
            elif "hotel" in context or "resort" in context:
                llm_result["personas"]["location"] = "hotel/resort"
            elif "hospital" in context or "clinic" in context:
                llm_result["personas"]["location"] = "hospital/medical facility"
            elif "home" in context or "house" in context:
                llm_result["personas"]["location"] = "home"
            elif any(place in context for place in ["abroad", "foreign", "overseas", "international"]):
                llm_result["personas"]["location"] = "abroad (unspecified location)"
            else:
                llm_result["personas"]["location"] = "unspecified location"

            logger.warning(f"Added missing location: {llm_result['personas']['location']}")

        # If is_abroad is missing, infer from location
        if "is_abroad" not in llm_result["personas"]:
            location = llm_result["personas"].get("location", "").lower()
            is_abroad = any(term in location for term in ["abroad", "foreign", "overseas", "international"])

            # Also check for specific country or region names that would indicate abroad
            country_indicators = ["europe", "asia", "america", "africa", "australia"]
            if any(country in location for country in country_indicators):
                is_abroad = True

            llm_result["personas"]["is_abroad"] = is_abroad
            logger.warning(f"Added missing is_abroad: {is_abroad}")

        # Add relationship_to_policyholder if missing
        if "relationship_to_policyholder" not in llm_result["personas"]:
            # If affected person is the same as policy user, relationship is "self"
            if (llm_result["personas"].get("affected_person", "").lower() ==
                    llm_result["personas"].get("policy_user", "").lower() or
                    "self" in llm_result["personas"].get("affected_person", "").lower()):
                llm_result["personas"]["relationship_to_policyholder"] = "self"
            else:
                # Otherwise infer from context
                affected = llm_result["personas"].get("affected_person", "").lower()
                if any(term in affected for term in ["spouse", "wife", "husband", "partner"]):
                    llm_result["personas"]["relationship_to_policyholder"] = "spouse/partner"
                elif any(term in affected for term in ["child", "son", "daughter"]):
                    llm_result["personas"]["relationship_to_policyholder"] = "child/dependent"
                elif any(term in affected for term in ["parent", "father", "mother"]):
                    llm_result["personas"]["relationship_to_policyholder"] = "parent"
                else:
                    llm_result["personas"]["relationship_to_policyholder"] = "unknown (inferred)"
            logger.warning(
                f"Added missing relationship_to_policyholder: {llm_result['personas']['relationship_to_policyholder']}")

        # Add is_affected_covered if missing
        if "is_affected_covered" not in llm_result["personas"]:
            # Default assumption based on relationship
            rel = llm_result["personas"].get("relationship_to_policyholder", "").lower()
            if any(r in rel for r in ["self", "spouse", "partner", "child", "dependent"]):
                llm_result["personas"]["is_affected_covered"] = True
            else:
                llm_result["personas"]["is_affected_covered"] = False
            logger.warning(f"Added missing is_affected_covered: {llm_result['personas']['is_affected_covered']}")

        return llm_result


# End of models/persona/llm_based.py
# ================================================================================

# File 28/39: models/persona/location.py
# --------------------------------------------------------------------------------

#  src/models/persona/location.py

"""
Location detection for insurance queries.
Identifies where an event occurred.
"""
import re
from typing import Tuple


class LocationDetector:
    """
    Detects location information in insurance queries.
    """

    def identify_location(self, question: str) -> Tuple[str, bool]:
        """
        Identify location information from the insurance query.

        Args:
            question: The query to analyze

        Returns:
            Tuple of (location, is_abroad)
        """
        # Common location patterns
        airport_patterns = [
            r'(?:at|in)\s+(?:the)?\s+airport',
            r'baggage.{1,30}(?:lost|delayed|missing|derouted)',
            r'luggage.{1,30}(?:lost|delayed|missing|derouted)',
            r'airport.{1,30}(?:lost|delayed|missing)',
            r'flight.{1,30}(?:lost|delayed|missing)',
            r'check-in.{1,30}(?:lost|delayed|missing)',
        ]

        hotel_patterns = [
            r'(?:at|in)\s+(?:the|my|our)?\s+hotel',
            r'(?:at|in)\s+(?:the|a)?\s+resort',
            r'staying\s+(?:at|in)',
            r'during\s+(?:my|our)\s+stay',
            r'accommodation'
        ]

        hospital_patterns = [
            r'(?:at|in)\s+(?:the|a)?\s+hospital',
            r'medical\s+(?:facility|center|centre)',
            r'emergency\s+room',
            r'clinic',
            r'doctor',
            r'medical\s+treatment'
        ]

        abroad_patterns = [
            r'(?:abroad|overseas|internationally)',
            r'(?:in|to|from|at)\s+(?!home|my home|our home)([A-Z][a-z]+)',  # Country/city names
            r'foreign\s+(?:country|place|location)',
            r'outside\s+(?:my|the|our)\s+country',
            r'international\s+(?:trip|travel|journey)',
            r'vacation\s+(?:in|to|at)'
        ]

        domestic_patterns = [
            r'(?:at|in)\s+(?:my|our)\s+home',
            r'at\s+home',
            r'domestic',
            r'within\s+(?:my|the|our)\s+country'
        ]

        transportation_patterns = [
            r'(?:on|in)\s+(?:the|a)?\s+(?:train|bus|car|taxi|subway|metro)',
            r'driving',
            r'during\s+(?:the|my|our)?\s+(?:journey|trip|travel|transit)',
            r'(?:train|bus|subway|car|taxi)\s+(?:station|terminal|stop)'
        ]

        # First check if it's abroad (this should be checked first to combine with location)
        is_abroad = any(re.search(pattern, question, re.IGNORECASE) for pattern in abroad_patterns)

        # Check for specific locations by type
        if any(re.search(pattern, question, re.IGNORECASE) for pattern in airport_patterns):
            return "airport", is_abroad
        elif any(re.search(pattern, question, re.IGNORECASE) for pattern in hotel_patterns):
            return "hotel/resort", is_abroad
        elif any(re.search(pattern, question, re.IGNORECASE) for pattern in hospital_patterns):
            return "hospital/medical facility", is_abroad
        elif any(re.search(pattern, question, re.IGNORECASE) for pattern in transportation_patterns):
            return "during transportation", is_abroad
        elif any(re.search(pattern, question, re.IGNORECASE) for pattern in domestic_patterns):
            return "at home/domestic", False  # Domestic patterns override is_abroad

        # If no specific location but abroad is detected
        if is_abroad:
            return "abroad (unspecified location)", True

        # Default if no clear patterns
        return "unspecified location", False


# End of models/persona/location.py
# ================================================================================

# File 29/39: models/persona/rule_based.py
# --------------------------------------------------------------------------------

# src/models/persona/rule_based.py

"""
Rule-based persona extraction.
Extracts persona information using pattern matching and heuristics.
"""
import re
import logging
from typing import Dict, Any, List, Tuple, Optional

from models.persona.location import LocationDetector

logger = logging.getLogger(__name__)


class RuleBasedExtractor:
    """
    Rule-based extractor for persona information.
    Uses regex patterns and heuristics to extract information.
    """

    def __init__(self):
        """Initialize the rule-based extractor."""
        self.location_detector = LocationDetector()

    def extract(self, question: str) -> Dict[str, Any]:
        """
        Extract persona information using rule-based approach.

        Args:
            question: The insurance query to analyze

        Returns:
            Dictionary with persona information
        """
        # Initialize defaults
        policy_user = None
        affected_person = None
        mentioned_people = []
        relationship = None
        total_count = 1
        claimant_count = 1

        # Extract family relationships
        found_relationships, family_mentioned = self._extract_family_relationships(question)
        mentioned_people.extend(family_mentioned)

        # Extract non-family relationships
        other_relationships, other_mentioned = self._extract_non_family_relationships(question)
        found_relationships.extend(other_relationships)
        mentioned_people.extend(other_mentioned)

        # Try to identify who experienced the event
        affected_person = self._identify_affected_person(question)

        # Extract location information
        location, is_abroad = self.location_detector.identify_location(question)

        # Handle special cases
        (special_policy_user,
         special_mentioned,
         special_relationship,
         special_total_count,
         special_claimant_count,
         special_affected_person) = self._handle_special_case_personas(question, found_relationships)

        if special_policy_user:
            policy_user = special_policy_user
        if special_mentioned:
            mentioned_people = special_mentioned
        if special_relationship:
            relationship = special_relationship
        if special_total_count:
            total_count = special_total_count
        if special_claimant_count:
            claimant_count = special_claimant_count
        if special_affected_person:
            affected_person = special_affected_person

        # If not a special case, determine defaults
        if not policy_user:
            (policy_user,
             relationship,
             default_total,
             default_claimant,
             relationship) = self._determine_default_persona_info(
                question,
                found_relationships
            )

            # Only use defaults if not set by special cases
            if not special_total_count:
                total_count = default_total
            if not special_claimant_count:
                claimant_count = default_claimant

        # If there are specific people mentioned, update total count
        if mentioned_people:
            mentioned_count = len(mentioned_people)
            total_count = max(total_count, mentioned_count)

        # If affected_person is still None, default to policy_user
        if not affected_person:
            affected_person = f"{policy_user} (inferred)"

        # Determine relationship to policyholder
        relationship_to_policyholder = self._determine_relationship_to_policyholder(policy_user, affected_person)

        # Determine if affected person is likely covered
        is_affected_covered = self._is_likely_covered(relationship_to_policyholder)

        # Format the result
        return {
            "personas": {
                "policy_user": policy_user,
                "affected_person": affected_person,
                "location": location,
                "is_abroad": is_abroad,
                "relationship_to_policyholder": relationship_to_policyholder,
                "is_affected_covered": is_affected_covered,
                "mentioned_people": ", ".join(mentioned_people) if mentioned_people else "None specifically mentioned",
                "total_count": total_count,
                "claimant_count": claimant_count,
                "relationship": relationship if relationship else "Not clearly specified"
            }
        }

    def _extract_family_relationships(self, question: str) -> Tuple[List[str], List[str]]:
        """
        Extract family relationship terms from the question.

        Args:
            question: The question to analyze

        Returns:
            Tuple of (found_relationships, mentioned_people)
        """
        found_relationships = []
        mentioned_people = []

        # Family relationship terms
        family_terms = {
            'daughter': 'daughter',
            'son': 'son',
            'child': 'child',
            'children': 'children',
            'wife': 'wife',
            'husband': 'husband',
            'spouse': 'spouse',
            'partner': 'partner',
            'mother': 'mother',
            'father': 'father',
            'parent': 'parent',
            'parents': 'parents',
            'grandparent': 'grandparent',
            'grandmother': 'grandmother',
            'grandfather': 'grandfather',
            'sister': 'sister',
            'brother': 'brother',
            'sibling': 'sibling',
            'aunt': 'aunt',
            'uncle': 'uncle',
            'cousin': 'cousin',
            'niece': 'niece',
            'nephew': 'nephew',
            'family': 'family member'
        }

        # Patterns for indirect references
        indirect_references = [
            r'my\s+(?:business\s+)?partner\s+had',
            r'my\s+(?:family\s+)?member\s+(?:had|was|got)',
            r'my\s+(?:colleague|coworker)\s+(?:had|was|got)',
            r'(?:death|illness|injury)\s+of\s+(?:my|the)',
        ]

        # Check for family members and relationships
        for term, relationship_type in family_terms.items():
            if re.search(r'\b' + term + r'\b', question, re.IGNORECASE):
                found_relationships.append(relationship_type)

                # Check if this is mentioned as a non-claimant
                is_indirect = any(
                    re.search(pattern + r'.*\b' + term + r'\b', question, re.IGNORECASE)
                    for pattern in indirect_references
                )

                # Add to mentioned people
                if is_indirect:
                    mentioned_people.append(f"{relationship_type} (not a claimant)")
                else:
                    mentioned_people.append(relationship_type)

        return found_relationships, mentioned_people

    def _extract_non_family_relationships(self, question: str) -> Tuple[List[str], List[str]]:
        """
        Extract non-family relationship terms from the question.

        Args:
            question: The question to analyze

        Returns:
            Tuple of (found_relationships, mentioned_people)
        """
        found_relationships = []
        mentioned_people = []

        # Non-family relationship terms
        other_terms = {
            'friend': 'friend',
            'friends': 'friends',
            'colleague': 'colleague',
            'coworker': 'coworker',
            'business partner': 'business partner',
            'neighbor': 'neighbor',
            'guest': 'guest',
            'traveler': 'fellow traveler'
        }

        # Patterns for indirect references
        indirect_references = [
            r'my\s+(?:business\s+)?partner\s+had',
            r'my\s+(?:family\s+)?member\s+(?:had|was|got)',
            r'my\s+(?:colleague|coworker)\s+(?:had|was|got)',
            r'(?:death|illness|injury)\s+of\s+(?:my|the)',
        ]

        # Check for non-family relationships
        for term, relationship_type in other_terms.items():
            if re.search(r'\b' + term + r'\b', question, re.IGNORECASE):
                found_relationships.append(relationship_type)

                # Check if this is mentioned as a non-claimant
                is_indirect = any(
                    re.search(pattern + r'.*\b' + term + r'\b', question, re.IGNORECASE)
                    for pattern in indirect_references
                )

                # Add to mentioned people
                if is_indirect:
                    mentioned_people.append(f"{relationship_type} (not a claimant)")
                else:
                    mentioned_people.append(relationship_type)

        return found_relationships, mentioned_people

    def _count_people_from_numeric_mentions(self, question: str) -> int:
        """
        Extract explicit numeric mentions of people in the question.

        Args:
            question: The question to analyze

        Returns:
            The explicitly mentioned count of people, or 0 if none found
        """
        # Pattern for phrases like "family of X", "group of X", "X of us", etc.
        patterns = [
            r'family\s+of\s+(\d+)',
            r'group\s+of\s+(\d+)',
            r'(\d+)\s+of\s+us',
            r'(\d+)\s+people',
            r'(\d+)\s+persons',
            r'(\d+)\s+travelers',
            r'(\d+)\s+members',
            r'for\s+(\d+)'
        ]

        # Look for matches
        for pattern in patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            if matches:
                # Return the first numeric match as an integer
                try:
                    return int(matches[0])
                except (ValueError, IndexError):
                    continue

        return 0  # No explicit numeric mentions found

    def _count_people_from_pronouns(self, question: str) -> int:
        """
        Count distinct people based on pronoun usage, with improved contextual understanding.

        Args:
            question: The question to analyze

        Returns:
            Estimated count of distinct people
        """
        # Check if "our" is being used in institutional context rather than indicating multiple people
        institutional_our_pattern = r'\b(our|at our) (hotel|resort|company|office|facility|premises|building|property|organization|institution)\b'
        has_institutional_our = bool(re.search(institutional_our_pattern, question, re.IGNORECASE))

        # Check for different types of pronouns
        has_first_person_singular = bool(re.search(r'\b(I|me|my)\b', question, re.IGNORECASE))

        # Modified first person plural check to exclude institutional "our"
        has_first_person_plural = bool(re.search(r'\b(we|us)\b', question, re.IGNORECASE))
        if not has_first_person_plural and re.search(r'\bour\b', question, re.IGNORECASE) and not has_institutional_our:
            has_first_person_plural = True

        has_second_person = bool(re.search(r'\b(you|your)\b', question, re.IGNORECASE))
        has_third_person_singular_male = bool(re.search(r'\b(he|him|his)\b', question, re.IGNORECASE))
        has_third_person_singular_female = bool(re.search(r'\b(she|her)\b', question, re.IGNORECASE))
        has_third_person_plural = bool(re.search(r'\b(they|them|their)\b', question, re.IGNORECASE))

        # Count distinct people based on pronoun types
        distinct_people_count = 0

        if has_first_person_singular:
            distinct_people_count += 1  # The speaker/policy holder

        if has_first_person_plural and not has_institutional_our:
            # 'We' implies at least 2 people, but only if not institutional
            distinct_people_count = max(distinct_people_count, 2)

        # Only count 'you' as another person when it's clearly referring to a different individual
        # and not the customer service or entity being addressed
        if has_second_person and re.search(r'\b(you|your) (?:and|with|also|too)\b', question, re.IGNORECASE):
            distinct_people_count += 1

        # More contextual checks for second person
        # Don't count 'you' in service requests like "can you help"
        if has_second_person and not re.search(r'(?:can|could|would|will|please)\s+you', question, re.IGNORECASE):
            distinct_people_count += 1

        if has_third_person_singular_male:
            distinct_people_count += 1

        if has_third_person_singular_female:
            distinct_people_count += 1

        if has_third_person_plural:
            # 'They' implies at least 2 more people
            if distinct_people_count == 0:
                distinct_people_count = 2  # Minimum for 'they'
            else:
                distinct_people_count += 1  # Add at least one more person

        # If no pronouns were found, default to 1 person (the claimant)
        if distinct_people_count == 0:
            distinct_people_count = 1

        return distinct_people_count

    def _determine_default_persona_info(
            self,
            question: str,
            found_relationships: List[str]
    ) -> Tuple[str, str, int, int, str]:
        """
        Determine default persona information based on pronouns and context.

        Args:
            question: The question to analyze
            found_relationships: List of relationships found in the question

        Returns:
            Tuple of (policy_user, relationship, total_count, claimant_count, relationship)
        """
        # Default based on pronouns
        if re.search(r'\b(I|me|my)\b', question, re.IGNORECASE) and not found_relationships:
            policy_user = "policyholder (individual)"
            relationship = "none mentioned"
            claimant_count = 1

        elif re.search(r'\b(we|us|our)\b', question, re.IGNORECASE) and not found_relationships:
            policy_user = "policyholder (group)"
            relationship = "group (unspecified)"
            claimant_count = 2  # At least 2 people

        elif found_relationships:
            # If relationships found but no clear policy user, assume relationship is claimant
            policy_user = found_relationships[0]
            relationship = "family member of policyholder"
            claimant_count = 1

        else:
            # Default if nothing could be determined
            policy_user = "undetermined"
            relationship = "Not clearly specified"
            claimant_count = 1

        # Get people count from pronouns
        pronoun_count = self._count_people_from_pronouns(question)

        # Get explicit numeric mentions of people count
        numeric_count = self._count_people_from_numeric_mentions(question)

        # Use the larger of the two counts
        total_count = max(pronoun_count, numeric_count)

        # If we have an explicit numeric count and we're in a family/group context,
        # update the claimant count accordingly
        if numeric_count > 0 and re.search(r'\b(we|us|our|family|group)\b', question, re.IGNORECASE):
            claimant_count = numeric_count

        return policy_user, relationship, total_count, claimant_count, relationship


    def _identify_affected_person(self, question: str) -> Optional[str]:
        """
        Identify who experienced the event/accident/illness in the query.

        Args:
            question: The query to analyze

        Returns:
            String describing who experienced the event or None if unclear
        """
        # Patterns for first-person event experiencing
        first_person_patterns = [
            r'I\s+(?:had|have|got|experienced|suffered|am suffering|was diagnosed with|developed)\s+(?:a|an)?\s*(?:illness|sickness|disease|condition|injury|accident|problem)',
            r'I\s+(?:broke|injured|hurt|damaged|lost)\s+my',
            r'I\s+(?:am|was|feel|felt)\s+(?:sick|ill|unwell|not well|injured)',
            r'my\s+(?:illness|sickness|disease|condition|injury|accident|problem)',
            r'I\s+need\s+(?:a|to see)\s+(?:doctor|medical|physician|hospital)',
            r'I\s+(?:had|have)\s+(?:pain|discomfort|symptoms)',
        ]

        # Patterns for family members experiencing events
        family_patterns = {
            'child': r'(?:my|our)\s+(?:child|kid|son|daughter)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'spouse': r'(?:my|our)\s+(?:spouse|husband|wife|partner)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'parent': r'(?:my|our)\s+(?:parent|father|mother|dad|mom)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'family': r'(?:my|our)\s+(?:family member|relative|brother|sister|sibling|uncle|aunt|cousin|nephew|niece)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
        }

        # Check for first person as affected
        for pattern in first_person_patterns:
            if re.search(pattern, question, re.IGNORECASE):
                return "policyholder (self)"

        # Check for family members as affected
        for family_type, pattern in family_patterns.items():
            if re.search(pattern, question, re.IGNORECASE):
                return f"{family_type} of policyholder"

        # Check for others as affected
        other_patterns = {
            'friend': r'(?:my|our)\s+friend\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'colleague': r'(?:my|our)\s+(?:colleague|coworker|co-worker)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'travel companion': r'(?:my|our)\s+(?:travel companion|fellow traveler|traveling partner)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
        }

        for other_type, pattern in other_patterns.items():
            if re.search(pattern, question, re.IGNORECASE):
                return f"{other_type} of policyholder"

        # Default to None if no clear matches
        return None

    def _handle_special_case_personas(
            self,
            question: str,
            found_relationships: List[str]
    ) -> Tuple[Optional[str], Optional[List[str]], Optional[str], Optional[int], Optional[int], Optional[str]]:
        """
        Handle special case persona scenarios, now including affected person.

        Args:
            question: The question to analyze
            found_relationships: List of relationships found in the question

        Returns:
            Tuple of (policy_user, mentioned_people, relationship, total_count, claimant_count, affected_person)
            Returns None for any values that couldn't be determined by special cases
        """
        policy_user = None
        mentioned_people = None
        relationship = None
        total_count = None
        claimant_count = None
        affected_person = None

        # Define family relation terms for consistent use
        family_terms = [
            'daughter', 'son', 'child', 'children', 'wife', 'husband', 'spouse', 'partner',
            'mother', 'father', 'parent', 'parents', 'grandparent', 'grandmother', 'grandfather',
            'sister', 'brother', 'sibling', 'aunt', 'uncle', 'cousin', 'niece', 'nephew',
            'family', 'relative'
        ]

        # Create regex pattern for family relationships
        family_pattern = r'\b(' + '|'.join(family_terms) + r')\b'

        # Special case: business partner scenario
        if re.search(r'my\s+(?:business\s+)?partner\s+had', question, re.IGNORECASE):
            if re.search(r'I\s+am\s+traveling\s+alone', question, re.IGNORECASE):
                policy_user = "policyholder (individual traveler)"
                mentioned_people = ["business partner (not traveling/insured)"]
                relationship = "business relationship (but not traveling together)"
                total_count = 2
                claimant_count = 1
                affected_person = "business partner (not a claimant)"
            else:
                policy_user = "policyholder (individual)"
                mentioned_people = ["business partner"]
                relationship = "business relationship"
                total_count = 2
                claimant_count = 1
                affected_person = "business partner"

        # Special case: my family member scenarios (using any family term)
        elif re.search(r'\bmy\s+' + family_pattern, question, re.IGNORECASE):
            policy_user = "policyholder (family member)"
            relationship = "family - " + ", ".join(found_relationships)

            # Check if family member is experiencing the event
            affected_match = re.search(
                r'my\s+(' + '|'.join(family_terms) + r')\s+(?:is|was|had|got|needs|has been|suffered|experienced)',
                question, re.IGNORECASE)
            if affected_match:
                affected_person = affected_match.group(1)

            # Extract all mentioned family members
            all_family_matches = re.findall(r'my\s+(' + '|'.join(family_terms) + r')', question, re.IGNORECASE)
            mentioned_people = [match for match in all_family_matches if match]

            # Check for explicit family size
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0

            if family_size > 0:
                total_count = family_size
                # Check if all family members are claiming
                if re.search(r'(?:all|everyone|each|the whole family|all \d+ of us|for all of us)', question,
                             re.IGNORECASE):
                    claimant_count = family_size
                else:
                    # Check if only family member is a claimant or multiple
                    if affected_person and not re.search(r'(?:all|everyone|each|all \d+ of us)', question,
                                                         re.IGNORECASE):
                        claimant_count = 1  # Just the affected family member
                    else:
                        claimant_count = min(family_size, len(mentioned_people) + 1)  # +1 for policyholder
            else:
                # No explicit family size
                total_count = max(2, len(mentioned_people) + 1)  # At least policyholder + mentioned people
                claimant_count = 1 if affected_person else total_count  # Either just affected or all

        # Special case: our family member scenario (using any family term)
        elif re.search(r'\bour\s+' + family_pattern, question, re.IGNORECASE):
            policy_user = "policyholder (joint policy with family)"
            relationship = "family - " + ", ".join(found_relationships)

            # Check for explicit family size
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0

            # Check who is actually affected/claiming
            affected_terms = [
                'all of us', 'everyone', 'the whole family',
                r'all \d+ of us', r'for the \d+ of us', 'for all of us'
            ]

            # Extract the specific affected family member if mentioned
            affected_match = re.search(r'(?:my|our)\s+(' + '|'.join(
                family_terms) + r')\s+(?:is|was|had|got|needs|has been|suffered|experienced)', question, re.IGNORECASE)
            if affected_match:
                affected_person = affected_match.group(1)

            # If the question indicates everyone is affected/claiming
            everyone_affected = any(re.search(pattern, question, re.IGNORECASE) for pattern in affected_terms)

            if family_size > 0:
                total_count = family_size

                # Determine claimant count based on context
                if everyone_affected:
                    claimant_count = family_size
                    # If we found a specific affected person but everyone is claiming
                    if affected_person:
                        affected_person = f"{affected_person} (but whole family of {family_size} is claiming)"
                elif affected_person:
                    # Check if only the affected person is claiming
                    if re.search(r'(?:only|just)\s+(?:my|our|the)\s+' + affected_person, question, re.IGNORECASE):
                        claimant_count = 1
                    else:
                        # If reimbursement is for all
                        if re.search(r'reimburse\s+(?:the|our|all).*(trip|vacation|holiday)', question, re.IGNORECASE):
                            claimant_count = family_size
                            affected_person = f"{affected_person} (but whole family of {family_size} is claiming)"
                        else:
                            claimant_count = 1
                else:
                    # Default to all family members if no specific affected person
                    claimant_count = family_size
            else:
                # No explicit family size, use "we/our" to imply at least 2
                total_count = max(2, len(found_relationships) + 1)
                claimant_count = 1 if affected_person and not everyone_affected else total_count

        # Special case: we/family scenario without explicit "our family" phrase
        elif re.search(r'\b(we|us)\b', question, re.IGNORECASE) and any(
                term in question.lower() for term in family_terms):
            policy_user = "policyholder (family group)"
            relationship = "family group"

            # Extract all mentioned family members
            all_family_matches = re.findall(r'(?:the|my|our)\s+(' + '|'.join(family_terms) + r')', question,
                                            re.IGNORECASE)
            mentioned_people = [match for match in all_family_matches if match]

            # Check for explicit family size
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0

            # Look for affected person
            affected_match = re.search(r'(?:my|our|the)\s+(' + '|'.join(
                family_terms) + r')\s+(?:is|was|had|got|needs|has been|suffered|experienced)', question, re.IGNORECASE)
            if affected_match:
                affected_person = affected_match.group(1)

            # Determine counts
            if family_size > 0:
                total_count = family_size

                # Check if everyone is claiming
                if re.search(r'(?:reimburse|refund|cover)\s+(?:the|our|all|us|everyone).*(trip|vacation|holiday|stay)',
                             question, re.IGNORECASE):
                    claimant_count = family_size
                    # If specific person affected but all claiming
                    if affected_person:
                        affected_person = f"{affected_person} (but all {family_size} travelers claiming)"
                elif affected_person:
                    # Default to just affected person claiming unless specified
                    claimant_count = 1
                else:
                    claimant_count = family_size
            else:
                # No explicit size, determine from context
                total_count = max(2, len(mentioned_people) + 1)  # At least 2 for "we"
                claimant_count = 1 if affected_person else total_count

        # Special case: traveling alone but mentioning business partner
        elif re.search(r'I\s+am\s+traveling\s+alone', question, re.IGNORECASE) and 'partner' in question.lower():
            policy_user = "policyholder (individual traveler)"
            mentioned_people = ["business partner (not traveling/insured)"]
            relationship = "business relationship"
            total_count = 2
            claimant_count = 1
            affected_person = "policyholder (self)"

        # Special case: personal item theft or loss
        elif re.search(r'(my|I).*(handbag|purse|bag|phone|document|wallet|passport|luggage)', question, re.IGNORECASE):
            policy_user = "policyholder (individual)"
            mentioned_people = []
            relationship = "none mentioned"

            # Check for group/family mentions
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0
            if family_size > 0 and re.search(r'\b(we|us|our|family)\b', question, re.IGNORECASE):
                total_count = family_size

                # Check if other people's items are also affected
                if re.search(r'(?:our|all our|everyone\'s).*(handbag|purse|bag|phone|document|wallet|passport|luggage)',
                             question, re.IGNORECASE):
                    claimant_count = family_size
                    affected_person = "entire group's belongings"
                else:
                    claimant_count = 1  # Usually only the owner of the item is claiming
                    affected_person = "policyholder's belongings"
            else:
                total_count = 1
                claimant_count = 1
                affected_person = "policyholder (self)"

        # Special case: illness or medical condition
        elif re.search(r'(I had|I got|I am|I was).*?(sick|ill|poisoning|diarrhea|vomiting|stomach|pain|fever|injured)',
                       question, re.IGNORECASE):
            policy_user = "policyholder (individual)"
            mentioned_people = []
            relationship = "none mentioned"

            # Check for group/family mentions
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0
            if family_size > 0 and re.search(r'\b(we|us|our|family)\b', question, re.IGNORECASE):
                total_count = family_size

                # Check if only the person is sick or everyone
                if re.search(r'(we all|all of us).*?(sick|ill|injured|affected)', question, re.IGNORECASE):
                    claimant_count = family_size
                    affected_person = "entire family/group"
                else:
                    # Check for trip cancellation for everyone
                    if re.search(
                            r'(?:reimburse|refund|cover)\s+(?:the|our|all|us|everyone).*(trip|vacation|holiday|stay)',
                            question, re.IGNORECASE):
                        claimant_count = family_size
                        affected_person = "policyholder (but whole group of " + str(family_size) + " is claiming)"
                    else:
                        claimant_count = 1
                        affected_person = "policyholder (self)"
            else:
                total_count = 1
                claimant_count = 1
                affected_person = "policyholder (self)"

        return policy_user, mentioned_people, relationship, total_count, claimant_count, affected_person

    def _determine_relationship_to_policyholder(self, policy_user: str, affected_person: str) -> str:
        """
        Determine the relationship between the affected person and policyholder.

        Args:
            policy_user: The identified policy user
            affected_person: The identified affected person

        Returns:
            String describing relationship
        """
        # If they appear to be the same person
        if policy_user.lower() in affected_person.lower() or affected_person.lower() in policy_user.lower():
            return "self"

        # Check for keywords in the affected person description
        affected_lower = affected_person.lower()
        if any(term in affected_lower for term in ["spouse", "wife", "husband", "partner"]):
            return "spouse/partner"
        elif any(term in affected_lower for term in ["child", "son", "daughter"]):
            return "child/dependent"
        elif any(term in affected_lower for term in ["parent", "father", "mother"]):
            return "parent"
        elif any(term in affected_lower for term in ["brother", "sister", "sibling"]):
            return "sibling"
        elif any(term in affected_lower for term in ["friend", "colleague", "coworker"]):
            return "non-family relation"
        elif "family" in affected_lower:
            return "family member"

        # Default if no clear relationship
        return "unknown"

    def _is_likely_covered(self, relationship: str) -> bool:
        """
        Determine if the affected person is likely covered based on their relationship.

        Args:
            relationship: Relationship to policyholder

        Returns:
            Boolean indicating likely coverage
        """
        # These relationships are typically covered
        covered_relationships = [
            "self", "spouse/partner", "child/dependent", "family member"
        ]

        # Check if relationship is in typically covered list
        return relationship in covered_relationships


# End of models/persona/rule_based.py
# ================================================================================

# File 30/39: models/qwen_model.py
# --------------------------------------------------------------------------------

# src/models/qwen_model.py

"""
Qwen model client implementation for insurance policy analysis.
Based on the HuggingFace model client but optimized for Qwen models.
"""
import logging
import os
import re
from typing import Dict, Any

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from config import get_local_model_path, get_model_config
from models.hf_model import HuggingFaceModelClient
from models.json_utils.extractors import JSONExtractor
from models.persona.extractor import PersonaExtractor

logger = logging.getLogger(__name__)


class QwenModelClient(HuggingFaceModelClient):
    """
    Specialized client for Qwen models.
    Inherits from HuggingFaceModelClient but with Qwen-specific optimizations.
    """

    def __init__(self, model_name: str, sys_prompt: str):
        """
        Initialize a Qwen model client.

        Args:
            model_name: Name of the Qwen model to use
            sys_prompt: System prompt for the model
        """
        logger.info(f"Loading Qwen model: {model_name}")
        self._log_cache_locations()

        # Check for local model first
        model_path = get_local_model_path(model_name)
        self.model_config = get_model_config(model_name)

        self.pipe = self._initialize_qwen_pipeline(model_path)
        self.base_prompt = sys_prompt.strip()

        self.persona_extractor = PersonaExtractor(self.pipe)
        self.json_extractor = JSONExtractor()

    def _check_qwen_model_in_scratch(self, model_name: str) -> str:
        """Check if Qwen model exists in scratch directory."""
        # Support both 32B and 7B models
        model_mappings = {
            "Qwen/Qwen2.5-32B": "qwen2.5-32b",
            "qwen2.5-32b": "qwen2.5-32b",
            "Qwen/Qwen2.5-7B": "qwen2.5-7b",  # Add 7B support
            "qwen2.5-7b": "qwen2.5-7b"  # Add 7B support
        }

        if model_name in model_mappings:
            scratch_model_path = os.path.join(
                "/cluster/scratch",
                os.environ.get("USER", ""),
                "models",
                model_mappings[model_name]
            )

            if os.path.exists(scratch_model_path):
                logger.info(f"Found Qwen model in scratch: {scratch_model_path}")
                return scratch_model_path
            else:
                logger.warning(f"Qwen model not found in scratch directory.")

        return model_name

    def _initialize_qwen_pipeline(self, model_path: str):
        """Initialize the Qwen pipeline with optimized settings."""
        try:
            logger.info(f"Initializing Qwen pipeline with model: {model_path}")

            # Load tokenizer with Qwen-specific settings
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="left"  # Qwen often works better with left padding
            )

            # Ensure pad token is set
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # Load model with Qwen-optimized settings
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=self.model_config["torch_dtype"],
                device_map=self.model_config["device_map"],
                trust_remote_code=self.model_config["trust_remote_code"],
                low_cpu_mem_usage=self.model_config["low_cpu_mem_usage"]
            )

            # Create pipeline
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer
            )

            logger.info("Qwen pipeline successfully initialized")
            return pipe

        except Exception as e:
            logger.error(f"Error initializing Qwen pipeline: {e}")
            raise

    def _generate_model_output(self, prompt: str) -> Dict[str, Any]:
        """
        Generate output from Qwen model with optimized parameters.
        """
        try:
            # Qwen-specific generation parameters
            generation_params = {
                "max_new_tokens": self.model_config.get("max_new_tokens", 512),
                "temperature": None,
                "do_sample": self.model_config.get("do_sample", False),
                "repetition_penalty": self.model_config.get("repetition_penalty", 1.1),
                "num_return_sequences": 1,
                "return_full_text": False,
                "pad_token_id": self.pipe.tokenizer.eos_token_id,
                "eos_token_id": self.pipe.tokenizer.eos_token_id,
                "top_p": None,
                "top_k": None,
                "no_repeat_ngram_size": self.model_config.get("no_repeat_ngram_size", 3),
            }

            # Generate text
            outputs = self.pipe(prompt, **generation_params)

            # Get the generated text
            generated = outputs[0]["generated_text"]

            generated = self._clean_qwen_output(generated)
            logger.debug(f"Qwen model output: {generated}")

            # Try to extract JSON from the output
            extracted_json = self.json_extractor.extract_json(generated)

            if extracted_json:
                return extracted_json

            # If extraction fails, return formatted error
            logger.error("Failed to extract valid JSON from Qwen model output")
            return self._create_error_response("Failed to extract valid JSON from model output")

        except Exception as e:
            error_msg = f"Error during Qwen model inference: {str(e)}"
            logger.error(error_msg)
            return self._create_error_response(error_msg)

    def _clean_qwen_output(self, text: str) -> str:
        """
        Clean Qwen output to improve JSON extraction.
        """
        # Remove thinking tags
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

        # Remove any explanatory text before JSON
        lines = text.split('\n')
        json_started = False
        cleaned_lines = []

        for line in lines:
            if '{' in line and not json_started:
                json_started = True
            if json_started:
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines) if cleaned_lines else text

    def _build_prompt(self, question: str, context_text: str, persona_text: str) -> str:
        """
        Build the full prompt for Qwen model using proper chat template.
        """
        # Build user content - NO JSON schema
        if persona_text:
            user_content = f"{context_text}\n\nQuestion: {question}\n\n{persona_text}"
        else:
            user_content = f"{context_text}\n\nQuestion: {question}"

        # Use Qwen's apply_chat_template if available
        if hasattr(self.pipe.tokenizer, 'apply_chat_template'):
            try:
                messages = [
                    # {"role": "system", "content": },
                    {"role": "user", "content": (self.base_prompt + user_content)}
                ]

                formatted_prompt = self.pipe.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=True
                )
                logger.debug(f"Qwen chat template prompt: {formatted_prompt}")
                return formatted_prompt

            except Exception as e:
                logger.warning(f"Failed to use chat template: {e}, falling back to manual formatting")

        return ""



# End of models/qwen_model.py
# ================================================================================

# File 31/39: models/shared_client.py
# --------------------------------------------------------------------------------

# src/models/shared_client.py

"""
Shared model client for memory-efficient relevance filtering.
Uses the same model pipeline with different prompts to avoid loading multiple models.
"""
import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)


class SharedModelClient:
    """
    A wrapper that uses the same model pipeline with different prompts.
    This avoids loading multiple model instances and saves GPU memory.
    """

    def __init__(self, base_client, relevance_prompt: str):
        """
        Initialize the shared model client.

        Args:
            base_client: The main model client (HuggingFaceModelClient or OpenAIModelClient)
            relevance_prompt: The prompt to use for relevance filtering
        """
        self.base_client = base_client
        self.relevance_prompt = relevance_prompt
        self.original_prompt = base_client.base_prompt
        self.original_json_format = getattr(base_client, 'json_format', None)
        logger.info(f"Initialized SharedModelClient for relevance filtering")

    def query(self, question: str, context_files: List[str]) -> Dict[str, Any]:
        """
        Query the model using the relevance prompt.
        Temporarily switches the base client's prompt, runs the query, then restores.

        Args:
            question: The question to analyze for relevance
            context_files: List of relevant policy files

        Returns:
            Dictionary containing relevance analysis result
        """
        # Store original values
        original_prompt = self.base_client.base_prompt
        original_json_format = getattr(self.base_client, 'json_format', None)

        try:
            # Temporarily switch to relevance prompt
            self.base_client.base_prompt = self.relevance_prompt.strip()

            # Also temporarily switch JSON format if the base client has one
            if hasattr(self.base_client, 'json_format'):
                self.base_client.json_format = """
                    Return exactly this JSON:
                    {
                      "is_relevant": true/false,
                      "reason": "Brief explanation (â‰¤ 25 words)"
                    }
                    """

            logger.debug(f"Switched to relevance prompt for query: {question}")

            # Query with relevance prompt (persona extraction not needed for relevance checks)
            result = self.base_client.query(question, context_files, use_persona=False)

            logger.debug(f"Relevance filtering result: {result}")
            return result

        except Exception as e:
            logger.error(f"Error in shared model relevance query: {str(e)}")
            # Return a safe default that assumes relevance
            return {
                "is_relevant": True,
                "reason": f"Error in relevance check: {str(e)}"
            }
        finally:
            # Always restore original values
            self.base_client.base_prompt = original_prompt
            if hasattr(self.base_client, 'json_format') and original_json_format is not None:
                self.base_client.json_format = original_json_format
            logger.debug("Restored original prompt and JSON format after relevance check")


# End of models/shared_client.py
# ================================================================================

# File 32/39: models/vector_store.py
# --------------------------------------------------------------------------------

# models/vector_store.py (Enhanced Version)

import logging
import re
import numpy as np
import os
from typing import List, Dict, Any, Optional, Union
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader

# Import chunking components
from models.chunking.base import ChunkingStrategy
from models.chunking.factory import ChunkingFactory, create_preset_strategy, auto_register_strategies

logger = logging.getLogger(__name__)


class EnhancedLocalVectorStore:
    """
    Enhanced vector store with pluggable chunking strategies.

    Supports multiple chunking approaches through the Strategy pattern,
    making it easy to test and compare different chunking methods.
    """

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 chunking_strategy: str = "simple",
                 chunking_config: Optional[Dict[str, Any]] = None):
        """
        Initialize enhanced vector store.

        Args:
            model_name: Sentence transformer model for embeddings
            chunking_strategy: Strategy name ('simple', 'structural', 'semantic', etc.)
            chunking_config: Optional configuration for the chunking strategy
        """
        logger.info(f"Initializing EnhancedLocalVectorStore with {chunking_strategy} chunking")

        # Auto-register strategies on first use
        auto_register_strategies()

        # Initialize sentence transformer
        try:
            self.model = SentenceTransformer(model_name)
            logger.info(f"Successfully loaded sentence-transformers model")
        except Exception as e:
            logger.error(f"Error loading embedding model {model_name}: {e}")
            raise RuntimeError(f"Failed to load embedding model: {e}")

        # Initialize chunking strategy
        self.chunking_strategy = self._create_chunking_strategy(
            chunking_strategy, chunking_config
        )

        # Storage for embeddings and chunks
        self.embeddings = None
        self.text_chunks = []
        self.chunk_metadata = []
        self.indexed_files = {}

        logger.info(f"EnhancedLocalVectorStore initialized successfully")

    def _create_chunking_strategy(self, strategy_name: str,
                                  config: Optional[Dict[str, Any]]) -> ChunkingStrategy:
        """Create the chunking strategy."""
        try:
            # Check if it's a preset
            if strategy_name in ['fast', 'balanced', 'comprehensive', 'research']:
                return create_preset_strategy(strategy_name)
            else:
                return ChunkingFactory.create_strategy(strategy_name, config)
        except Exception as e:
            logger.warning(f"Failed to create {strategy_name} strategy: {e}")
            logger.info("Falling back to simple chunking strategy")
            return ChunkingFactory.create_strategy('simple', {'max_length': 512})

    def get_chunking_info(self) -> Dict[str, Any]:
        """Get information about the current chunking strategy."""
        strategy_info = self.chunking_strategy.get_strategy_info()
        strategy_info.update({
            'total_chunks': len(self.text_chunks),
            'total_files_indexed': len(self.indexed_files),
            'has_embeddings': self.embeddings is not None,
            'embedding_model': self.model.get_sentence_embedding_dimension()
        })
        return strategy_info

    def switch_chunking_strategy(self, new_strategy: str,
                                 config: Optional[Dict[str, Any]] = None,
                                 reindex: bool = False):
        """
        Switch to a different chunking strategy.

        Args:
            new_strategy: Name of the new strategy
            config: Optional configuration for the new strategy
            reindex: Whether to re-index existing files with new strategy
        """
        logger.info(f"Switching from {self.chunking_strategy.name} to {new_strategy}")

        old_strategy = self.chunking_strategy
        self.chunking_strategy = self._create_chunking_strategy(new_strategy, config)

        if reindex and self.indexed_files:
            logger.info("Re-indexing files with new chunking strategy")
            file_paths = list(self.indexed_files.keys())
            self.index_documents(file_paths)
        else:
            logger.info("Strategy switched. Use reindex=True to re-process existing files.")

    def extract_policy_id(self, path: str) -> str:
        """Extract policy ID from filename."""
        filename = os.path.basename(path)
        match = re.match(r'^(\d+)_', filename)
        if match:
            return match.group(1)
        else:
            logger.warning(f"Could not extract policy ID from filename: {filename}")
            return os.path.splitext(filename)[0]

    def get_file_type(self, path: str) -> str:
        """Determine file type based on extension."""
        _, ext = os.path.splitext(path.lower())
        if ext == '.pdf':
            return 'pdf'
        elif ext == '.txt':
            return 'txt'
        else:
            return 'unknown'

    def extract_text_from_file(self, path: str) -> str:
        """Extract text from a file based on its type."""
        file_type = self.get_file_type(path)

        if file_type == 'pdf':
            return self._extract_text_from_pdf(path)
        elif file_type == 'txt':
            return self._extract_text_from_txt(path)
        else:
            logger.warning(f"Unsupported file type for {path}. Supported types: PDF, TXT")
            return ""

    def _extract_text_from_pdf(self, path: str) -> str:
        """Extract text from a PDF file."""
        try:
            reader = PdfReader(path)
            return "\n".join([page.extract_text() or "" for page in reader.pages])
        except Exception as e:
            logger.error(f"Error extracting text from PDF {path}: {e}")
            return ""

    def _extract_text_from_txt(self, path: str, encoding: str = 'utf-8') -> str:
        """Extract text from a TXT file."""
        try:
            with open(path, 'r', encoding=encoding) as file:
                return file.read()
        except UnicodeDecodeError:
            # Try with different encodings if utf-8 fails
            for fallback_encoding in ['latin-1', 'cp1252', 'iso-8859-1']:
                try:
                    with open(path, 'r', encoding=fallback_encoding) as file:
                        logger.info(f"Successfully read {path} using {fallback_encoding} encoding")
                        return file.read()
                except UnicodeDecodeError:
                    continue
            logger.error(f"Could not decode {path} with any common encoding")
            return ""
        except Exception as e:
            logger.error(f"Error extracting text from TXT {path}: {e}")
            return ""

    def index_documents(self, file_paths: Union[List[str], str]):
        """Index documents using the current chunking strategy."""
        if isinstance(file_paths, str):
            file_paths = [file_paths]

        logger.info(f"Indexing {len(file_paths)} documents using {self.chunking_strategy.name} strategy")

        # Reset storage
        self.text_chunks = []
        self.chunk_metadata = []
        self.indexed_files = {}

        all_chunk_results = []

        for path in file_paths:
            try:
                policy_id = self.extract_policy_id(path)
                file_type = self.get_file_type(path)
                logger.info(f"Processing {file_type.upper()} document: {path}")

                text = self.extract_text_from_file(path)
                if not text:
                    logger.warning(f"No text extracted from {path}")
                    continue

                # Use chunking strategy to process text
                chunk_results = self.chunking_strategy.chunk_text(
                    text=text,
                    policy_id=policy_id,
                    max_length=512  # Default, strategy may ignore
                )

                # Store file information
                self.indexed_files[path] = {
                    'policy_id': policy_id,
                    'file_type': file_type,
                    'chunk_count': len(chunk_results),
                    'strategy_used': self.chunking_strategy.name
                }

                all_chunk_results.extend(chunk_results)

                logger.info(f"Created {len(chunk_results)} chunks for policy {policy_id}")

            except Exception as e:
                logger.error(f"Error processing document {path}: {e}")

        if not all_chunk_results:
            logger.warning("No chunks created from documents")
            return

        # Extract texts and metadata
        self.text_chunks = self.chunking_strategy.get_chunk_text_list(all_chunk_results)
        self.chunk_metadata = self.chunking_strategy.get_metadata_list(all_chunk_results)

        logger.info(f"Creating embeddings for {len(self.text_chunks)} chunks")
        self.embeddings = self.embed(self.text_chunks)

        logger.info(f"Successfully indexed {len(self.text_chunks)} chunks using {self.chunking_strategy.name} strategy")

    def embed(self, texts: List[str]) -> np.ndarray:
        """Create embeddings for a list of text chunks."""
        if not texts:
            return np.array([])

        try:
            embeddings = self.model.encode(texts, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            return np.array([])

    def cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """Compute cosine similarity between query and document vectors."""
        if a.size == 0 or b.size == 0:
            return np.array([])

        a_norm = np.linalg.norm(a, axis=1, keepdims=True)
        b_norm = np.linalg.norm(b, axis=1, keepdims=True)

        a_norm = np.where(a_norm == 0, 1e-10, a_norm)
        b_norm = np.where(b_norm == 0, 1e-10, b_norm)

        a_normalized = a / a_norm
        b_normalized = b / b_norm

        return np.dot(a_normalized, b_normalized.T)

    def retrieve(self, query: str, k: int = 1, policy_id: Optional[str] = None) -> List[str]:
        """Retrieve the k most relevant text chunks for a query."""
        if self.embeddings is None or self.embeddings.size == 0:
            logger.warning("No embeddings available for retrieval")
            return []

        if not self.text_chunks:
            logger.warning("No text chunks available for retrieval")
            return []

        logger.info(f"Retrieving top {k} chunks for query using {self.chunking_strategy.name} strategy")
        query_embedding = self.embed([query])

        if query_embedding.size == 0:
            logger.warning("Failed to create query embedding")
            return []

        similarities = self.cosine_similarity(query_embedding, self.embeddings)
        if similarities.size == 0:
            logger.warning("Failed to compute similarities")
            return []

        similarities = similarities[0]

        # Filter by policy if specified
        if policy_id:
            policy_mask = np.array([
                meta.policy_id == policy_id for meta in self.chunk_metadata
            ])
            if np.any(policy_mask):
                filtered_similarities = np.where(policy_mask, similarities, -1)
                similarities = filtered_similarities

        # Get top k indices
        k = min(k, len(similarities))
        top_indices = np.argsort(similarities)[-k:][::-1]

        return [self.text_chunks[i] for i in top_indices]

    def get_chunk_statistics(self) -> Dict[str, Any]:
        """Get detailed statistics about the chunks."""
        if not self.chunk_metadata:
            return {"error": "No chunks available"}

        stats = {
            "strategy": self.chunking_strategy.name,
            "total_chunks": len(self.chunk_metadata),
            "chunk_types": {},
            "coverage_types": {},
            "avg_word_count": 0,
            "chunks_with_amounts": 0,
            "chunks_with_conditions": 0,
            "chunks_with_exclusions": 0,
            "policies_indexed": len(set(meta.policy_id for meta in self.chunk_metadata if meta.policy_id))
        }

        total_words = 0
        for meta in self.chunk_metadata:
            # Count chunk types
            chunk_type = meta.chunk_type
            stats["chunk_types"][chunk_type] = stats["chunk_types"].get(chunk_type, 0) + 1

            # Count coverage types
            coverage_type = meta.coverage_type
            stats["coverage_types"][coverage_type] = stats["coverage_types"].get(coverage_type, 0) + 1

            # Accumulate word count
            total_words += meta.word_count

            # Count special characteristics
            if meta.has_amounts:
                stats["chunks_with_amounts"] += 1
            if meta.has_conditions:
                stats["chunks_with_conditions"] += 1
            if meta.has_exclusions:
                stats["chunks_with_exclusions"] += 1

        stats["avg_word_count"] = total_words / len(self.chunk_metadata) if self.chunk_metadata else 0

        return stats

    def get_available_strategies(self) -> List[str]:
        """Get list of available chunking strategies."""
        return ChunkingFactory.get_available_strategies()

    def compare_strategies(self, strategies: List[str], sample_text: str,
                           policy_id: str = "test") -> Dict[str, Dict[str, Any]]:
        """
        Compare different chunking strategies on sample text.

        Args:
            strategies: List of strategy names to compare
            sample_text: Text to chunk for comparison
            policy_id: Policy ID for testing

        Returns:
            Dictionary mapping strategy names to their results
        """
        comparison_results = {}

        for strategy_name in strategies:
            try:
                # Create strategy instance
                strategy = ChunkingFactory.create_strategy(strategy_name)

                # Chunk the sample text
                chunks = strategy.chunk_text(sample_text, policy_id)

                # Analyze results
                comparison_results[strategy_name] = {
                    "chunk_count": len(chunks),
                    "avg_chunk_length": np.mean([len(chunk.text) for chunk in chunks]),
                    "chunk_types": list(set(chunk.metadata.chunk_type for chunk in chunks)),
                    "coverage_types": list(set(chunk.metadata.coverage_type for chunk in chunks)),
                    "chunks_with_amounts": sum(1 for chunk in chunks if chunk.metadata.has_amounts),
                    "strategy_info": strategy.get_strategy_info()
                }

            except Exception as e:
                comparison_results[strategy_name] = {
                    "error": str(e),
                    "chunk_count": 0
                }

        return comparison_results


# Backward compatibility: alias to existing LocalVectorStore interface
LocalVectorStore = EnhancedLocalVectorStore


# End of models/vector_store.py
# ================================================================================

# File 33/39: models/verifier.py
# --------------------------------------------------------------------------------

# src/models/verifier.py

"""
Verification module for reviewing and correcting insurance analysis outputs.
"""
import logging
import json
from typing import Dict, Any, List, Tuple
from models.base import BaseModelClient
from prompts.verification_prompts import VerificationPrompts

logger = logging.getLogger(__name__)


class ResultVerifier:
    """
    Verifies and potentially corrects insurance analysis results using a second LLM pass.
    """

    def __init__(self, model_client: BaseModelClient, model_type: str = "phi4"):
        """
        Initialize the verifier with a model client.

        Args:
            model_client: The model client to use for verification
            model_type: Type of model ('phi4' or 'qwen')
        """
        self.model_client = model_client
        self.model_type = model_type
        self.verification_prompt_template = VerificationPrompts.get_verification_prompt(model_type)

        # Store original prompt and replace with verification prompt
        self.original_prompt = model_client.base_prompt
        # Don't set the prompt here - we'll do it per verification

    def __del__(self):
        """Restore original prompt when verifier is destroyed."""
        if hasattr(self, 'model_client') and hasattr(self, 'original_prompt'):
            self.model_client.base_prompt = self.original_prompt

    def verify_result(
        self,
        question: str,
        context_texts: List[str],
        previous_result: Dict[str, Any],
        iterations: int = 1
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Verify and potentially correct a previous analysis result.

        Args:
            question: The original user question
            context_texts: The policy context chunks
            previous_result: The result from the initial analysis
            iterations: Number of verification iterations (default: 1)

        Returns:
            Tuple of (final_result, verification_info)
        """
        current_result = previous_result
        verification_history = []

        for iteration in range(iterations):
            logger.info(f"=== VERIFICATION ITERATION {iteration + 1}/{iterations} ===")

            # Prepare the verification prompt with all placeholders filled
            filled_verification_prompt = self._prepare_filled_verification_prompt(
                question, context_texts, current_result
            )

            # Temporarily set the model's prompt to the filled verification prompt
            original_prompt = self.model_client.base_prompt
            self.model_client.base_prompt = filled_verification_prompt

            try:
                # Run verification - pass empty question and context since everything is in the prompt
                verification_response = self.model_client.query(
                    question="",  # Empty as everything is in the filled prompt
                    context_files=[],  # Empty as context is already in the prompt
                    use_persona=False
                )

                logger.info(f"Verification response: {verification_response}")

                # Process verification result
                verified_result, verification_info = self._process_verification_response(
                    verification_response, current_result
                )

                verification_history.append({
                    "iteration": iteration + 1,
                    "status": verification_info.get("status", "unknown"),
                    "changes": verification_info.get("changes_made", "none")
                })

                # If no changes were made, we can stop early
                if verification_info.get("status") == "confirmed":
                    logger.info(f"Result confirmed at iteration {iteration + 1}, stopping verification")
                    break

                # Update current result for next iteration
                current_result = verified_result

            except Exception as e:
                logger.error(f"Error during verification iteration {iteration + 1}: {e}")
                verification_history.append({
                    "iteration": iteration + 1,
                    "status": "error",
                    "error": str(e)
                })
                break
            finally:
                # Always restore the original prompt
                self.model_client.base_prompt = original_prompt

        # Compile final verification info
        final_verification_info = {
            "iterations_performed": len(verification_history),
            "history": verification_history,
            "final_status": verification_history[-1].get("status", "unknown") if verification_history else "not_performed"
        }

        return current_result, final_verification_info

    def _prepare_filled_verification_prompt(
        self,
        question: str,
        context_texts: List[str],
        previous_result: Dict[str, Any]
    ) -> str:
        """
        Prepare the verification prompt with all placeholders filled in.
        """
        # Combine context texts
        combined_context = "\n\n".join(context_texts)

        # Format previous result for display
        previous_result_str = json.dumps(previous_result, indent=2, ensure_ascii=False)

        # Fill in ALL the placeholders in the verification prompt template
        filled_prompt = self.verification_prompt_template.replace(
            "{{USER_QUESTION}}", question
        ).replace(
            "{{POLICY_CONTEXT}}", combined_context
        ).replace(
            "{{PREVIOUS_RESULT}}", previous_result_str
        )

        # Log the filled prompt for debugging
        logger.debug(f"Filled verification prompt length: {len(filled_prompt)} characters")
        logger.debug(f"First 500 chars of filled prompt: {filled_prompt[:500]}...")

        return filled_prompt

    def _process_verification_response(
        self,
        verification_response: Dict[str, Any],
        original_result: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Process the verification response and extract the verified result and info.
        """
        # Extract verification info if present
        verification_info = verification_response.get("verification", {
            "status": "unknown",
            "changes_made": "verification info not provided"
        })

        # Extract the verified answer
        if "answer" in verification_response:
            verified_result = {"answer": verification_response["answer"]}
        else:
            # If verification failed to provide proper format, keep original
            logger.warning("Verification response missing 'answer' field, keeping original")
            verified_result = original_result
            verification_info = {
                "status": "error",
                "changes_made": "invalid verification response format"
            }

        # Log what changed
        if verification_info.get("status") == "corrected":
            logger.info(f"Verification made corrections: {verification_info.get('changes_made', 'not specified')}")
            self._log_changes(original_result, verified_result)

        return verified_result, verification_info

    def _log_changes(self, original: Dict[str, Any], verified: Dict[str, Any]) -> None:
        """Log the differences between original and verified results."""
        original_answer = original.get("answer", {})
        verified_answer = verified.get("answer", {})

        changes = []

        if original_answer.get("eligibility") != verified_answer.get("eligibility"):
            changes.append(f"Eligibility: '{original_answer.get('eligibility')}' â†’ '{verified_answer.get('eligibility')}'")

        if original_answer.get("outcome_justification") != verified_answer.get("outcome_justification"):
            changes.append("Outcome justification updated")

        if original_answer.get("payment_justification") != verified_answer.get("payment_justification"):
            changes.append("Payment justification updated")

        if changes:
            logger.info(f"Changes made: {', '.join(changes)}")
        else:
            logger.info("No changes detected in verification")


class SharedModelVerifier(ResultVerifier):
    """
    A verifier that temporarily changes the prompt of a shared model client.
    This avoids loading multiple model instances.
    """

    def __init__(self, model_client: BaseModelClient, model_type: str = "phi4"):
        """
        Initialize verifier that will temporarily use the model with a different prompt.

        Args:
            model_client: The shared model client
            model_type: Type of model ('phi4' or 'qwen')
        """
        self.model_client = model_client
        self.model_type = model_type
        self.verification_prompt_template = VerificationPrompts.get_verification_prompt(model_type)
        # Don't change the prompt in __init__, do it per verification call

    def verify_result(
        self,
        question: str,
        context_texts: List[str],
        previous_result: Dict[str, Any],
        iterations: int = 1
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Verify result by temporarily switching the model's prompt.
        This version properly fills in all placeholders before setting the prompt.
        """
        # Use the parent's implementation which now properly fills placeholders
        return super().verify_result(question, context_texts, previous_result, iterations)


# End of models/verifier.py
# ================================================================================

# File 34/39: output_formatter.py
# --------------------------------------------------------------------------------

# src/output_formatter.py

import os
import re
import json
import logging
import datetime
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)


def extract_policy_id(file_path: str) -> str:
    """
    Extract policy ID from PDF or TXT filename.
    Examples:
    - "10_nobis_policy.pdf" -> "10"
    - "18_medical_coverage.txt" -> "18"
    - "10-1_AXA 20220316 DIP AGGIUNTIVO ALI@TOP.pdf" -> "10-1"
    """
    filename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(filename)[0]

    # Extract policy ID from the filename using regex
    match = re.match(r'^([\d-]+)_', name_without_ext)
    if match:
        return match.group(1)
    else:
        logger.warning(f"Could not extract policy ID from filename: {filename}")
        return name_without_ext


def format_results_as_json(policy_path: str, question_results: List[List[str]]) -> Dict[str, Any]:
    """
    Format question results for a single policy as a JSON object.

    Args:
        policy_path: Path to the policy PDF
        question_results: List of question results in the format:
                         [model_name, q_id, question, eligibility, outcome_justification, payment_justification]

    Returns:
        A JSON-serializable dictionary for the policy
    """
    policy_id = extract_policy_id(policy_path)

    # Initialize the policy JSON structure
    policy_json = {
        "policy_id": policy_id,
        "questions": []
    }

    # Add each question result to the policy JSON
    for result in question_results:
        # Handle both 6 and 7 element results (7th element in error cases)
        if len(result) == 6:
            _, q_id, question, eligibility, outcome_justification, payment_justification = result
        elif len(result) == 7:
            # Error result with extra empty string - just ignore the last element
            _, q_id, question, eligibility, outcome_justification, payment_justification, _ = result
        else:
            logger.error(f"Unexpected result format with {len(result)} elements: {result}")
            continue

        question_json = {
            "request_id": q_id,
            "question": question,
            "outcome": eligibility,
            "outcome_justification": outcome_justification,
            "payment_justification": payment_justification,
        }

        policy_json["questions"].append(question_json)

    return policy_json


def get_timestamp_dir() -> str:
    """
    Generate a timestamp-based directory name in the format DD-MM-YY--HH-MM-SS.

    Returns:
        Timestamp string for directory naming
    """
    now = datetime.datetime.now()
    return now.strftime("%d-%m-%y--%H-%M-%S")

# Global variable to store the current run's timestamp
_current_run_timestamp = None


def get_or_create_run_timestamp() -> str:
    """
    Get the current run's timestamp, creating it if it doesn't exist.
    This ensures all outputs from a single run use the same timestamp.

    Returns:
        The timestamp for the current run
    """
    global _current_run_timestamp
    if _current_run_timestamp is None:
        _current_run_timestamp = get_timestamp_dir()
    return _current_run_timestamp


def reset_run_timestamp() -> None:
    """Reset the run timestamp (useful for testing or new runs)."""
    global _current_run_timestamp
    _current_run_timestamp = None


def create_model_specific_output_dir(base_output_dir: str, model_name: str, k: int = None,
                                     use_timestamp: bool = True, complete_policy: bool = False,
                                     prompt_name: Optional[str] = None) -> str:
    """
    Create a directory structure based on model name, k parameter (or complete-policy), timestamp, and prompt name.

    Args:
        base_output_dir: Base directory for outputs
        model_name: Name of the model (e.g., "microsoft/phi-4", "qwen/qwen-2.5-72b-instruct")
        k: Number of chunks parameter (optional, ignored if complete_policy=True)
        use_timestamp: Whether to include timestamp in directory structure (default: True)
        complete_policy: Whether using complete policy mode (default: False)
        prompt_name: Name of the prompt used (optional, e.g., "precise_v4_qwen")

    Returns:
        Path to the created directory

    Directory structure:
        For RAG mode:
            base_output_dir/model_name/k=3/DD-MM-YY--HH-MM-SS/prompt_name/
        For complete policy mode:
            base_output_dir/model_name/complete-policy/DD-MM-YY--HH-MM-SS/prompt_name/
    """
    # Clean the model name for directory naming
    if "/" in model_name:
        # For patterns like "microsoft/phi-4" or "qwen/qwen-2.5-72b"
        clean_model_name = model_name.replace("/", "_").replace(":", "_")
    else:
        clean_model_name = model_name

    # Build the directory path step by step
    path_components = [base_output_dir, clean_model_name]

    # Add mode-specific subdirectory
    if complete_policy:
        path_components.append("complete-policy")
    elif k is not None:
        path_components.append(f"k={k}")

    # Add timestamp directory if requested
    if use_timestamp:
        timestamp = get_or_create_run_timestamp()
        path_components.append(timestamp)

    # Add prompt name subdirectory if provided
    if prompt_name:
        path_components.append(prompt_name)

    # Create the full path
    output_dir = os.path.join(*path_components)

    # Create the directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Created output directory: {output_dir}")

    return output_dir


def save_policy_json(policy_data: Dict[str, Any], output_dir: str, model_name: str = None,
                     k: int = None, use_timestamp: bool = True, complete_policy: bool = False,
                     prompt_name: Optional[str] = None) -> str:
    """
    Save policy results to a JSON file in the appropriate directory.

    Args:
        policy_data: Dictionary containing policy results
        output_dir: Base output directory
        model_name: Model name for directory organization
        k: Number of chunks parameter for subdirectory (ignored if complete_policy=True)
        use_timestamp: Whether to use timestamp in directory structure
        complete_policy: Whether using complete policy mode
        prompt_name: Name of the prompt used (optional)

    Returns:
        Path to the saved JSON file
    """
    # Create the appropriate output directory
    if model_name:
        final_output_dir = create_model_specific_output_dir(
            output_dir, model_name, k, use_timestamp=use_timestamp,
            complete_policy=complete_policy, prompt_name=prompt_name
        )
    else:
        final_output_dir = output_dir
        os.makedirs(final_output_dir, exist_ok=True)

    # Create filename based on policy ID
    policy_id = policy_data.get("policy_id", "unknown")
    filename = f"policy_{policy_id}_results.json"
    filepath = os.path.join(final_output_dir, filename)

    # Save the JSON file
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(policy_data, f, indent=2, ensure_ascii=False)

    logger.info(f"Saved policy results to: {filepath}")

    return filepath


def process_policy_results(policy_paths: List[str], all_results: List[List[str]],
                           output_dir: str, model_name: str, prompt_name: Optional[str] = None) -> List[str]:
    """
    Process all results and create a JSON file for each policy in model-specific directory.

    Args:
        policy_paths: List of paths to policy PDFs
        all_results: List of all question results
        output_dir: Base output directory
        model_name: Model name for organizing outputs
        prompt_name: Name of the prompt used (optional)

    Returns:
        List of paths to the saved JSON files
    """
    saved_files = []

    # Convert to absolute path if needed
    if not os.path.isabs(output_dir):
        output_dir = os.path.abspath(output_dir)

    # Group results by policy ID
    for policy_path in policy_paths:
        policy_id = extract_policy_id(policy_path)
        logger.info(f"Processing results for policy ID: {policy_id}")

        # Filter results for this policy (you may need to modify this based on your data structure)
        policy_results = all_results

        # Format and save policy JSON
        policy_json = format_results_as_json(policy_path, policy_results)
        saved_path = save_policy_json(policy_json, output_dir, model_name, prompt_name=prompt_name)
        saved_files.append(saved_path)

    return saved_files


# End of output_formatter.py
# ================================================================================

# File 35/39: prompts/__init__.py
# --------------------------------------------------------------------------------

# src/prompts/__init__.py

"""
Prompts module for insurance policy analysis.
"""

from .insurance_prompts import InsurancePrompts

__all__ = ['InsurancePrompts']


# End of prompts/__init__.py
# ================================================================================

# File 36/39: prompts/insurance_prompts.py
# --------------------------------------------------------------------------------

# src/prompts/insurance_prompts.py

"""
Collection of prompts for insurance policy analysis.
This module provides a structured way to manage different prompt templates
for various insurance analysis tasks.
"""


class InsurancePrompts:
    """
    A collection of prompt templates for insurance policy analysis.

    This class provides various system prompts for different insurance analysis scenarios,
    making it easy to select and modify prompts as needed.
    """

    @classmethod
    def standard_coverage(cls) -> str:
        """
        Standard prompt for determining coverage eligibility and amounts.

        Returns:
            str: The prompt template.
        """
        return """
            You are an expert assistant helping users understand their insurance coverage.
            Given a question and access to a policy document, follow these instructions:

            1. Determine if the case is covered:
               - Answer with one of the following:
                 - "Yes"
                 - "No - Unrelated event"
                 _ "No - condition(s) not met"
            2. If the answer is "Yes" then always:
               - Quote the exact sentence(s) from the policy that support your decision.
               - Quote the exact sentence from the policy that specifies this amount.
            4. Is the answer is "No - Unrelated event":
               - Then it is not necessary to quote anything.
            5. If the answer is "No - condition(s) not met":
               - Then, quote the exact sentence(s) from the policy that justify your decision, 
               don't add extrac content, others characters or punctuations mark that are not in the original policy

            Return the answer in JSON format with the following fields:
            {
              "answer": {
                "eligibility": "Yes | No - Unrelated event | No - condition(s) not met",
                "eligibility_policy": "Quoted text from policy",
                "amount_policy": "Amount like '1000 CHF' or null",
              }
            }
        """

    @classmethod
    def detailed_coverage(cls) -> str:
        """
        Detailed prompt that includes late reporting and multiple insured parties.

        Returns:
            str: The prompt template.
        """
        return """
            You are an expert assistant helping users understand their insurance coverage.
            Given a question, that you need to interpret correctly, and access to a policy document, 
            follow these instructions:
            
            1. First, identify the situation in which the event occurred, determine the individuals affected, 
            and verify whether they are covered under the policy.
            2. Determine if the case is covered:
               - Answer with one of the following:
                 - "Yes"
                 - "No - Unrelated event"
                 _ "No - condition(s) not met"
            3. If the answer is "Yes" then always:
               - Quote the exact sentence(s) from the policy that support your decision.
               - Quote the exact sentence from the policy that specifies this amount.
            4. Is the answer is "No - Unrelated event":
            5. If the answer is "No - condition(s) not met":
                - Then, quote the exact sentence(s) from the policy that justify your decision.

            Return the answer in JSON format with the following fields:
            {
              "answer": {
                "eligibility": "Yes | No - Unrelated event | No - condition(s) not met | Maybe",
                "eligibility_policy": "Quoted text from policy",
                "amount_policy_line": "Quoted policy text or null"
              }
            }
        """

    @classmethod
    def precise_coverage(cls) -> str:
        """
        Improved and more precise prompt for determining coverage eligibility and payout amounts.
        """
        return """
            You are an expert assistant that explains insurance coverage.
            
            ==========  TASKS  ==========
            1. FIND the single policy chapter, section, paragraph, or sentence that matches the userâ€™s event.
            2. DECIDE eligibility:
               â€¢ "Yes"
               â€¢ "No - Unrelated event"
               â€¢ "No - condition(s) not met"
            3. QUOTE policy:
               â€¢ If "Yes":   â€“ sentence(s) that grant coverage
                             â€“ sentence(s) that state the amount **(if an amount sentence exists)**
               â€¢ If "No - condition(s) not met": quote only the sentence(s) that show the missing condition
               â€¢ If "No - Unrelated event": no quote
            4. SANITY CHECK  
               â€“ If you found both a coverage sentence *and* an amount sentence â†’ eligibility must be "Yes".  
               â€“ If you found a coverage sentence but no amount sentence anywhere in the policy â†’ eligibility is still 
                 "Yes" and "amount_policy" must be null.
            5. OUTPUT exactly in the JSON schema below.
            
            ==========  OUTPUT SCHEMA  ==========
            {
              "answer": {
                "eligibility": "...",
                "eligibility_policy": "...",
                "amount_policy": "..."
              }
            }
            
            ==========  EXAMPLE  (follow this layout)  ==========
            User event: â€œMy checked bag never arrived â€“ can I claim?â€
            Policy snippet: Â«In the event that the air carrier fails to deliver ... Option 1 â‚¬ 150,00 Option 2 â‚¬ 350,00 ...Â»
            Expected answer:
            {
              "answer": {
                "eligibility": "Yes",
                "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                "amount_policy": "Option 1 â‚¬ 150,00 Option 2 â‚¬ 350,00 Option 3 â‚¬ 500,00"
              }
            }
            (Do NOT output this example again.)
            
            ==========  REMEMBER  ==========
            â€¢ Return *only* valid JSON â€“ no markdown, no explanations.
            â€¢ Do NOT invent keys or punctuation not present in the policy.
            â€¢ Do not truncate, or modify the quoted text. Do NOT use '[...]', 'â€¦', or paraphrased summaries.
        """

    @classmethod
    def precise_coverage_v2(cls) -> str:
        """
        Improved and more precise prompt for determining coverage eligibility and payout amounts.
        """
        return """
                You are an expert assistant that explains insurance coverage.

                ==========  TASKS  ==========
                1. FIND the single policy chapter, section, paragraph, or sentence that matches the userâ€™s event.
                2. DECIDE eligibility:
                   â€¢ "Yes"
                   â€¢ "No - Unrelated event"
                   â€¢ "No - condition(s) not met"
                3. QUOTE policy:
                   â€¢ If "Yes":   â€“ sentence(s) that grant coverage
                                 â€“ sentence(s) that state the amount **(if an amount sentence exists)**
                   â€¢ If "No - condition(s) not met": quote only the sentence(s) that show the missing condition
                   â€¢ If "No - Unrelated event": no quote
                4. SANITY CHECK  
                   â€“ If you found both a coverage sentence *and* an amount sentence â†’ eligibility must be "Yes".  
                   â€“ If you found a coverage sentence but no amount sentence anywhere in the policy â†’ eligibility is still 
                     "Yes" and "amount_policy" must be null.
                5. OUTPUT exactly in the JSON schema below.

                ==========  WHEN DECIDING â€œcondition(s) not metâ€ VS. â€œYesâ€  ==========

                â€¢ If the userâ€™s event matches the loss description in a coverage clause:
                    â€“ Check the *same* clause (and any cross-referenced article) for
                      explicit prerequisites, exclusions, or timing limits.
                    â€“ If at least one of those conditions is clearly **not satisfied
                      in the userâ€™s story**, choose "No - condition(s) not met".
                    â€“ Otherwise choose "Yes".

                â€¢ Treat a prerequisite as **satisfied by default** when it is
                  *logically inherent* in the event:
                    (e.g. a derouted/lost/late bag was checked in; a cancellation
                    request implies the trip hasnâ€™t started yet; a hospitalised
                    person hasnâ€™t travelled).

                â€¢ DO NOT require procedural steps (PIR, police report, 24-h notice, etc.)
                  to be mentioned; assume they can still be provided later unless user
                  admits they didnâ€™t do them.


                ==========  OUTPUT SCHEMA  ==========
                {
                  "answer": {
                    "eligibility": "...",
                    "eligibility_policy": "...",
                    "amount_policy": "..."
                  }
                }

                ==========  EXAMPLE  (follow this layout)  ==========
                User event: â€œMy checked bag never arrived â€“ can I claim?â€
                Policy snippet: Â«In the event that the air carrier fails to deliver ... Indemnity amount for baggage 
                                loss option 1 â‚¬ 150,00 Option 2 â‚¬ 350,00 ...Â»
                Expected answer:
                {
                  "answer": {
                    "eligibility": "Yes",
                    "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage",
                    "amount_policy": "The Indemnification option selected and 
                        operative will be only the one resulting in the policy certificate according to the following: 
                        Indemnity amount for baggage loss option 1 â‚¬ 150,00 Option 2 â‚¬ 350,00 Option 3 â‚¬ 500,00"
                  }
                }
                (Do NOT output this example again.)

                ==========  REMEMBER  ==========
                â€¢ Return *only* valid JSON â€“ no markdown, no explanations.
                â€¢ Do NOT invent keys or punctuation not present in the policy.
                â€¢ Keep quotes verbatim (no â€œ[â€¦]â€ ellipses).
            """

    @classmethod
    def precise_coverage_v2_1(cls) -> str:
        """
        Improved and more precise prompt for determining coverage eligibility and payout amounts.
        """
        return (
            # ---------- ROLE ----------
            "SYSTEM: You are a readâ€‘only machine that copies **exact** sentences from POLICY_CONTEXT.\n"
            "You MUST NOT add, omit, reâ€‘order, or paraphrase words in any quoted sentence.\n"
            "Square brackets '[' or ']' and ellipses '...' or 'â€¦' are FORBIDDEN anywhere in the output.\n"
            "If you would violate the above rules, reply with the exact string \"###RULE_VIOLATION###\" instead.\n\n"

            # ---------- INPUT ----------
            "POLICY_CONTEXT:\n"
            "{{RETRIEVED_POLICY_TEXT}}\n\n"
            "QUESTION:\n"
            "{{USER_QUESTION}}\n\n"

            # ---------- TASKS ----------
            "TASKS\n"
            "1. Decide eligibility exactly as one of:\n"
            "   \"Yes\" | \"No - Unrelated event\" | \"No - condition(s) not met\" | \"No - Exclusion applies\"\n"
            "2. eligibility_policy â†’ copy ONLY the full policy sentence(s) that *directly* justify the decision in stepÂ 1.\n"
            "   â€¢ Quote them verbatim, in the order they appear.\n"
            "   â€¢ If no such sentence exists, use \"\" (empty string).\n"
            "3. amount_policy â†’ if any quoted sentence contains a monetary amount, copy that sentence (or the contiguous\n"
            "   sentence group). Otherwise set amount_policy to null.\n"
            "4. Consistency check: if both a coverage sentence and an amount sentence are present, eligibility must be \"Yes\".\n\n"

            # ---------- OUTPUT SCHEMA ----------
            "OUTPUT JSON (return exactly one object and nothing else):\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"Yes | No - Unrelated event | No - condition(s) not met | No - Exclusion applies\",\n"
            "    \"eligibility_policy\": \"\",\n"
            "    \"amount_policy\": null\n"
            "  }\n"
            "}\n\n"

            # ---------- FINAL REMINDER ----------
            "The VERY FIRST character of your reply must be '{' and you must stop immediately after the matching '}'."
        )

    @classmethod
    def precise_coverage_qwen_v2(cls) -> str:
        """
        Strict compliance prompt for Qwen3-14B that:
        - DEMANDS verbatim policy text copying
        - PROHIBITS any interpretation or paraphrasing
        - ENFORCES machine-readable JSON output
        - RETURNS COMPLETE POLICY SEGMENTS (no truncation)
        """
        return (
            "ROLE: Insurance Policy Compliance Engine\n\n"
            "OPERATING PRINCIPLES:\n"
            "1. You are a policy matching system, NOT an interpreter\n"
            "2. Your output must be legally defensible as direct policy citation\n\n"
            "INPUT PROCESSING:\n"
            "1. Read the claim description\n"
            "2. Scan policy excerpts for LITERAL matches\n"
            "3. Identify the DECISIVE POLICY SEGMENT that conclusively determines coverage\n\n"
            "DECISION REQUIREMENTS:\n"
            "1. Categorize using ONLY these exact phrases:\n"
            "   - \"Yes\" (policy explicitly approves)\n"
            "   - \"No - Unrelated event\" (policy never mentions this event type)\n"
            "   - \"No - condition(s) not met\" (policy mentions but excludes this case)\n"
            "2. Copy the COMPLETE DECISIVE POLICY SEGMENT verbatim (no truncation, no ellipsis)\n"
            "3. Extract monetary amounts EXACTLY as written (\"â‚¬500\" not \"500 EUR\")\n\n"
            "OUTPUT SPECIFICATION:\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"[exact_phrase]\",\n"
            "    \"eligibility_policy\": \"[complete_verbatim_text_from_policy]\",\n"
            "    \"amount_policy\": [\"exact_amount\"|null]\n"
            "  }\n"
            "}\n\n"
            "COMPLIANCE RULES:\n"
            "1. POLICY TEXT MUST:\n"
            "   - Be copied character-for-character\n"
            "   - Come from the provided excerpts ONLY\n"
            "   - Be enclosed in double quotes\n"
            "   - Include the COMPLETE relevant sentence or clause\n"
            "   - NEVER use [...] or ellipsis or truncation\n"
            "\n"
            "2. AMOUNTS MUST:\n"
            "   - Use original formatting (\"â‚¬1,000\" not \"1000\")\n"
            "   - Be null (unquoted) if unspecified\n"
            "\n"
            "3. STRICT PROHIBITIONS:\n"
            "   - NO combining multiple policy segments\n"
            "   - NO explanatory phrases like \"because\" or \"as stated\"\n"
            "   - NO markdown, headings, or whitespace deviations\n"
            "   - NO truncation with [...] or ellipsis\n"
            "   - NO abbreviating or shortening policy text\n\n"
            "VALID OUTPUT EXAMPLES:\n"
            "{\"answer\": {\"eligibility\": \"Yes\", \"eligibility_policy\": \"Baggage loss covered up to the insured amount\", \"amount_policy\": \"â‚¬1,200\"}}\n"
            "{\"answer\": {\"eligibility\": \"No - condition(s) not met\", \"eligibility_policy\": \"Excludes pre-existing medical conditions not declared at time of purchase\", \"amount_policy\": null}}\n"
            "{\"answer\": {\"eligibility\": \"No - condition(s) not met\", \"eligibility_policy\": \"which lasts more than 4 hours with respect to the arrival time stipulated in the flight plan\", \"amount_policy\": null}}\n\n"
            "INVALID OUTPUT EXAMPLES:\n"
            "{\"answer\": {\"eligibility\": \"No\", \"eligibility_policy\": \"This isn't covered\"}}  # Paraphrased\n"
            "{\"answer\": {\"eligibility\": \"Yes\", \"eligibility_policy\": \"Covered\"}}  # Too vague\n"
            "{\"answer\": {\"eligibility\": \"Yes\", \"eligibility_policy\": \"Pages 12-14 describe coverage\"}}  # Reference not text\n"
            "{\"answer\": {\"eligibility\": \"No - condition(s) not met\", \"eligibility_policy\": \"delay [...] more than 4 hours\"}}  # INVALID: Contains [...]\n"
        )

    @classmethod
    def precise_coverage_qwen_v3(cls) -> str:
        """
        Compact, deterministic prompt for Qwen-14B-Chat / Qwen-3-235B-Chat.
        â€¢ model returns ONE well-formed JSON object
        â€¢ quote verbatim and in full â€“ no truncation, no paraphrase
        â€¢ first character must be â€œ{â€, generation stops after the closing brace
        """
        return (
            "You are an insurance-coverage compliance assistant.\n\n"
            # ---------- DYNAMIC CONTENT (insert before sending) ----------
            "POLICY_CONTEXT:\n"
            "{{RETRIEVED_POLICY_TEXT}}\n\n"
            "QUESTION:\n"
            "{{USER_QUESTION}}\n\n"
            # ---------- TASKS ----------
            "TASKS\n"
            "1. Decide eligibility exactly as one of:\n"
            "   \"Yes\" | \"No - Unrelated event\" | \"No - condition(s) not met\"\n"
            "2. Quote policy text verbatim and in full:\n"
            "   â€¢ If \"Yes\": quote both the full coverage sentence(s) and the full amount sentence(s), if present.\n"
            "   â€¢ If \"No - condition(s) not met\": quote only the full sentence(s) showing the unmet condition.\n"
            "   â€¢ If \"No - Unrelated event\": leave eligibility_policy empty.\n"
            "   âš ï¸ IMPORTANT: Do not truncate, or modify the quoted text. Do NOT use '[...]', 'â€¦', or paraphrased summaries.\n"
            "3. If the quoted text contains a monetary amount, copy it exactly; otherwise set amount_policy to null.\n"
            "4. Sanity-check: if both coverage **and** amount sentences are present, eligibility must be \"Yes\".\n\n"
            # ---------- OUTPUT FORMAT ----------
            "Return exactly this JSON (no markdown, no commentary):\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"â€¦\",\n"
            "    \"eligibility_policy\": \"â€¦\",\n"
            "    \"amount_policy\": \"â€¦\" | null\n"
            "  }\n"
            "}\n\n"
            "First character of your reply must be \"{\" and you must stop right after the closing brace."
        )

    @classmethod
    def precise_coverage_v2_2(cls) -> str:
        """
        Improved and more precise prompt for determining coverage eligibility and payout amounts.
        """
        return """
                    You are an expert assistant that explains insurance coverage.

                    ==========  TASKS  ==========
                    1. FIND the single policy chapter, section, paragraph, or sentence that matches the userâ€™s event.  
                    2. DECIDE eligibility:  
                       â€¢ "Yes"  
                       â€¢ "No - condition(s) not met"  
                    3. QUOTE policy:  
                       â€¢ If "Yes":   â€“ sentence(s) that grant coverage  
                                     â€“ sentence(s) that state the amount **(if an amount sentence exists)**  
                       â€¢ If "No - condition(s) not met": quote only the sentence(s) that show the missing condition  
                    4. SANITY CHECK  
                       â€“ If you found both a coverage sentence *and* an amount sentence â†’ eligibility must be **"Yes"**.  
                       â€“ If you found a coverage sentence but no amount sentence anywhere in the policy â†’ eligibility is still **"Yes"** and `"amount_policy"` must be null.
                    
                    ==========  WHEN DECIDING â€œcondition(s) not metâ€ VS. â€œYesâ€  ==========
                    
                    â€¢ If the userâ€™s event matches the loss description in a coverage clause:  
                        â€“ Check the *same* clause (and any cross-referenced article) for explicit prerequisites, exclusions, or timing limits.  
                        â€“ If at least one of those conditions is clearly **not satisfied in the userâ€™s story**, choose **"No - condition(s) not met"**.  
                        â€“ Otherwise choose **"Yes"**.
                    
                    â€¢ Treat a prerequisite as **satisfied by default** when it is *logically inherent* in the event:  
                      (e.g. a derouted/lost/late bag was checked in; a cancellation request implies the trip hasnâ€™t started yet; a hospitalised person hasnâ€™t travelled).
                    
                    â€¢ DO NOT require procedural steps (PIR, police report, 24-h notice, etc.) to be mentioned; assume they can still be provided later unless user admits they didnâ€™t do them.
                    
                    ==========  OUTPUT SCHEMA  ==========
                    {
                      "answer": {
                        "eligibility": "...",
                        "eligibility_policy": "...",
                        "amount_policy": "..."
                      }
                    }
                    
                    ==========  EXAMPLE  (follow this layout)  ==========
                    User event: â€œMy checked bag never arrived â€“ can I claim?â€
                    Policy snippet: Â«In the event that the air carrier fails to deliver ... Indemnity amount for baggage 
                                    loss option 1 â‚¬ 150,00 Option 2 â‚¬ 350,00 ...Â»
                    Expected answer:
                    {
                      "answer": {
                        "eligibility": "Yes",
                        "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                        "amount_policy": "The Insured Person may choose... The Indemnification option selected and 
                            operative will be only the one resulting in the policy certificate according to the following: 
                            Indemnity amount for baggage loss option 1 â‚¬ 150,00 Option 2 â‚¬ 350,00 Option 3 â‚¬ 500,00"
                      }
                    }
                    (Do NOT output this example again.)
                    
                    ==========  REMEMBER  ==========
                    â€¢ Return *only* valid JSON â€“ no markdown, no explanations.  
                    â€¢ Do NOT invent keys or punctuation not present in the policy.  
                    â€¢ Keep quotes verbatim (no â€œ[â€¦]â€ ellipses).

                """

    @classmethod
    def precise_coverage_v3(cls) -> str:
        """
        Minimal, no-frills prompt:
        â€¢ contains all decision logic
        â€¢ tells the model to output ONE JSON object and nothing else
        â€¢ no ENDJSON sentinel, no markdown, no examples
        â€¢ matches the expected ground truth structure with outcome_justification and payment_justification
        â€¢ prevents hallucination by being very explicit about using only actual policy text
        """
        return (
            # ---------- CRITICAL SYSTEM INSTRUCTION ----------
            "**SYSTEM CONSTRAINT**: You are a text-copying system that CANNOT generate new sentences. "
            "You may ONLY output text that exists character-for-character in the POLICY_CONTEXT below.\n"
            "**IF YOU CANNOT FIND EXACT MATCHING TEXT, YOU MUST USE EMPTY STRING \"\"**\n\n"

            # ---------- ROLE ----------
            "You are an insuranceâ€‘coverage assistant. You must ONLY use text that appears **verbatim** in the POLICY_CONTEXT below. "
            "**NEVER** invent, create, paraphrase, or hallucinate any content.\n"
            "Square brackets '[' or ']' and ellipses '...' or 'â€¦' are **FORBIDDEN** anywhere in the output.\n"
            "If you cannot find relevant text in the POLICY_CONTEXT, use empty string \"\".\n"
            "If you would violate these rules, reply with \"###RULE_VIOLATION###\".\n\n"
            # ---------- INPUT ----------
            "POLICY_CONTEXT:\n"
            "{{RETRIEVED_POLICY_TEXT}}\n\n"
            "QUESTION:\n"
            "{{USER_QUESTION}}\n\n"
            # ---------- TASKS ----------
            "TASKS\n"
            "1. Decide eligibility exactly as one of:\n"
            "   â€¢ \"Yes\"                       (the policy grants coverage)\n"
            "   â€¢ \"No - condition(s) not met\" (the policy is referenced but the stated conditions are not satisfied **or** "
            "                                    an exclusion removes coverage)\n"
            "   â€¢ \"No - Unrelated event\"      (the policy text you have does not relate to the question at all)\n"
            "2. Build the JSON fields:\n"
            "   â€¢ If eligibility == \"Yes\":\n"
            "       â€“ outcome_justification â†’ copy **all** policy sentence(s) that explicitly grant coverage, verbatim, "
            "         in their original order. ONLY use text from POLICY_CONTEXT above.\n"
            "       â€“ payment_justification â†’ if any of those sentences (or an immediatelyâ€‘following sentence) contains a "
            "         monetary amount, copy that full sentence/group; otherwise set payment_justification to null.\n"
            "   â€¢ If eligibility == \"No - condition(s) not met\":\n"
            "       â€“ outcome_justification â†’ copy **all** policy sentence(s) that reference the event but show why "
            "         conditions are not met or why exclusions apply, verbatim, in their original order. "
            "         If no such sentences exist in the POLICY_CONTEXT above, use empty string \"\".\n"
            "       â€“ payment_justification â†’ null\n"
            "   â€¢ If eligibility == \"No - Unrelated event\":\n"
            "       â€“ outcome_justification â†’ empty string \"\"\n"
            "       â€“ payment_justification â†’ null\n"
            "3. **CRITICAL CHECK**: Before outputting outcome_justification, verify that the exact text appears in the "
            "   POLICY_CONTEXT above. If you cannot find it, use empty string \"\".\n\n"
            # ---------- OUTPUT SCHEMA ----------
            "Return **one** JSON object that matches exactly this schema:\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"Yes | No - Unrelated event | No - condition(s) not met\",\n"
            "    \"outcome_justification\": \"\",\n"
            "    \"payment_justification\": null\n"
            "  }\n"
            "}\n\n"
            # ---------- OUTPUT RULES ----------
            "Rules for the fields:\n"
            "â€¢ eligibility            â†’ one of the three strings above, caseâ€‘sensitive.\n"
            "â€¢ outcome_justification  â†’ empty string \"\" for \"No - Unrelated event\" OR when no relevant policy text exists "
            "                            in the POLICY_CONTEXT above. Otherwise copy relevant policy text verbatim. "
            "                            **NEVER** create, invent, or paraphrase content.\n"
            "â€¢ payment_justification  â†’ null except when you copy a sentence that contains a monetary amount (only permitted "
            "                            when eligibility is \"Yes\"). Copy the **whole** sentence(s), no trimming, no added text.\n\n"
            # ---------- FINAL REMINDER ----------
            "The VERY FIRST character you output must be '{' and you must stop immediately after the matching '}'.\n"
            "**CRITICAL**: Only use text that actually appears in the POLICY_CONTEXT above. "
            "If you cannot find relevant text in the POLICY_CONTEXT, use empty string \"\" for outcome_justification. "
            "Do not invent, create, or hallucinate any content.\n\n"
        )


    @classmethod
    def precise_coverage_v4(cls) -> str:
        """
        Prompt for deterministic coverage decisions and payout amounts.
        """
        return """
                You are an expert assistant that explains insurance coverage.

                ==========  TASKS  ==========
                1. FIND the single policy chapter, section, paragraph, or sentence that matches the userâ€™s event.
                1.b CHECK that a guarantee actually exists for the userâ€™s risk
                    â€¢ If no clause grants this type of benefit â†’ choose "No - Unrelated event".
                1.c CONFIRM the guarantee is active for timing / territory / object
                    â€¢ If the guarantee is outside its validity window â†’ "No - Unrelated event".

                2. DECIDE eligibility:            # (use exactly one of)
                   â€¢ "Yes"
                   â€¢ "No - Unrelated event"
                   â€¢ "No - condition(s) not met"

                3. QUOTE policy:
                   â€¢ If "Yes":   â€“ sentence(s) that grant coverage
                                 â€“ sentence(s) that state the amount **(if an amount sentence exists)**
                   â€¢ If "No - condition(s) not met": quote only the sentence(s) that show the missing condition
                   â€¢ If "No - Unrelated event": no quote

                4. SANITY CHECK
                   â€“ If you found both a coverage sentence *and* an amount sentence â†’ eligibility must be "Yes".
                   â€“ If you found a coverage sentence but no amount sentence anywhere in the policy â†’ eligibility is still 
                     "Yes" and "amount_policy" must be null.

                5. OUTPUT exactly in the JSON schema below.

                ==========  WHEN DECIDING â€œcondition(s) not metâ€ VS. â€œYesâ€  ==========

                â€¢ If the userâ€™s event matches the loss description in a coverage clause:
                    â€“ Check the *same* clause (and any cross-referenced article) for
                      explicit prerequisites, exclusions, timing limits, territorial limits, sub-limits, or person definitions.
                    â€“ If at least one of those conditions is clearly **not satisfied
                      in the userâ€™s story**, choose "No - condition(s) not met".
                    â€“ Otherwise choose "Yes".

                â€¢ Treat a prerequisite as **satisfied by default** when it is
                  *logically inherent* in the event
                  (e.g. a derouted/lost/late bag was checked in; a cancellation
                  request implies the trip hasnâ€™t started yet; a hospitalised
                  person hasnâ€™t travelled).

                â€¢ DO NOT require procedural steps (PIR, police report, 24-h notice, etc.)
                  to be mentioned; assume they can still be provided later unless the user
                  admits they didnâ€™t do them.

                ==========  OUTPUT SCHEMA  ==========
                {
                  "answer": {
                    "eligibility": "...",
                    "eligibility_policy": "...",
                    "amount_policy": "..."
                  }
                }

                ==========  EXAMPLE  (positive)  ==========
                User event: â€œMy checked bag never arrived â€“ can I claim?â€
                Policy snippet: Â«In the event that the air carrier fails to deliver ... Indemnity amount for baggage 
                                loss option 1 â‚¬ 150,00 Option 2 â‚¬ 350,00 ...Â»
                Expected answer:
                {
                  "answer": {
                    "eligibility": "Yes",
                    "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                    "amount_policy": "The Insured Person may choose... Indemnity amount for baggage loss option 1 â‚¬ 150,00 ..."
                  }
                }

                ==========  EXAMPLE  (negative â€“ guarantee expired)  ==========
                User event: â€œI am already abroad and my colleague at home was hospitalised; can I claim the unused nights?â€
                Policy snippet: Â«The 'Trip Cancellation' guarantee ... starts at booking and ends when the Insured begins to use the first service ...Â»
                Expected answer:
                {
                  "answer": {
                    "eligibility": "No - Unrelated event",
                    "eligibility_policy": "",
                    "amount_policy": null
                  }
                }

                (Do NOT output these examples again.)

                ==========  REMEMBER  ==========
                â€¢ Return *only* valid JSON â€“ no markdown, no explanations.
                â€¢ Do NOT invent keys or punctuation not present in the policy.
                â€¢ Keep quotes verbatim (no â€œ[â€¦]â€ ellipses).
            """

    @classmethod
    def relevance_filter_v1(cls) -> str:
        """
        Prompt optimised for Microsoft Phi-4.
        Determines whether a user question is completely unrelated to the policy.
        """
        return """
                You are an INSURANCE-POLICY RELEVANCE FILTER.

                ==========  TASKS  ==========
                1. DECIDE whether the userâ€™s question is **COMPLETELY UNRELATED** to the coverage topics in the policy text.
                   â€¢ Focus only on the high-level type of loss/event (baggage, trip cancellation, medical, rental car, etc.).
                   â€¢ Ignore exclusions, sub-limits, conditions, dates, wording quirks.
                   â€¢ If the question mentions any loss/event that the policy covers â†’ it is RELATED.
                2. GIVE a brief one-sentence reason.

                ==========  OUTPUT  ==========
                Return exactly this JSON:
                {
                  "is_relevant": true/false,
                  "reason": "Brief explanation (â‰¤ 25 words)"
                }

                ==========  EXAMPLES  ==========
                â€¢ Policy covers baggage loss  
                  Question: â€œWill you pay for overseas hospital bills?â€  
                  â‡’ { "is_relevant": false, "reason": "Hospital bills are medical expenses; policy covers only baggage loss." }

                â€¢ Policy covers trip cancellation  
                  Question: â€œMy rental car got scratchedâ€”am I covered?â€  
                  â‡’ { "is_relevant": false, "reason": "Rental-car damage is unrelated to trip cancellation coverage." }

                â€¢ Policy covers baggage loss  
                  Question: â€œMy suitcase was stolen from the taxiâ€”can I claim?â€  
                  â‡’ { "is_relevant": true, "reason": "Stolen baggage is a form of baggage loss covered by the policy." }

                (Do NOT output these examples again.)

                ==========  REMEMBER  ==========
                â€¢ Output **only** the JSONâ€”no markdown, no extra commentary.
                â€¢ Use lowercase true/false.
                â€¢ Keep the reason short and specific.
            """

    @classmethod
    def relevance_filter_v2(cls) -> str:
        """
        Prompt optimised for Microsoft Phi-4.
        Determines whether a user question is completely unrelated to the policy.
        """
        return """
                    You are an **INSURANCE-POLICY RELEVANCE FILTER**.
                    
                    ====================  TASK  ====================
                    Decide if the userâ€™s question is ABOUT a loss/event type that the policy covers.
                    
                    â€¢ Work ONLY at the **high-level category**: baggage loss, trip cancellation, medical costs, rental-car damage, personal liability, etc.  
                    â€¢ DO NOT worry about:
                      â€“ how the loss happened (airport vs. taxi vs. hotel)  
                      â€“ exclusions, sub-limits, dates, conditions, documents, deductibles  
                      â€“ whether the policy will actually pay.  
                    â€¢ If the question involves ANY loss/event type that appears in the policy text â†’ it is **RELATED**.
                    
                    ====================  OUTPUT  ==================
                    Return exactly this JSON (no markdown, no extra words):
                    
                    {
                      "is_relevant": true/false,
                      "reason": "Brief â‰¤ 25 words explaining the category match or mismatch"
                    }
                    
                    ====================  EXAMPLES  ================
                    â€¢ Policy section â€œBaggage loss or delayâ€  
                      Q: â€œMy suitcase was stolen from a taxiâ€”can I claim?â€  
                      â†’ { "is_relevant": true, "reason": "Baggage theft is a type of baggage loss mentioned in the policy." }
                    
                    â€¢ Policy section â€œTrip cancellationâ€  
                      Q: â€œI broke my leg abroad; will you cover hospital bills?â€  
                      â†’ { "is_relevant": false, "reason": "Hospital bills are medical expenses, not trip cancellation." }
                    
                    â€¢ Policy section â€œMedical expenses abroadâ€  
                      Q: â€œAirline lost my snowboardâ€”am I covered?â€  
                      â†’ { "is_relevant": false, "reason": "Baggage loss is not a medical-expense event." }
                    
                    (Do NOT repeat these examples in your answer.)
                                    """

    @classmethod
    def precise_coverage_qwen_v4(cls) -> str:
        """
        Extremeâ€‘strict extractor prompt.
        Outputs one JSON object ONLY, with no ellipses, tags or prose.
        """
        return (
            # ---------- ROLE ----------
            "SYSTEM: You are a machine whose only function is to copy exact sentences from POLICY_CONTEXT.\n"
            "You are NOT permitted to explain, comment, think, or insert ellipses (\"...\" or \"[â€¦]\").\n"
            "If a required quote spans multiple sentences, copy all of them in full; otherwise copy the single sentence.\n"
            "Square brackets '[' or ']' and threeâ€‘dot sequences '...' are FORBIDDEN anywhere in the output.\n\n"

            # ---------- INPUT ----------
            "POLICY_CONTEXT:\n"
            "{{RETRIEVED_POLICY_TEXT}}\n\n"
            "QUESTION:\n"
            "{{USER_QUESTION}}\n\n"
            "{{QUERY_ANALYSIS}}\n\n"
            # ---------- TASK ----------
            "TASK: Produce exactly one JSON object with this schema â€” and nothing else:\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"Yes | No - Unrelated event | No - condition(s) not met\",\n"
            "    \"outcome_justification\": \"<verbatim policy sentence(s) | \"\">\",\n"
            "    \"payment_justification\": \"<verbatim amount sentence(s) | \"\" | null>\"\n"
            "  }\n"
            "}\n\n"
            
            # ---------- DECISION LOGIC (MANDATORY) ----------
            "DETERMINE eligibility:\n"
            "â€¢ If the policy EXPLICITLY grants coverage for the scenario â†’ \"Yes\".\n"
            "â€¢ If the scenario is addressed but at least one condition is NOT met â†’ \"No - condition(s) not met\".\n"
            "â€¢ If the scenario is NOT mentioned / outside policy scope â†’ \"No - Unrelated event\".\n\n"
            "POPULATE outcome_justification:\n"
            "â€¢ For \"Yes\" â€” quote ALL sentence(s) that grant coverage.\n"
            "â€¢ For \"No - condition(s) not met\" â€” quote ALL sentence(s) that show the unmet condition(s).\n"
            "â€¢ For \"No - Unrelated event\" â€” use \"\" (empty string).\n\n"
            "POPULATE payment_justification:\n"
            "â€¢ Only when eligibility == \"Yes\".\n"
            "  â€“ If a specific amount / limit / deductible is mentioned in the quoted coverage **or elsewhere in POLICY_CONTEXT**, copy that sentence (or contiguous sentences if they belong together).\n"
            "  â€“ If no amount sentence exists, use \"\" (empty string).\n"
            "â€¢ For any eligibility other than \"Yes\", use null.\n\n"

            # ---------- RULES ----------
            "RULES (MANDATORY):\n"
            "1. Do NOT output anything before or after the JSON object.\n"
            "2. NO chainâ€‘ofâ€‘thought, NO <think> tags, NO explanations.\n"
            "3. NO ellipsis, NO square brackets, NO truncation; copy sentences exactly as printed.\n"
            "4. If a required quote is absent, use \"\" (empty string) or null as instructed.\n"
            "5. If eligibility â‰  \"Yes\", payment_justification must be null.\n"
            "6. Any violation of rules 1â€‘3 is a critical error.\n"
        )

    @classmethod
    def precise_coverage_v3_phi4(cls) -> str:
        """
        Ultra-strict extractor for phi-4 that forbids any generative insurance language.
        """
        return (
            # ---------- ROLE ----------
            "YOU ARE A TEXT SCANNER. YOU MAY ONLY COPY TEXT THAT ALREADY EXISTS IN POLICY_CONTEXT.\n"
            "YOU HAVE ZERO INSURANCE KNOWLEDGE. YOU MUST NOT WRITE NEW INSURANCE SENTENCES.\n\n"

            # ---------- FORBIDDEN PHRASES ----------
            "NEVER WRITE ANY OF THESE (or variations):\n"
            "âŒ \"The policy does not cover\"\n"
            "âŒ \"Coverage is not provided\"\n"
            "âŒ \"The insurance covers\"\n"
            "âŒ \"This is not covered\"\n"
            "âŒ \"According to the policy\"\n"
            "âŒ ANY sentence you create yourself\n\n"

            # ---------- ALLOWED ACTION ----------
            "ALLOWED:\n"
            "âœ“ Scan POLICY_CONTEXT.\n"
            "âœ“ Copy sentence(s) verbatim.\n"
            "âœ“ Use an empty string \"\" if nothing to copy.\n\n"

            # ---------- INPUT ----------
            "POLICY_CONTEXT:\n"
            "{{RETRIEVED_POLICY_TEXT}}\n\n"
            "QUESTION:\n"
            "{{USER_QUESTION}}\n\n"

            # ---------- SCANNING WORKFLOW ----------
            "WORKFLOW:\n"
            "1. Decide eligibility:\n"
            "   â€¢ If no sentence about the QUESTION topic exists â†’ eligibility = \"No - Unrelated event\".\n"
            "   â€¢ If only exclusion / unmet-condition sentences exist â†’ eligibility = \"No - condition(s) not met\".\n"
            "   â€¢ If at least one inclusion / coverage sentence exists â†’ eligibility = \"Yes\".\n"
            "2. outcome_justification = copy the EXACT sentence(s) that drove the decision; else \"\".\n"
            "3. payment_justification = copy sentence(s) that state a monetary amount; else null.\n\n"

            # ---------- OUTPUT FORMAT ----------
            "OUTPUT EXACTLY (no extra keys, no text after } ):\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"_____\",\n"
            "    \"outcome_justification\": \"_____\",\n"
            "    \"payment_justification\": null\n"
            "  }\n"
            "}\n\n"

            # ---------- CRITICAL WARNINGS ----------
            "âš ï¸ If you create ANY sentence not in POLICY_CONTEXT â†’ output \"###RULE_VIOLATION###\" instead of JSON.\n"
            "âš ï¸ NOTHING after the closing brace.\n"

            # ---------- SELF-CHECK ----------
            "BEFORE SENDING, VERIFY:\n"
            "â€¢ outcome_justification is copied verbatim or \"\".\n"
            "â€¢ payment_justification is copied verbatim or null.\n"
            "â€¢ No invented insurance wording appears.\n"
        )

    @classmethod
    def precise_coverage_v5_phi4(cls) -> str:
        """
        Enhanced prompt for Phi-4 that reduces false negatives while maintaining accuracy.
        """
        return (
            # STRICT COPYING REQUIREMENT
            "YOU ARE A POLICY TEXT SCANNER. YOU COPY EXACT SENTENCES FROM POLICY_CONTEXT.\n"
            "YOU CANNOT CREATE NEW SENTENCES OR PARAPHRASE.\n\n"

            # CONTEXT UNDERSTANDING
            "UNDERSTANDING THE CONTEXT:\n"
            "â€¢ The POLICY_CONTEXT below contains excerpts from an insurance policy\n"
            "â€¢ Some policies cover MANY scenarios (look for 'ALL RISKS' or 'Assistenza in Viaggio')\n"
            "â€¢ Some policies cover SPECIFIC scenarios only\n"
            "â€¢ Your job: determine if the scenario IS or IS NOT addressed in the context\n\n"

            "POLICY_CONTEXT:\n"
            "{{RETRIEVED_POLICY_TEXT}}\n\n"

            "QUESTION:\n"
            "{{USER_QUESTION}}\n\n"

            "SCANNING RULES:\n"
            "1. SCAN the POLICY_CONTEXT for any mention of:\n"
            "   â€¢ The specific event type (theft, illness, delay, etc.)\n"
            "   â€¢ General categories that include the event (baggage, medical, assistance)\n"
            "   â€¢ 'ALL RISKS' or comprehensive coverage statements\n\n"

            "2. CLASSIFY the scenario:\n"
            "   â€¢ FOUND with coverage â†’ 'Yes'\n"
            "   â€¢ FOUND with exclusion/condition â†’ 'No - condition(s) not met'\n"
            "   â€¢ NOT FOUND anywhere â†’ 'No - Unrelated event'\n\n"

            "3. IMPORTANT MATCHES TO RECOGNIZE:\n"
            "   â€¢ 'bagaglio' or 'baggage' â†’ covers ALL baggage issues (theft, loss, damage)\n"
            "   â€¢ 'spese mediche' or 'medical' â†’ covers ALL medical issues\n"
            "   â€¢ 'assistenza' or 'assistance' â†’ covers help/support scenarios\n"
            "   â€¢ 'ALL RISKS' â†’ covers MOST scenarios unless excluded\n\n"

            "4. COPY TEXT:\n"
            "   â€¢ outcome_justification: Copy the EXACT sentence(s) that mention the coverage\n"
            "   â€¢ payment_justification: Copy amount sentences if present, else null\n"
            "   â€¢ For 'No - Unrelated event': use empty string \"\"\n\n"

            "OUTPUT (exactly this format):\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"Yes|No - Unrelated event|No - condition(s) not met\",\n"
            "    \"outcome_justification\": \"<copied text or empty string>\",\n"
            "    \"payment_justification\": \"<copied text or null>\"\n"
            "  }\n"
            "}\n\n"

            "NEVER write 'The policy does not cover...' - only copy existing text!"
        )

    @classmethod
    def precise_coverage_v5_qwen(cls) -> str:
        """
        Enhanced prompt for Qwen that reduces false negatives with strict format.
        """
        return (
            # ROLE DEFINITION
            "ROLE: Insurance policy text extractor (not interpreter)\n"
            "CONSTRAINT: Output only text that exists in POLICY_CONTEXT\n\n"

            # MATCHING RULES
            "MATCHING RULES:\n"
            "1. Event categories (match ANY of these):\n"
            "   BAGGAGE: 'bagaglio', 'baggage', 'luggage' â†’ includes theft/loss/damage\n"
            "   MEDICAL: 'mediche', 'medical', 'hospital' â†’ includes illness/injury\n"
            "   CANCELLATION: 'annullamento', 'cancellation' â†’ includes trip changes\n"
            "   ASSISTANCE: 'assistenza', 'assistance' â†’ includes help/support\n"
            "   ALL-RISKS: 'ALL RISKS', 'all risks' â†’ includes most scenarios\n\n"

            "2. Scenario mapping:\n"
            "   'stolen bag' â†’ BAGGAGE category\n"
            "   'sick/doctor' â†’ MEDICAL category\n"
            "   'can't leave' â†’ CANCELLATION category\n"
            "   'help needed' â†’ ASSISTANCE category\n\n"

            # INPUT SECTIONS
            "POLICY_CONTEXT:\n"
            "{{RETRIEVED_POLICY_TEXT}}\n\n"

            "QUESTION:\n"
            "{{USER_QUESTION}}\n\n"

            # DECISION WORKFLOW
            "WORKFLOW:\n"
            "1. Find category match:\n"
            "   âœ“ Category found + coverage stated â†’ 'Yes'\n"
            "   âœ“ Category found + exclusion stated â†’ 'No - condition(s) not met'\n"
            "   âœ— No category match â†’ 'No - Unrelated event'\n\n"

            "2. Copy sentences:\n"
            "   â€¢ 'Yes': copy coverage sentence(s) + amount sentence(s)\n"
            "   â€¢ 'No - condition(s) not met': copy exclusion sentence(s)\n"
            "   â€¢ 'No - Unrelated event': use \"\"\n\n"

            # OUTPUT FORMAT
            "OUTPUT:\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"[exact_value]\",\n"
            "    \"outcome_justification\": \"[copied_text_or_empty]\",\n"
            "    \"payment_justification\": \"[copied_text_or_null]\"\n"
            "  }\n"
            "}\n\n"

            # STRICT RULES
            "FORBIDDEN:\n"
            "â€¢ Creating new sentences\n"
            "â€¢ Using [...] or ellipsis\n"
            "â€¢ Paraphrasing\n"
            "â€¢ Adding explanations\n"
        )

    @classmethod
    def get_prompt(cls, prompt_name: str) -> str:
        """
        Get a specific prompt by name.

        Args:
            prompt_name: Name of the prompt to retrieve

        Returns:
            The prompt template string

        Raises:
            ValueError: If the prompt name is not found
        """
        prompt_map = {
            "standard": cls.standard_coverage(),
            "detailed": cls.detailed_coverage(),
            "precise": cls.precise_coverage(),
            "precise_v2": cls.precise_coverage_v2(),
            "precise_v2_1": cls.precise_coverage_v2_1(),
            "precise_v2_2": cls.precise_coverage_v2_2(),
            "precise_v3": cls.precise_coverage_v3(),
            "precise_v4": cls.precise_coverage_v4(),
            "relevance_filter_v1": cls.relevance_filter_v1(),
            "relevance_filter_v2": cls.relevance_filter_v2(),
            "precise_v2_qwen": cls.precise_coverage_qwen_v2(),
            "precise_v3_qwen": cls.precise_coverage_qwen_v3(),
            "precise_v4_qwen": cls.precise_coverage_qwen_v4(),
            "precise_v3_phi-4_v2": cls.precise_coverage_v3_phi4(),
            "precise_v5": cls.precise_coverage_v5_phi4(),
            "precise_v5_qwen": cls.precise_coverage_v5_qwen()
        }

        if prompt_name not in prompt_map:
            raise ValueError(f"Prompt '{prompt_name}' not found. Available prompts: {', '.join(prompt_map.keys())}")

        return prompt_map[prompt_name]



# End of prompts/insurance_prompts.py
# ================================================================================

# File 37/39: prompts/verification_prompts.py
# --------------------------------------------------------------------------------

# src/prompts/insurance_prompts.py

"""
Verification prompts for evaluating and correcting insurance policy analysis outputs.
"""

class VerificationPrompts:
    """
    Collection of prompts for verifying and correcting insurance analysis results.
    """

    @classmethod
    def verification_prompt_phi4(cls) -> str:
        """
        Verification prompt optimized for Phi-4 model.
        """
        return (
            # ROLE DEFINITION
            "You are an insurance policy verification expert. Your task is to review and potentially correct "
            "previous insurance coverage determinations.\n\n"
            
            # INPUT SECTIONS
            "ORIGINAL QUESTION:\n"
            "{{USER_QUESTION}}\n\n"
            
            "POLICY CONTEXT PROVIDED:\n"
            "{{POLICY_CONTEXT}}\n\n"
            
            "PREVIOUS ANALYSIS RESULT:\n"
            "{{PREVIOUS_RESULT}}\n\n"
            
            # VERIFICATION TASKS
            "VERIFICATION TASKS:\n"
            "1. Check if the eligibility decision is correct based on the policy context\n"
            "2. Verify that ALL quoted text actually exists in the policy context (character-for-character)\n"
            "3. Ensure the logic connecting the question to the policy text is sound\n"
            "4. Confirm that amount information (if any) is accurately extracted\n\n"
            
            # CRITICAL RULE
            "CRITICAL VERIFICATION RULE:\n"
            "If ANY quoted text in outcome_justification or payment_justification does NOT appear "
            "VERBATIM in the policy context above, you MUST:\n"
            "- Set outcome_justification to empty string \"\"\n"
            "- Set payment_justification to null\n"
            "- Change eligibility to \"No - Unrelated event\" if no relevant text exists\n\n"
            
            # DECISION LOGIC
            "DECISION PROCESS:\n"
            "1. If the previous result is CORRECT:\n"
            "   - Keep all fields exactly as they are\n"
            "   - Set verification_status to 'confirmed'\n\n"
            
            "2. If the previous result has ERRORS:\n"
            "   - Correct the eligibility if wrong\n"
            "   - Fix any misquoted text (must be verbatim from policy)\n"
            "   - Update justifications with correct quotes\n"
            "   - Set verification_status to 'corrected'\n\n"
            
            "3. Common errors to check:\n"
            "   - Eligibility says 'Yes' but no coverage text exists\n"
            "   - Quoted text not found in policy context\n"
            "   - Wrong eligibility category selected\n"
            "   - Hallucinated or paraphrased quotes\n\n"
            
            # OUTPUT FORMAT - MATCHING ORIGINAL
            "OUTPUT FORMAT (JSON only, matching original format exactly):\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"Yes|No - Unrelated event|No - condition(s) not met\",\n"
            "    \"outcome_justification\": \"<verbatim policy text or empty string>\",\n"
            "    \"payment_justification\": \"<verbatim amount text or null>\"\n"
            "  },\n"
            "  \"verification\": {\n"
            "    \"status\": \"confirmed|corrected\",\n"
            "    \"changes_made\": \"<brief description of corrections or 'none'>\"\n"
            "  }\n"
            "}\n\n"
            
            # STRICT RULES
            "CRITICAL RULES:\n"
            "- ONLY use text that appears verbatim in POLICY CONTEXT PROVIDED above\n"
            "- Never create new sentences or paraphrase\n"
            "- If you cannot find supporting text in the context, use empty string \"\"\n"
            "- For payment_justification: use null (not \"null\" string) when no amount exists\n"
            "- Output must start with '{' and end with '}'\n"
            "- No markdown, no commentary, just the JSON\n"
        )

    @classmethod
    def verification_prompt_qwen(cls) -> str:
        """
        Verification prompt optimized for Qwen models.
        """
        return (
            # SYSTEM ROLE
            "SYSTEM: Insurance verification expert - reviews and corrects coverage determinations\n"
            "CONSTRAINT: Output only text found verbatim in POLICY_CONTEXT\n\n"
            
            # INPUTS
            "INPUTS FOR VERIFICATION:\n"
            "=====================================\n"
            "USER_QUESTION:\n"
            "{{USER_QUESTION}}\n\n"
            
            "POLICY_CONTEXT:\n"
            "{{POLICY_CONTEXT}}\n\n"
            
            "PREVIOUS_RESULT:\n"
            "{{PREVIOUS_RESULT}}\n"
            "=====================================\n\n"
            
            # VERIFICATION CHECKLIST
            "VERIFICATION CHECKLIST:\n"
            "â–¡ Eligibility matches policy content?\n"
            "â–¡ ALL quotes exist verbatim in context?\n"
            "â–¡ Logic from questionâ†’policy is valid?\n"
            "â–¡ Amount correctly extracted?\n"
            "â–¡ No hallucinated content?\n\n"
            
            # CRITICAL VERIFICATION RULE
            "CRITICAL: If quoted text is NOT in policy context:\n"
            "â†’ Set outcome_justification = \"\"\n"
            "â†’ Set payment_justification = null\n"
            "â†’ Correct eligibility accordingly\n\n"
            
            # ERROR PATTERNS
            "COMMON ERROR PATTERNS:\n"
            "â€¢ 'Yes' with no supporting coverage text â†’ Change to 'No - Unrelated event'\n"
            "â€¢ Paraphrased quotes â†’ Replace with exact text or \"\"\n"
            "â€¢ Wrong eligibility category â†’ Correct based on policy\n"
            "â€¢ Created text not in policy â†’ Remove and use \"\"\n\n"
            
            # CORRECTION WORKFLOW
            "WORKFLOW:\n"
            "1. Parse previous result\n"
            "2. Verify each field against policy\n"
            "3. If correct â†’ status='confirmed'\n"
            "4. If errors â†’ fix and status='corrected'\n\n"
            
            # OUTPUT SCHEMA - EXACT FORMAT
            "REQUIRED OUTPUT (exact format):\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"[exact_value]\",\n"
            "    \"outcome_justification\": \"[exact_policy_text_or_empty]\",\n"
            "    \"payment_justification\": \"[exact_amount_text_or_null]\"\n"
            "  },\n"
            "  \"verification\": {\n"
            "    \"status\": \"confirmed|corrected\",\n"
            "    \"changes_made\": \"[what_was_fixed_or_none]\"\n"
            "  }\n"
            "}\n\n"
            
            # ENFORCEMENT
            "FORBIDDEN:\n"
            "Ã— Creating new sentences\n"
            "Ã— Using [...] or ellipsis\n"
            "Ã— Paraphrasing policy text\n"
            "Ã— Adding explanations outside JSON\n"
            "Ã— Using string \"null\" instead of null value\n"
        )

    @classmethod
    def get_verification_prompt(cls, model_type: str) -> str:
        """
        Get appropriate verification prompt based on model type.

        Args:
            model_type: Either 'phi4' or 'qwen'

        Returns:
            Verification prompt string
        """
        if 'phi' in model_type.lower():
            return cls.verification_prompt_phi4()
        elif 'qwen' in model_type.lower():
            return cls.verification_prompt_qwen()
        else:
            # Default to phi4 style
            return cls.verification_prompt_phi4()


# End of prompts/verification_prompts.py
# ================================================================================

# File 38/39: rag_runner.py
# --------------------------------------------------------------------------------

# src/rag_runner.py

import logging
from typing import Optional, Tuple, List

from config import *
from models.factory import get_model_client, get_shared_relevance_client
from models.verifier import SharedModelVerifier
from utils import read_questions, list_policy_paths
from models.vector_store import LocalVectorStore, EnhancedLocalVectorStore
from output_formatter import extract_policy_id, format_results_as_json, save_policy_json, \
    create_model_specific_output_dir
from prompts.insurance_prompts import InsurancePrompts

logger = logging.getLogger(__name__)


def load_complete_policy(pdf_path: str) -> str:
    """
    Load the complete text content of a policy PDF.

    Args:
        pdf_path: Path to the PDF file

    Returns:
        Complete text content of the policy
    """
    try:
        import PyPDF2

        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            full_text = []

            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                if text:
                    full_text.append(text)

            complete_text = "\n".join(full_text)
            logger.info(f"Loaded complete policy: {len(complete_text)} characters")
            return complete_text

    except Exception as e:
        logger.error(f"Error loading complete policy from {pdf_path}: {e}")
        # Try alternative method with pdfplumber
        try:
            import pdfplumber

            with pdfplumber.open(pdf_path) as pdf:
                full_text = []
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        full_text.append(text)

                complete_text = "\n".join(full_text)
                logger.info(f"Loaded complete policy with pdfplumber: {len(complete_text)} characters")
                return complete_text

        except Exception as e2:
            logger.error(f"Error with pdfplumber: {e2}")
            raise


def check_policy_size_for_model(policy_text: str, model_name: str) -> None:
    """
    Check if policy size might exceed model token limits and warn user.
    """
    # Rough estimation: 1 token â‰ˆ 4 characters
    estimated_tokens = len(policy_text) / 4

    # Model context windows (approximate)
    model_limits = {
        "gpt-4": 8192,
        "gpt-4-32k": 32768,
        "gpt-4o": 128000,
        "gpt-3.5": 4096,
        "phi-4": 100000,
        "qwen": 62768,
        "qwen-2.5-72b": 32768,
    }

    for model_key, limit in model_limits.items():
        if model_key in model_name.lower():
            if estimated_tokens > limit * 0.8:  # 80% threshold
                logger.warning(
                    f"Complete policy (~{int(estimated_tokens)} tokens) may exceed "
                    f"{model_name} context limit ({limit} tokens). "
                    f"Consider using RAG mode or a model with larger context."
                )
            break


def get_vector_store(strategy: str, model_name: str):
    """Factory function to get appropriate vector store based on strategy."""
    if strategy == "simple":
        return LocalVectorStore(model_name=model_name)
    elif strategy == "section":
        return EnhancedLocalVectorStore(
            model_name=model_name,
            chunking_strategy="section",
            chunking_config={
                "max_section_length": 2000,
                "preserve_subsections": True,
                "include_front_matter": False,
                "sentence_window_size": 5
            }
        )
    elif strategy == "smart_size":
        return EnhancedLocalVectorStore(
            model_name=model_name,
            chunking_strategy="smart_size",
            chunking_config={
                "base_chunk_words": 105,
                "min_chunk_words": 50,
                "max_chunk_words": 280,
                "importance_multiplier": 1.6,
                "preserve_complete_clauses": True,
                "overlap_words": 5
            }
        )
    elif strategy == "semantic":
        return EnhancedLocalVectorStore(
            model_name=model_name,
            chunking_strategy="semantic",
            chunking_config={
                "embedding_model": "all-MiniLM-L6-v2",
                "breakpoint_threshold_type": "percentile",
                "breakpoint_threshold_value": 75,
                "min_chunk_sentences": 2,
                "max_chunk_sentences": 15,
                "preserve_paragraph_boundaries": True,
                "device": "cpu"
            }
        )
    elif strategy == "graph":
        # NEW: Graph-based strategy for PankRAG
        return EnhancedLocalVectorStore(
            model_name=model_name,
            chunking_strategy="graph",
            chunking_config={
                "max_chunk_size": 512,
                "community_size": 50,
                "enable_hierarchical": True
            }
        )
    elif strategy == "semantic_graph":
        # Semantic graph-based strategy - combines embeddings with graph structure
        return EnhancedLocalVectorStore(
            model_name=model_name,
            chunking_strategy="semantic_graph",
            chunking_config={
                "max_chunk_size": 512,
                "similarity_threshold": 0.75,
                "min_community_size": 3,
                "enable_hierarchical": False,
                "embedding_model": "all-MiniLM-L6-v2",
                "semantic_window": 5,
                "entity_weight": 1.5,
                "sequential_weight": 0.8
            }
        )
    elif strategy == "hybrid":
        return EnhancedLocalVectorStore(
            model_name=model_name,
            chunking_strategy="hybrid",
            chunking_config={
                "max_chunk_words": 500,
                "min_chunk_words": 50,
                "overlap_words": 20,
                "semantic_threshold": 0.75,
                "embedding_model": "all-MiniLM-L6-v2",
                "include_cross_references": True,
                "preserve_tables": False
            }
        )
    else:
        raise ValueError(f"Unknown RAG strategy: {strategy}")


def check_query_relevance(
        question: str,
        context_chunks: List[str],
        relevance_client
) -> Tuple[bool, str]:
    """
    Check if a query is relevant to the policy content.

    Args:
        question: The user query
        context_chunks: Retrieved context chunks from the policy
        relevance_client: Model client initialized with the relevance filter prompt

    Returns:
        Tuple of (is_relevant, reason)
    """
    try:
        # Use the relevance client to query
        response = relevance_client.query(question, context_files=context_chunks)

        # Extract the relevance information from the response
        if isinstance(response, dict):
            is_relevant = response.get("is_relevant", True)  # Default to True if key missing
            reason = response.get("reason", "No reason provided")
            return bool(is_relevant), str(reason)  # Ensure proper types
        else:
            logger.warning(f"Unexpected response format from relevance check: {response}")
            return True, "Unexpected response format, assuming relevant"

    except Exception as e:
        logger.warning(f"Error in relevance check: {e}, assuming relevant")
        return True, "Error in relevance check, assuming relevant"


def run_rag(
        model_provider: str = "openai",
        model_name: str = "gpt-4o",
        max_questions: Optional[int] = None,
        output_dir: Optional[str] = None,
        prompt_name: str = "standard",
        use_persona: bool = False,
        question_ids: Optional[list] = None,
        policy_id: Optional[str] = None,
        k: int = 3,
        filter_irrelevant: bool = False,
        relevance_prompt_name: str = "relevance_filter_v1",
        rag_strategy: str = "simple",
        complete_policy: bool = False,
        use_verifier: bool = False,
        verifier_iterations: int = 1,
) -> None:
    """
    Executes the RAG pipeline using a modular model client, either OpenAI or HuggingFace.
    Now generates a JSON file for each policy with question results.

    Args:
        model_provider (str): One of "openai" or "hf" (Hugging Face).
        model_name (str): Model name (e.g., "gpt-4o" or "microsoft/phi-4").
        max_questions (Optional[int]): Maximum number of questions to process (None = all questions).
        output_dir (Optional[str]): Directory to save JSON output files.
        prompt_name (str): Name of the prompt template to use.
        use_persona (bool): Whether to use persona extraction for the queries (default: False).
        question_ids (Optional[list]): List of question IDs to process (None = all questions).
        policy_id (Optional[str]): Filter to only process a specific policy ID (None = all policies).
        k (int): Number of context chunks to retrieve for each question (default: 3).
        filter_irrelevant (bool): Whether to filter out irrelevant context chunks (default: False).
        relevance_prompt_name (str): Name of the prompt template to use for relevance filtering.
        rag_strategy (str): Name of the approach strategy
        complete_policy (bool): Whether to pass the complete policy document instead of using RAG (default: False)
        use_verifier (bool): Whether to use verification to review and correct results (default: False)
        verifier_iterations (int): Number of verification iterations to perform (default: 1)
    """
    # Select the prompt template
    try:
        # Get the prompt by name from our InsurancePrompts class
        sys_prompt = InsurancePrompts.get_prompt(prompt_name)
        logger.info(f"Using prompt template: {prompt_name}")
    except ValueError as e:
        # If prompt not found, fall back to standard prompt
        logger.warning(f"Prompt selection error: {str(e)}. Falling back to standard prompt.")
        sys_prompt = InsurancePrompts.standard_coverage()

    # Create output directory for JSON files
    if output_dir is None:
        output_dir = os.path.join(base_dir, JSON_PATH)

    model_output_dir = create_model_specific_output_dir(
        output_dir, model_name, k, complete_policy=complete_policy, prompt_name=prompt_name
    )
    logger.info(f"JSON output will be saved to: {model_output_dir}")

    # List all policy PDFs
    pdf_paths = list_policy_paths(DOCUMENT_DIR)
    if not pdf_paths:
        logger.error("No PDF policies found in directory")
        return

    # Filter policies by ID if specified
    if policy_id:
        filtered_paths = []
        for path in pdf_paths:
            if extract_policy_id(path) == policy_id:
                filtered_paths.append(path)
                logger.info(f"Found policy with ID {policy_id}: {os.path.basename(path)}")

        if not filtered_paths:
            logger.warning(f"No policy found with ID {policy_id}. Check if the policy exists and ID is correct.")
            return

        pdf_paths = filtered_paths
        logger.info(f"Filtered to {len(pdf_paths)} policies matching ID {policy_id}")

    # Read questions (use max_questions if provided, otherwise use all questions)
    questions_df = read_questions(DATASET_PATH)

    # First filter by question_ids if provided
    if question_ids:
        questions_df = questions_df[questions_df["Id"].astype(str).isin(question_ids)]
        logger.info(f"Filtered to {len(questions_df)} questions by ID: {', '.join(question_ids)}")

    # Then apply max_questions limit
    if max_questions is not None:
        # Limit to specified number of questions
        questions = questions_df[["Id", "Questions"]].to_numpy()[:max_questions]
        logger.info(f"Processing {len(questions)} out of {len(questions_df)} questions (limited by --num-questions)")
    else:
        # Use all questions (or all filtered by ID)
        questions = questions_df[["Id", "Questions"]].to_numpy()
        logger.info(f"Processing all {len(questions)} questions")

    # Process each policy
    for pdf_path in pdf_paths:
        current_policy_id = extract_policy_id(pdf_path)
        logger.info(f"Processing policy ID: {current_policy_id} from file: {os.path.basename(pdf_path)}")

        # Initialize the model client
        model_client = get_model_client(model_provider, model_name, sys_prompt)

        # Set up verifier if requested
        verifier = None
        if use_verifier:
            # Determine model type for verification prompt selection
            model_type = "phi4" if "phi" in model_name.lower() else "qwen"
            verifier = SharedModelVerifier(model_client, model_type=model_type)
            logger.info(f"Verification enabled with {verifier_iterations} iteration(s)")

        # Initialize relevance checking client if filtering is enabled and not in complete policy mode
        relevance_client = None
        if filter_irrelevant and not complete_policy:
            try:
                relevance_prompt = InsurancePrompts.get_prompt(relevance_prompt_name)
                # Use shared model client to avoid loading a second model instance
                relevance_client = get_shared_relevance_client(model_client, relevance_prompt)
                logger.info(f"Relevance filtering enabled - using shared model with prompt: {relevance_prompt_name}")
            except ValueError as e:
                logger.warning(f"Relevance prompt selection error: {str(e)}. Falling back to relevance_filter_v1.")
                relevance_prompt = InsurancePrompts.relevance_filter_v1()
                relevance_client = get_shared_relevance_client(model_client, relevance_prompt)

        # Handle complete policy mode vs RAG mode
        complete_policy_text = None
        context_provider = None

        if complete_policy:
            # Load the complete policy text
            logger.info(f"Loading complete policy document for policy: {current_policy_id}")
            try:
                complete_policy_text = load_complete_policy(pdf_path)
                logger.info(f"Complete policy loaded: {len(complete_policy_text)} characters")

                # Check if policy size might exceed model limits
                check_policy_size_for_model(complete_policy_text, model_name)
            except Exception as e:
                logger.error(f"Failed to load complete policy: {e}")
                continue
        else:
            # Initialize vector store with just this policy file
            try:
                logger.info(f"Initializing vector store for policy: {current_policy_id}")
                context_provider = get_vector_store(rag_strategy, EMBEDDING_MODEL_PATH)
                context_provider.index_documents([pdf_path])
                logger.info(f"Vector store initialized successfully for policy: {current_policy_id}")
            except Exception as e:
                logger.error(f"Error initializing vector store for policy {current_policy_id}: {e}")
                logger.info("Continuing without vector store")
                context_provider = None

        # Process all questions for this policy
        policy_results = []
        verification_info_dict = {}  # Store verification info for each question

        for q_id, question in questions:
            try:
                logger.info(f"â†’ Querying policy {current_policy_id} with question {q_id}: {question}")

                # Get context based on mode
                context_texts = []

                if complete_policy:
                    # Use complete policy as context
                    context_texts = [complete_policy_text]
                    logger.info(f"Using complete policy as context ({len(complete_policy_text)} characters)")
                else:
                    # Get relevant context if vector store is available
                    if context_provider:
                        try:
                            context_texts = context_provider.retrieve(question, k=k)
                            logger.info(f"Retrieved {len(context_texts)} context chunks")
                        except Exception as e:
                            logger.error(f"Error retrieving context: {e}")

                    # Only check relevance if filtering is enabled and we have context chunks
                    if filter_irrelevant and relevance_client and context_texts:
                        # First assess query relevance using the lightweight filter
                        is_relevant, reason = check_query_relevance(question, context_texts, relevance_client)

                        if not is_relevant:
                            logger.info(f"âœ“ Question {q_id} marked as IRRELEVANT: {reason}")
                            # Create standardized "irrelevant" response and skip main processing
                            result_row = [
                                model_name,
                                str(q_id),
                                question,
                                "No - Unrelated event",
                                "",
                                None,
                            ]
                            policy_results.append(result_row)
                            logger.info(f"Added irrelevant result: {result_row}")
                            continue  # Skip to next question
                        else:
                            logger.info(
                                f"âœ“ Question {q_id} IS RELEVANT: {reason} - proceeding with detailed analysis")

                # Query the model with the question and context
                response = model_client.query(question, context_files=context_texts, use_persona=use_persona)

                # COMPREHENSIVE LOGGING OF MODEL RESPONSE
                logger.info(f"=== RAG RESPONSE LOGGING Q{q_id} ===")
                logger.info(f"Raw model response: {response}")
                logger.info(f"Response type: {type(response)}")

                # Apply verification if enabled
                verification_info = None
                if use_verifier and verifier:
                    logger.info(f"=== STARTING VERIFICATION FOR Q{q_id} ===")
                    try:
                        response, verification_info = verifier.verify_result(
                            question=question,
                            context_texts=context_texts,
                            previous_result=response,
                            iterations=verifier_iterations
                        )
                        logger.info(f"Verification completed: {verification_info}")
                        logger.info(f"Final verified response: {response}")
                        verification_info_dict[str(q_id)] = verification_info
                    except Exception as e:
                        logger.error(f"Verification failed: {e}, using original response")
                        verification_info = {"status": "error", "error": str(e)}
                        verification_info_dict[str(q_id)] = verification_info

                if isinstance(response, dict) and "answer" in response:
                    answer = response["answer"]
                    logger.info(f"Answer section: {answer}")
                    logger.info(f"Answer keys: {list(answer.keys())}")

                    # Log each field extraction
                    eligibility = answer.get('eligibility', 'MISSING')
                    outcome_just = answer.get('outcome_justification', 'MISSING')
                    payment_just = answer.get('payment_justification', 'MISSING')

                    logger.info(f"Eligibility: '{eligibility}'")
                    logger.info(f"Outcome justification: '{outcome_just}'")
                    logger.info(f"Payment justification: '{payment_just}'")

                    # Also check for old field names (debugging)
                    old_eligibility_policy = answer.get('eligibility_policy', 'NOT_FOUND')
                    old_amount_policy = answer.get('amount_policy', 'NOT_FOUND')
                    logger.info(f"OLD eligibility_policy (should be NOT_FOUND): '{old_eligibility_policy}'")
                    logger.info(f"OLD amount_policy (should be NOT_FOUND): '{old_amount_policy}'")
                else:
                    logger.warning(f"Unexpected response format: {response}")

                result_row = [
                    model_name,
                    str(q_id),
                    question,
                    response.get("answer", {}).get("eligibility", ""),
                    response.get("answer", {}).get("outcome_justification", ""),  # NEW FIELD NAME
                    response.get("answer", {}).get("payment_justification", ""),  # NEW FIELD NAME
                ]

                logger.info(f"Created result_row: {result_row}")
                logger.info(f"Result row length: {len(result_row)}")
                logger.info(f"=== END RAG RESPONSE LOGGING ===")

                policy_results.append(result_row)
                logger.info(f"âœ“ Processed question {q_id} for policy {current_policy_id}")

            except Exception as e:
                logger.error(f"âœ— Error processing question {q_id} for policy {current_policy_id}: {e}")
                error_result = [model_name, str(q_id), question, "Error", str(e), "", ""]
                policy_results.append(error_result)
                logger.info(f"Added error result: {error_result}")

        # COMPREHENSIVE LOGGING BEFORE JSON FORMATTING
        logger.info(f"=== FINAL POLICY RESULTS FOR {current_policy_id} ===")
        logger.info(f"Number of results: {len(policy_results)}")
        for i, result in enumerate(policy_results):
            logger.info(f"Result {i}: {result}")
        logger.info(f"=== END POLICY RESULTS ===")

        # Format and save policy JSON
        policy_json = format_results_as_json(pdf_path, policy_results)

        # COMPREHENSIVE LOGGING OF FINAL JSON
        logger.info(f"=== GENERATED JSON FOR {current_policy_id} ===")
        logger.info(f"JSON keys: {list(policy_json.keys())}")
        logger.info(f"Policy ID in JSON: {policy_json.get('policy_id', 'MISSING')}")

        if "questions" in policy_json:
            logger.info(f"JSON questions count: {len(policy_json['questions'])}")
            # Log first 3 questions in detail
            for i, q in enumerate(policy_json["questions"][:3]):
                logger.info(f"JSON Question {i}: {q}")
            if len(policy_json["questions"]) > 3:
                logger.info(f"... and {len(policy_json['questions']) - 3} more questions")

        logger.info(f"Complete JSON structure: {policy_json}")
        logger.info(f"=== END GENERATED JSON ===")

        save_policy_json(
            policy_json, output_dir, model_name, k,
            use_timestamp=True, complete_policy=complete_policy, prompt_name=prompt_name
        )
        logger.info(f"Saved JSON for policy {current_policy_id}")

    logger.info("âœ… RAG run completed successfully.")


def run_batch_rag(
        model_provider: str = "openai",
        model_name: str = "gpt-4o",
        max_questions: Optional[int] = None,
        output_dir: Optional[str] = None,
        prompt_name: str = "standard",
        use_persona: bool = False,
        question_ids: Optional[list] = None,
        policy_id: Optional[str] = None,
        k: int = 3,
        filter_irrelevant: bool = False,
        relevance_prompt_name: str = "relevance_filter_v1",
        rag_strategy: str = "simple",
        complete_policy: bool = False,
        use_verifier: bool = False,
        verifier_iterations: int = 1,
) -> None:
    """
    Alternative implementation that processes all policies together and then
    organizes results into separate JSON files.

    Args:
        model_provider (str): One of "openai" or "hf" (Hugging Face).
        model_name (str): Model name (e.g., "gpt-4o" or "microsoft/phi-4").
        max_questions (Optional[int]): Maximum number of questions to process (None = all questions).
        output_dir (Optional[str]): Directory to save JSON output files.
        prompt_name (str): Name of the prompt template to use.
        use_persona (bool): Whether to use persona extraction for the queries (default: False).
        question_ids (Optional[list]): List of question IDs to process (None = all questions).
        policy_id (Optional[str]): Filter to only process a specific policy ID (None = all policies).
        k (int): Number of context chunks to retrieve for each question (default: 3).
        filter_irrelevant (bool): Whether to filter out irrelevant context chunks (default: False).
        relevance_prompt_name (str): Name of the prompt template to use for relevance filtering.
        rag_strategy (str): Name of the approach strategy
        complete_policy (bool): Whether to pass the complete policy document instead of using RAG (default: False)
        use_verifier (bool): Whether to use verification to review and correct results (default: False)
        verifier_iterations (int): Number of verification iterations to perform (default: 1)
    """
    # Select the prompt template
    try:
        # Get the prompt by name from our InsurancePrompts class
        sys_prompt = InsurancePrompts.get_prompt(prompt_name)
        logger.info(f"Using prompt template: {prompt_name}")
    except ValueError as e:
        # If prompt not found, fall back to standard prompt
        logger.warning(f"Prompt selection error: {str(e)}. Falling back to standard prompt.")
        sys_prompt = InsurancePrompts.standard_coverage()

    # Initialize the model client
    model_client = get_model_client(model_provider, model_name, sys_prompt)

    # Set up verifier if requested
    verifier = None
    if use_verifier:
        # Determine model type for verification prompt selection
        model_type = "phi4" if "phi" in model_name.lower() else "qwen"
        verifier = SharedModelVerifier(model_client, model_type=model_type)
        logger.info(f"Verification enabled with {verifier_iterations} iteration(s)")

    # Initialize relevance checking client if filtering is enabled and not in complete policy mode
    relevance_client = None
    if filter_irrelevant and not complete_policy:
        try:
            relevance_prompt = InsurancePrompts.get_prompt(relevance_prompt_name)
            # Use shared model client to avoid loading a second model instance
            relevance_client = get_shared_relevance_client(model_client, relevance_prompt)
            logger.info(f"Relevance filtering enabled - using shared model with prompt: {relevance_prompt_name}")
        except ValueError as e:
            logger.warning(f"Relevance prompt selection error: {str(e)}. Falling back to relevance_filter_v1.")
            relevance_prompt = InsurancePrompts.relevance_filter_v1()
            relevance_client = get_shared_relevance_client(model_client, relevance_prompt)

    # List all policy PDFs
    pdf_paths = list_policy_paths(DOCUMENT_DIR)

    # Filter policies by ID if specified
    if policy_id:
        filtered_paths = []
        for path in pdf_paths:
            if extract_policy_id(path) == policy_id:
                filtered_paths.append(path)
                logger.info(f"Found policy with ID {policy_id}: {os.path.basename(path)}")

        if not filtered_paths:
            logger.warning(f"No policy found with ID {policy_id}. Check if the policy exists and ID is correct.")
            return

        pdf_paths = filtered_paths
        logger.info(f"Filtered to {len(pdf_paths)} policies matching ID {policy_id}")

    # Structure to store results by policy
    policy_results = {}
    policy_verification_info = {}  # Store verification info by policy

    # Create output directory for JSON files
    if output_dir is None:
        output_dir = os.path.join(base_dir, "resources/results/json_output")

    model_output_dir = create_model_specific_output_dir(
        output_dir, model_name, k, complete_policy=complete_policy, prompt_name=prompt_name
    )
    logger.info(f"JSON output will be saved to: {model_output_dir}")

    # Structure to store complete policies if needed
    complete_policies = {}

    # Initialize and populate based on mode
    context_provider = None
    if complete_policy:
        # Load all complete policies
        logger.info(f"Loading complete text for {len(pdf_paths)} policies")
        for pdf_path in pdf_paths:
            current_policy_id = extract_policy_id(pdf_path)
            try:
                policy_text = load_complete_policy(pdf_path)
                complete_policies[current_policy_id] = policy_text
                logger.info(f"Loaded complete policy {current_policy_id}: {len(policy_text)} characters")

                # Check if policy size might exceed model limits
                check_policy_size_for_model(policy_text, model_name)
            except Exception as e:
                logger.error(f"Failed to load policy {current_policy_id}: {e}")
    else:
        # Initialize and populate the vector store with all PDFs
        if pdf_paths:
            logger.info(f"Initializing vector store with {len(pdf_paths)} PDF documents")
            try:
                context_provider = get_vector_store(rag_strategy, EMBEDDING_MODEL_PATH)
                context_provider.index_documents(pdf_paths)
                logger.info(f"Vector store initialized successfully")
            except Exception as e:
                logger.error(f"Error initializing vector store: {e}")
                logger.info("Continuing without vector store")

    questions_df = read_questions(DATASET_PATH)
    # First filter by question_ids if provided
    if question_ids:
        questions_df = questions_df[questions_df["Id"].astype(str).isin(question_ids)]
        logger.info(f"Filtered to {len(questions_df)} questions by ID: {', '.join(question_ids)}")

    # Then apply max_questions limit
    if max_questions is not None:
        # Limit to specified number of questions
        questions = questions_df[["Id", "Questions"]].to_numpy()[:max_questions]
        logger.info(f"Processing {len(questions)} out of {len(questions_df)} questions (limited by --num-questions)")
    else:
        # Use all questions (or all filtered by ID)
        questions = questions_df[["Id", "Questions"]].to_numpy()
        logger.info(f"Processing all {len(questions)} questions")

    # Initialize policy result dictionaries
    for pdf_path in pdf_paths:
        current_policy_id = extract_policy_id(pdf_path)
        policy_results[current_policy_id] = {
            "policy_path": pdf_path,
            "results": []
        }
        policy_verification_info[current_policy_id] = {}

    # Process all questions for all policies
    for q_id, question in questions:
        try:
            logger.info(f"â†’ Querying: {question}")

            # Process each policy separately for this question
            for pdf_path in pdf_paths:
                current_policy_id = extract_policy_id(pdf_path)
                logger.info(f"  Processing policy {current_policy_id}")

                # Get context based on mode
                context_texts = []

                if complete_policy:
                    # Use complete policy text
                    if current_policy_id in complete_policies:
                        context_texts = [complete_policies[current_policy_id]]
                        logger.info(
                            f"Using complete policy for {current_policy_id} ({len(context_texts[0])} characters)")
                    else:
                        logger.error(f"Complete policy not loaded for {current_policy_id}")
                        continue
                else:
                    # Get relevant context for this policy
                    if context_provider:
                        try:
                            # Try to retrieve context specifically for this policy
                            context_texts = context_provider.retrieve(query=question, k=k, policy_id=current_policy_id)
                            logger.info(f"Retrieved {len(context_texts)} context chunks for policy {current_policy_id}")
                        except Exception as e:
                            logger.error(f"Error retrieving context: {e}")

                    # Only check relevance in RAG mode
                    if filter_irrelevant and relevance_client and context_texts:
                        # First assess query relevance using the lightweight filter
                        is_relevant, reason = check_query_relevance(question, context_texts, relevance_client)

                        if not is_relevant:
                            logger.info(
                                f"âœ“ Question {q_id} marked as IRRELEVANT for policy {current_policy_id}: {reason}")
                            # Create standardized "irrelevant" response and skip main processing
                            result_row = [
                                model_name,
                                str(q_id),
                                question,
                                "No - Unrelated event",
                                "",
                                None,
                            ]
                            policy_results[current_policy_id]["results"].append(result_row)
                            logger.info(f"Added irrelevant result for policy {current_policy_id}: {result_row}")
                            continue  # Skip to next policy for this question
                        else:
                            logger.info(
                                f"âœ“ Question {q_id} IS RELEVANT for policy {current_policy_id}: {reason} - proceeding with detailed analysis")

                # Query the model with the question and context
                response = model_client.query(question, context_files=context_texts, use_persona=use_persona)

                # COMPREHENSIVE LOGGING OF MODEL RESPONSE (BATCH VERSION)
                logger.info(f"=== BATCH RAG RESPONSE Q{q_id} P{current_policy_id} ===")
                logger.info(f"Raw model response: {response}")
                logger.info(f"Response type: {type(response)}")

                # Apply verification if enabled
                verification_info = None
                if use_verifier and verifier:
                    logger.info(f"=== STARTING VERIFICATION FOR Q{q_id} P{current_policy_id} ===")
                    try:
                        response, verification_info = verifier.verify_result(
                            question=question,
                            context_texts=context_texts,
                            previous_result=response,
                            iterations=verifier_iterations
                        )
                        logger.info(f"Verification completed: {verification_info}")
                        logger.info(f"Final verified response: {response}")
                        policy_verification_info[current_policy_id][str(q_id)] = verification_info
                    except Exception as e:
                        logger.error(f"Verification failed: {e}, using original response")
                        verification_info = {"status": "error", "error": str(e)}
                        policy_verification_info[current_policy_id][str(q_id)] = verification_info

                if isinstance(response, dict) and "answer" in response:
                    answer = response["answer"]
                    logger.info(f"Answer section: {answer}")
                    logger.info(f"Answer keys: {list(answer.keys())}")

                    # Log each field extraction
                    eligibility = answer.get('eligibility', 'MISSING')
                    outcome_just = answer.get('outcome_justification', 'MISSING')
                    payment_just = answer.get('payment_justification', 'MISSING')

                    logger.info(f"Eligibility: '{eligibility}'")
                    logger.info(f"Outcome justification: '{outcome_just}'")
                    logger.info(f"Payment justification: '{payment_just}'")

                    # Also check for old field names (debugging)
                    old_eligibility_policy = answer.get('eligibility_policy', 'NOT_FOUND')
                    old_amount_policy = answer.get('amount_policy', 'NOT_FOUND')
                    logger.info(f"OLD eligibility_policy (should be NOT_FOUND): '{old_eligibility_policy}'")
                    logger.info(f"OLD amount_policy (should be NOT_FOUND): '{old_amount_policy}'")
                else:
                    logger.warning(f"Unexpected response format: {response}")

                result_row = [
                    model_name,
                    str(q_id),
                    question,
                    response.get("answer", {}).get("eligibility", ""),
                    response.get("answer", {}).get("outcome_justification", ""),  # NEW FIELD NAME
                    response.get("answer", {}).get("payment_justification", ""),  # NEW FIELD NAME
                ]

                logger.info(f"Batch result_row for policy {current_policy_id}: {result_row}")
                logger.info(f"=== END BATCH RAG RESPONSE ===")

                policy_results[current_policy_id]["results"].append(result_row)
                logger.info(f"âœ“ Processed question {q_id} for policy {current_policy_id}")

        except Exception as e:
            logger.error(f"âœ— Error processing question {q_id}: {e}")
            # Add error result to all policies
            for current_policy_id in policy_results:
                error_result = [model_name, str(q_id), question, "Error", str(e), "", ""]
                policy_results[current_policy_id]["results"].append(error_result)
                logger.info(f"Added error result for policy {current_policy_id}: {error_result}")

    # Format and save results for each policy
    for current_policy_id, data in policy_results.items():
        # COMPREHENSIVE LOGGING BEFORE JSON FORMATTING (BATCH VERSION)
        logger.info(f"=== BATCH FINAL POLICY RESULTS FOR {current_policy_id} ===")
        logger.info(f"Number of results: {len(data['results'])}")
        for i, result in enumerate(data["results"]):
            logger.info(f"Result {i}: {result}")
        logger.info(f"=== END BATCH POLICY RESULTS ===")

        # Get verification info for this policy if available
        # verification_dict = policy_verification_info.get(current_policy_id, {}) if use_verifier else None

        policy_json = format_results_as_json(data["policy_path"], data["results"])

        # COMPREHENSIVE LOGGING OF FINAL JSON (BATCH VERSION)
        logger.info(f"=== BATCH GENERATED JSON FOR {current_policy_id} ===")
        logger.info(f"JSON keys: {list(policy_json.keys())}")
        logger.info(f"Policy ID in JSON: {policy_json.get('policy_id', 'MISSING')}")

        if "questions" in policy_json:
            logger.info(f"JSON questions count: {len(policy_json['questions'])}")
            # Log first 3 questions in detail
            for i, q in enumerate(policy_json["questions"][:3]):
                logger.info(f"JSON Question {i}: {q}")
            if len(policy_json["questions"]) > 3:
                logger.info(f"... and {len(policy_json['questions']) - 3} more questions")

        logger.info(f"Complete JSON structure: {policy_json}")
        logger.info(f"=== END BATCH GENERATED JSON ===")

        save_policy_json(
            policy_json, output_dir, model_name, k,
            use_timestamp=True, complete_policy=complete_policy, prompt_name=prompt_name
        )
        logger.info(f"Saved JSON for policy {current_policy_id}")

    logger.info("âœ… RAG run completed successfully.")


# End of rag_runner.py
# ================================================================================

# File 39/39: utils.py
# --------------------------------------------------------------------------------

# src/utils.py
import json

import pandas as pd
import os
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def read_questions(path: str) -> pd.DataFrame:
    """
    Load an Excel file containing insurance-related questions.

    Args:
        path (str): Path to the Excel file.

    Returns:
        pd.DataFrame: DataFrame of questions.
    """
    logger.info(f"Read question path: {path}")
    df = pd.read_excel(path)
    return df

# def list_pdf_paths(directory: str) -> List[str]:
#     """
#     List all PDF file paths within the specified directory.
#
#     Args:
#         directory (str): Path to the directory containing PDFs.
#
#     Returns:
#         list[str]: List of absolute file paths.
#     """
#     logger.info(f"List pdf paths: {directory}")
#     return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(".pdf")]

def load_response_schema(path: str) -> Dict[str, Any]:
    """
    Load the JSON schema used to enforce assistant response structure.

    Args:
        path (str): Path to the JSON schema file.

    Returns:
        dict: JSON schema dictionary.
    """
    logger.info(f"Load response schema: {path}")
    with open(path, 'r', encoding='utf-8') as file:
        return json.load(file)


def list_policy_paths(directory: str) -> List[str]:
    """
    List all PDF and TXT file paths within the specified directory.

    Args:
        directory (str): Path to the directory containing policy files.

    Returns:
        list[str]: List of absolute file paths.
    """
    logger.info(f"List policy paths: {directory}")
    policy_files = []

    for f in os.listdir(directory):
        if f.endswith((".pdf", ".txt")):
            policy_files.append(os.path.join(directory, f))

    logger.info(f"Found {len(policy_files)} policy files (PDF and TXT)")
    return policy_files






# End of utils.py
# ================================================================================
