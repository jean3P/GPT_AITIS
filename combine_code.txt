# Combined Python Code from: /cluster/home/jeanpool.pereyrap/repos/GPT_AITIS/src
# Excluded directories: ./src/scripts
# Total files combined: 41
# ================================================================================


# File 1/41: assistant_manager.py
# --------------------------------------------------------------------------------

# src/assistant_manager.py

import openai
import logging
from config import OPENAI_API_KEY, VECTOR_STORE_EXPIRATION_DAYS, RESPONSE_FORMAT_PATH
from typing import List, Any

from utils import load_response_schema

logger = logging.getLogger(__name__)

client = openai.OpenAI(api_key=OPENAI_API_KEY)

def create_vector_store(name: str, file_paths: List[str], check: bool =True) -> Any:
    """
    Create a vector store in OpenAI's API, upload PDF files to it, and index them.

    Args:
        name (str): Name of the vector store.
        file_paths (list[str]): List of file paths to upload.
        check (bool): For logging purposes

    Returns:
        OpenAI VectorStore object
    """

    logger.info(f"Creating vector store")
    vector_store = client.vector_stores.create(
        name=name,
        expires_after={"anchor": "last_active_at", "days": VECTOR_STORE_EXPIRATION_DAYS},
    )
    streams = [open(path, "rb") for path in file_paths]
    file_batch = client.vector_stores.file_batches.upload_and_poll(vector_store_id=vector_store.id, files=streams)
    if check:
        logger.info(f"  File Batch Status: {file_batch.status}")
        logger.info(f"  File Batch Counts: {file_batch.file_counts}")

    return vector_store

def create_assistant(name: str, sys_prompt: str, model: str = "gpt-4o") -> Any:
    """
    Create an OpenAI Assistant configured to perform file search using a given system prompt.

    Args:
        name (str): Name of the assistant.
        sys_prompt (str): System-level instructions for the assistant.
        model (str): Model to use for generating responses.

    Returns:
        OpenAI Assistant object
    """
    logger.info(f"Creating assistant")
    response_schema = load_response_schema(RESPONSE_FORMAT_PATH)

    return client.beta.assistants.create(
        name=name,
        instructions=sys_prompt,
        model=model,
        tools=[{"type": "file_search"}],
        response_format={"type": "json_schema", "json_schema": response_schema}
    )


def update_assistant_vector(assistant_id: str, vector_store_id: str) -> None:
    """
    Link an assistant to a vector store so it can use file search during conversation.

    Args:
        assistant_id (str): The ID of the assistant.
        vector_store_id (str): The ID of the vector store.
    """
    logger.info(f"Update assistant vector")
    client.beta.assistants.update(
        assistant_id=assistant_id,
        tool_resources={"file_search": {"vector_store_ids": [vector_store_id]}}
    )


# End of assistant_manager.py
# ================================================================================

# File 2/41: config.py
# --------------------------------------------------------------------------------

# src/config.py
import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()
base_dir = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Load environment and configuration variables
OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY")
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
MODEL_NAME: str = "gpt-4o"
EMBEDDING_MODEL: str = "all-MiniLM-L6-v2"  # Keep for backward compatibility
OPENROUTER_API_KEY: str = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_SITE_URL: str = os.getenv("OPENROUTER_SITE_URL", "")  # Optional
OPENROUTER_SITE_NAME: str = os.getenv("OPENROUTER_SITE_NAME", "")  # Optional

# Configure embedding model path for the downloaded sentence transformer model
EMBEDDING_MODEL_PATH = "/cluster/scratch/jeanpool.pereyrap/models/embeddings/sentence-transformers_all-MiniLM-L6-v2"

# Verify the model exists, otherwise fall back to remote
if os.path.exists(EMBEDDING_MODEL_PATH):
    print(f"Using local embedding model from: {EMBEDDING_MODEL_PATH}")
else:
    # Fall back to HuggingFace model if local copy not found
    print(f"Local model not found at {EMBEDDING_MODEL_PATH}")
    print(f"Falling back to remote model")
    EMBEDDING_MODEL_PATH = "sentence-transformers/all-MiniLM-L6-v2"

# HuggingFace cache settings
# Set environment variables if not already set
if "HF_HUB_CACHE" not in os.environ:
    os.environ["HF_HUB_CACHE"] = "/cluster/scratch/cache/huggingface/hub"
if "HF_ASSETS_CACHE" not in os.environ:
    os.environ["HF_ASSETS_CACHE"] = "/cluster/scratch/cache/huggingface/assets"

# Constants for reference
HF_HUB_CACHE: str = os.environ["HF_HUB_CACHE"]
HF_ASSETS_CACHE: str = os.environ["HF_ASSETS_CACHE"]

# =============================================================================
# MODEL CONFIGURATION AND PATH MANAGEMENT
# =============================================================================

# Base directory for locally downloaded models
MODELS_BASE_DIR = f"/cluster/scratch/{os.environ.get('USER', 'jeanpool.pereyrap')}/models"

# Model path mappings - maps HuggingFace model names to local directories
MODEL_PATHS = {
    # Phi models
    "microsoft/phi-4": os.path.join(MODELS_BASE_DIR, "phi-4"),

    # Qwen models
    "Qwen/Qwen2.5-32B": os.path.join(MODELS_BASE_DIR, "qwen2.5-32b"),
    "Qwen/Qwen2.5-7B": os.path.join(MODELS_BASE_DIR, "qwen2.5-7b"),
    # "Qwen/Qwen2.5-14B": os.path.join(MODELS_BASE_DIR, "qwen2.5-14b"),

    # Local name aliases for convenience
    "phi-4": os.path.join(MODELS_BASE_DIR, "phi-4"),
    "qwen2.5-32b": os.path.join(MODELS_BASE_DIR, "qwen2.5-32b"),
    "qwen2.5-7b": os.path.join(MODELS_BASE_DIR, "qwen2.5-7b"),
    # "qwen2.5-14b": os.path.join(MODELS_BASE_DIR, "qwen2.5-14b"),
}

OPENROUTER_MODELS = {
    # Qwen models via OpenRouter
    "qwen/qwen-2.5-72b-instruct": "qwen/qwen-2.5-72b-instruct",
    "qwen/qwen-2.5-32b-instruct": "qwen/qwen-2.5-32b-instruct",
    "qwen/qwen-2.5-14b-instruct": "qwen/qwen-2.5-14b-instruct",
    "qwen/qwen-2.5-7b-instruct": "qwen/qwen-2.5-7b-instruct",
    "qwen/qwen-2.5-3b-instruct": "qwen/qwen-2.5-3b-instruct",

    # Other popular models
    "anthropic/claude-3.5-sonnet": "anthropic/claude-3.5-sonnet",
    "openai/gpt-4o": "openai/gpt-4o",
    "openai/gpt-4o-mini": "openai/gpt-4o-mini",
    "meta-llama/llama-3.1-405b-instruct": "meta-llama/llama-3.1-405b-instruct",
    "meta-llama/llama-3.1-70b-instruct": "meta-llama/llama-3.1-70b-instruct",
    "meta-llama/llama-3.1-8b-instruct": "meta-llama/llama-3.1-8b-instruct",
}


def get_openrouter_model_name(model_name: str) -> str:
    """Get the OpenRouter model name, with fallback to the input name."""
    return OPENROUTER_MODELS.get(model_name, model_name)


def is_openrouter_model(model_name: str) -> bool:
    """Check if a model name corresponds to an OpenRouter model."""
    if model_name in OPENROUTER_MODELS:
        return True

    # Check common OpenRouter patterns
    openrouter_patterns = [
        "qwen/", "anthropic/", "meta-llama/", "google/",
        "cohere/", "mistral/", "01-ai/", "deepseek/"
    ]

    return any(model_name.startswith(pattern) for pattern in openrouter_patterns)

# Model-specific generation configurations
MODEL_CONFIGS = {
    "microsoft/phi-4": {
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 1024,
        "temperature": 0.1,
        "do_sample": False,
        "repetition_penalty": 1.05,
        "pad_token_id": None  # Will be set to eos_token_id
    },
    "Qwen/Qwen2.5-32B": {
         # Model loading parameters (required)
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,

        # Generation parameters (essential for speed)
        "max_new_tokens": 1024,
        # "temperature": 0.0,
        "do_sample": True,
        "repetition_penalty": 1.0,
        "pad_token_id": None
    },
    "Qwen/Qwen2.5-7B": {
        # Same config as above
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 1024,
        "temperature": 0.0,
        "do_sample": False,
        "repetition_penalty": 1.1,
        "pad_token_id": None,
        "eos_token_id": None,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "output_scores": False,
        "use_cache": True,
        "no_repeat_ngram_size": 3,
    },
    "Qwen/Qwen2.5-14B": {
        # Same config as above
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 2048,
        "temperature": 0.0,
        "do_sample": False,
        "repetition_penalty": 1.1,
        "pad_token_id": None,
        "eos_token_id": None,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "output_scores": False,
        "use_cache": True,
        "no_repeat_ngram_size": 3,
    },
    "Qwen/Qwen3-14B": {
        # Same config as above
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 1024,
        "temperature": 0.0,
        "do_sample": False,
        "repetition_penalty": 1.1,
        "pad_token_id": None,
        "eos_token_id": None,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "output_scores": False,
        "use_cache": True,
        "no_repeat_ngram_size": 3,
    },
    "Qwen/Qwen3-32B": {
        # Same config as above
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 1024,
        "temperature": 0.0,
        "do_sample": False,
        "repetition_penalty": 1.1,
        "pad_token_id": None,
        "eos_token_id": None,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "output_scores": False,
        "use_cache": True,
        "no_repeat_ngram_size": 3,
    },
}

# Copy configs for local name aliases
MODEL_CONFIGS["phi-4"] = MODEL_CONFIGS["microsoft/phi-4"]
MODEL_CONFIGS["qwen2.5-32b"] = MODEL_CONFIGS["Qwen/Qwen2.5-32B"]
MODEL_CONFIGS["qwen2.5-7b"] = MODEL_CONFIGS["Qwen/Qwen2.5-7B"]
MODEL_CONFIGS["qwen2.5-14b"] = MODEL_CONFIGS["Qwen/Qwen2.5-14B"]

def get_clean_model_name(model_name: str) -> str:
    """
    Extract a clean model name for directory naming.

    Examples:
    - "microsoft/phi-4" -> "phi-4"
    - "qwen/qwen-2.5-72b-instruct" -> "qwen-2.5-72b-instruct"
    - "gpt-4o" -> "gpt-4o"
    - "Qwen/Qwen2.5-32B" -> "qwen2.5-32b"
    """
    # Handle local paths
    if "/" in model_name and not model_name.startswith(("http://", "https://")):
        # For patterns like "microsoft/phi-4" or "qwen/qwen-2.5-72b"
        clean_name = model_name.split("/")[-1]
    else:
        clean_name = model_name

    # Normalize to lowercase and replace dots/spaces with hyphens
    clean_name = clean_name.lower()
    clean_name = clean_name.replace(".", "-").replace(" ", "-").replace("_", "-")

    # Remove common prefixes/suffixes
    clean_name = clean_name.replace("-instruct", "").replace("-chat", "")

    return clean_name

def get_local_model_path(model_name: str) -> str:
    """
    Get local path for a model, falling back to original name if not found locally.

    Args:
        model_name: HuggingFace model name or local identifier

    Returns:
        Local path if model exists locally, otherwise original model name
    """
    # Check if it's already a local path
    if os.path.exists(model_name) and os.path.isdir(model_name):
        print(f"Using provided local path: {model_name}")
        return model_name

    # Check our model mappings
    local_path = MODEL_PATHS.get(model_name)
    if local_path and os.path.exists(local_path) and os.path.isdir(local_path):
        # Verify it's a valid model directory
        if os.path.exists(os.path.join(local_path, "config.json")):
            print(f"Using local model from: {local_path}")
            return local_path
        else:
            print(f"Local model directory found but missing config.json: {local_path}")

    # Check for case-insensitive matches
    model_name_lower = model_name.lower()
    for key, path in MODEL_PATHS.items():
        if key.lower() == model_name_lower and os.path.exists(path) and os.path.isdir(path):
            if os.path.exists(os.path.join(path, "config.json")):
                print(f"Using local model from case-insensitive match: {path}")
                return path

    # Fall back to original name (will download from HuggingFace)
    print(f"Local model not found for '{model_name}', using remote download")
    return model_name


def get_model_config(model_name: str) -> dict:
    """
    Get model-specific configuration.

    Args:
        model_name: Model name or path

    Returns:
        Dictionary with model configuration parameters
    """
    # First try to get config by exact name
    config = MODEL_CONFIGS.get(model_name)
    if config:
        return config.copy()

    # Try case-insensitive match
    model_name_lower = model_name.lower()
    for key, config in MODEL_CONFIGS.items():
        if key.lower() == model_name_lower:
            return config.copy()

    # Check if it's a local path and try to infer from directory name
    if os.path.exists(model_name):
        dir_name = os.path.basename(model_name.rstrip('/'))
        config = MODEL_CONFIGS.get(dir_name)
        if config:
            return config.copy()

    # Default configuration for unknown models
    print(f"Using default configuration for unknown model: {model_name}")
    return {
        "torch_dtype": "auto",
        "device_map": "auto",
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,
        "max_new_tokens": 2048,
        "temperature": 0.1,
        "do_sample": False,
        "repetition_penalty": 1.05,
        "pad_token_id": None
    }


def is_qwen_model(model_name: str) -> bool:
    """
    Check if a model is a Qwen model.

    Args:
        model_name: Model name or path

    Returns:
        True if it's a Qwen model, False otherwise
    """
    model_name_lower = model_name.lower()
    qwen_indicators = ["qwen", "qwen2", "qwen2.5"]

    # Check direct indicators
    for indicator in qwen_indicators:
        if indicator in model_name_lower:
            return True

    # Check if it's a local path pointing to a Qwen model
    if os.path.exists(model_name):
        dir_name = os.path.basename(model_name.rstrip('/')).lower()
        for indicator in qwen_indicators:
            if indicator in dir_name:
                return True

    return False


def is_phi_model(model_name: str) -> bool:
    """
    Check if a model is a Phi model.

    Args:
        model_name: Model name or path

    Returns:
        True if it's a Phi model, False otherwise
    """
    model_name_lower = model_name.lower()
    phi_indicators = ["phi", "microsoft/phi"]

    # Check direct indicators
    for indicator in phi_indicators:
        if indicator in model_name_lower:
            return True

    # Check if it's a local path pointing to a Phi model
    if os.path.exists(model_name):
        dir_name = os.path.basename(model_name.rstrip('/')).lower()
        if "phi" in dir_name:
            return True

    return False


# =============================================================================
# LEGACY COMPATIBILITY (keeping your existing variables)
# =============================================================================

# Keep your existing Qwen variables for backward compatibility
QWEN_MODEL_PATH = MODEL_PATHS.get("Qwen/Qwen2.5-32B", "/cluster/scratch/jeanpool.pereyrap/models/qwen2.5-32b")
QWEN_MODEL_NAME = "Qwen/Qwen2.5-32B"
QWEN_CONFIG = MODEL_CONFIGS["Qwen/Qwen2.5-32B"]

# =============================================================================
# APPLICATION PATHS AND SETTINGS
# =============================================================================

MAX_QUESTIONS = 1
DATASET_PATH: str = os.path.join(base_dir, "resources/questions/questions.xlsx")
DOCUMENT_DIR: str = os.path.join(base_dir, "resources/documents/policies/")
RESULT_PATH: str = os.path.join(base_dir, f"resources/results/run_output_{MAX_QUESTIONS}.tsv")
RESPONSE_FORMAT_PATH: str = os.path.join(base_dir, "resources/response_formats/travel_insurance_agent.json")
LOG_DIR: str = os.path.join(base_dir, "resources/results/logs")
JSON_PATH: str = os.path.join(base_dir, "resources/results/json_output")

RAW_GT_PATH: str = os.path.join(base_dir, "resources/raw_ground_truth/")
GT_PATH: str = os.path.join(base_dir, "resources/ground_truth/")

VECTOR_STORE_EXPIRATION_DAYS: int = 30
VECTOR_NAME_PREFIX: str = "AITIS_"
EVALUATION_RESULTS_PATH: str = os.path.join(base_dir, "resources/results/")
EVALUATION_RESULTS_FILES_PATH: str = os.path.join(base_dir, "resources/results/evaluation_results/")
DASHBOARD_PATH: str = os.path.join(base_dir, "resources/results/dashboard.html")

# New constants for embedding caching
EMBEDDINGS_DIR: str = os.path.join(base_dir, "resources/embeddings")
CACHE_EMBEDDINGS: bool = True  # Can be set to False to force re-embedding


# =============================================================================
# UTILITY FUNCTIONS FOR MODEL MANAGEMENT
# =============================================================================

def list_available_local_models() -> dict:
    """
    List all locally available models.

    Returns:
        Dictionary mapping model names to their local paths
    """
    available_models = {}

    for model_name, path in MODEL_PATHS.items():
        if os.path.exists(path) and os.path.isdir(path):
            # Check if it looks like a valid model directory
            if os.path.exists(os.path.join(path, "config.json")):
                available_models[model_name] = path

    return available_models


def get_model_info(model_name: str) -> dict:
    """
    Get comprehensive information about a model.

    Args:
        model_name: Model name or path

    Returns:
        Dictionary with model information
    """
    local_path = get_local_model_path(model_name)
    config = get_model_config(model_name)

    info = {
        "model_name": model_name,
        "local_path": local_path,
        "is_local": local_path != model_name and os.path.exists(local_path),
        "is_qwen": is_qwen_model(model_name),
        "is_phi": is_phi_model(model_name),
        "config": config
    }

    # Add size information if local
    if info["is_local"]:
        try:
            # Removed redundant 'import os' - os is already imported at module level
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(local_path):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        total_size += os.path.getsize(filepath)
            info["size_gb"] = round(total_size / (1024 ** 3), 2)
        except Exception as e:
            print(f"Warning: Could not calculate model size for {model_name}: {e}")
            info["size_gb"] = "unknown"

    return info




# End of config.py
# ================================================================================

# File 3/41: get_pdf_code.py
# --------------------------------------------------------------------------------

import fitz  # PyMuPDF
import re
import os
from typing import Dict, List, Tuple
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def extract_sections_from_pdf(pdf_path: str) -> Dict[str, str]:
    """
    Extract sections from a PDF file based on numbered headers.

    Args:
        pdf_path: Path to the PDF file

    Returns:
        Dictionary mapping section headers to their content
    """
    # Validate file exists
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF file not found: {pdf_path}")

    logging.info(f"Processing PDF: {pdf_path}")

    try:
        # Open the PDF
        doc = fitz.open(pdf_path)

        # Extract text with page numbers for debugging
        text_by_pages = []
        for i, page in enumerate(doc):
            page_text = page.get_text()
            text_by_pages.append(page_text)
            logging.info(f"Page {i + 1} extracted: {len(page_text)} characters")

        full_text = "\n".join(text_by_pages)
        logging.info(f"Total text extracted: {len(full_text)} characters")

        # More flexible regex pattern for section headers
        # This pattern looks for:
        # 1. Optional newlines
        # 2. One or more digits followed by a period
        # 3. One or more whitespace characters
        # 4. A word starting with uppercase letter followed by any characters (not just letters)
        # 5. The entire match must not exceed a reasonable length for a header
        pattern = r'(?:\n|\r\n?)?(\d+\.\s+[A-Z][^\n\r]{1,60})'

        # Find all matches
        matches = list(re.finditer(pattern, full_text))
        logging.info(f"Found {len(matches)} potential section headers")

        if not matches:
            # If no matches, try an alternative pattern without numbers
            alt_pattern = r'(?:\n|\r\n?)([A-Z][A-Z\s]{2,50}:?)'
            matches = list(re.finditer(alt_pattern, full_text))
            logging.info(f"Tried alternative pattern, found {len(matches)} potential headers")

        # Print found headers for debugging
        for match in matches:
            logging.info(f"Found header: '{match.group(1).strip()}'")

        # Group text into sections
        sections = {}
        for i, match in enumerate(matches):
            header = match.group(1).strip()
            start = match.end()

            # Determine end of this section (start of next section or end of document)
            end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)

            # Extract content
            content = full_text[start:end].strip()

            # Skip empty sections
            if not content:
                logging.warning(f"Empty content for section '{header}', skipping")
                continue

            sections[header] = content
            logging.info(f"Extracted section '{header}': {len(content)} characters")

        return sections

    except Exception as e:
        logging.error(f"Error processing PDF: {str(e)}")
        raise
    finally:
        # Close the document
        if 'doc' in locals():
            doc.close()


def print_sections(sections: Dict[str, str], preview_length: int = 300):
    """
    Print sections with preview of content

    Args:
        sections: Dictionary of sections
        preview_length: Number of characters to preview for each section
    """
    if not sections:
        print("No sections found!")
        return

    print(f"\nFound {len(sections)} sections:\n")

    for section_title, section_content in sections.items():
        print(f"\n{'=' * 80}")
        print(f"=== {section_title} ===")
        print(f"{'=' * 80}\n")

        preview = section_content[:preview_length]
        print(f"{preview}...")
        print(f"\n[Total: {len(section_content)} characters]")


def main():
    # Update this path to your PDF location
    from config import DOCUMENT_DIR
    pdf_path = os.path.join(DOCUMENT_DIR, "18_Nobis - Baggage loss EN.pdf")

    try:
        sections = extract_sections_from_pdf(pdf_path)
        print_sections(sections)

        # Optionally save to individual files
        # output_dir = "extracted_sections"
        # os.makedirs(output_dir, exist_ok=True)
        # for title, content in sections.items():
        #     safe_title = "".join(c if c.isalnum() or c.isspace() else "_" for c in title)
        #     with open(os.path.join(output_dir, f"{safe_title}.txt"), "w", encoding="utf-8") as f:
        #         f.write(content)

    except Exception as e:
        logging.error(f"Failed to process PDF: {str(e)}")


if __name__ == "__main__":
    main()


# End of get_pdf_code.py
# ================================================================================

# File 4/41: logging_utils.py
# --------------------------------------------------------------------------------

# src/logging_utils.py

import logging
from pathlib import Path
import datetime
from typing import Optional


def setup_logging(config, log_file: Optional[str] = None):
    """
    Set up logging configuration.

    Args:
        config: Configuration object.
        log_file: Optional specific log file name.
    """
    log_dir = Path(config["logging"]["log_dir"])
    log_level_str = config["logging"]["log_level"]
    log_level = getattr(logging, log_level_str.upper())

    # Create log directory if it doesn't exist
    if not log_dir.exists():
        log_dir.mkdir(parents=True)

    # Generate log file name if not provided
    if not log_file:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = f"swereflect_{timestamp}.log"

    # Ensure the parent directory of log_file exists
    log_path = log_dir / log_file
    log_path.parent.mkdir(parents=True, exist_ok=True)

    # Configure logging
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s [%(levelname)s] %(name)s (%(filename)s:%(lineno)d): %(message)s",
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler()
        ]
    )

    # Log initial message
    logging.info(f"Logging initialized at {log_path}")


# End of logging_utils.py
# ================================================================================

# File 5/41: main.py
# --------------------------------------------------------------------------------

# src/main.py

from config import LOG_DIR, OPENROUTER_API_KEY
from logging_utils import setup_logging
from rag_runner import run_rag, run_batch_rag
import argparse
import logging

logger = logging.getLogger(__name__)

if __name__ == "__main__":
    # Get available prompt names for the help message
    available_prompts = ", ".join([
        "standard", "detailed", "precise", "precise_v2", "precise_v3", "precise_v4",
        "precise_v2_1", "precise_v2_2", "precise_v2_qwen",
    ])

    # Get available relevance filter prompts
    available_relevance_prompts = ", ".join([
        "relevance_filter_v1", "relevance_filter_v2",
    ])

    # Parse command-line arguments
    parser = argparse.ArgumentParser(description="Run RAG system for insurance policy analysis")
    parser.add_argument("--model", choices=["openai", "hf", "qwen", "openrouter"], default="hf",
                        help="Model provider (openai, hf, qwen, or openrouter)")
    parser.add_argument("--model-name", default="microsoft/phi-4",
                        help="Name of the model to use. For OpenRouter, use format like 'qwen/qwen-2.5-72b-instruct'")
    parser.add_argument("--batch", action="store_true", help="Process all policies in a single batch")
    parser.add_argument("--num-questions", type=int, default=None,
                        help="Number of questions to process (default: all available questions)")
    parser.add_argument("--output-dir", default=None,
                        help="Directory to save JSON output files (default: resources/results/json_output)")
    parser.add_argument("--prompt", default="standard",
                        help=f"Prompt template to use. Available: {available_prompts}")
    parser.add_argument("--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        default="INFO", help="Set the logging level")
    parser.add_argument("--persona", action="store_true", help="Use persona extraction for queries")
    parser.add_argument("--questions", type=str,
                        help="Comma-separated list of question IDs to process (e.g., '1,2,3,4')")
    parser.add_argument("--policy-id", type=str,
                        help="Process only a specific policy ID (e.g., '20')")
    parser.add_argument("--k", type=int, default=3,
                        help="Number of context chunks to retrieve from vector store (default: 3)")
    parser.add_argument("--filter-irrelevant", action="store_true",
                        help="Filter out obviously irrelevant queries before processing")
    parser.add_argument("--prompt-relevant", default="relevance_filter_v1",
                        help=f"Relevance filter prompt to use. Available: {available_relevance_prompts}")
    parser.add_argument("--rag-strategy", choices=["simple", "enhanced", "hybrid"], default="simple",
                        help="RAG strategy to use (simple, enhanced, hybrid)")

    args = parser.parse_args()

    # Set up logging with the specified log level
    setup_logging({
        "logging": {
            "log_dir": LOG_DIR,
            "log_level": args.log_level
        }
    })

    logger.info(f"Starting RAG pipeline with model: {args.model}/{args.model_name}")
    logger.info(f"Using prompt template: {args.prompt}")
    logger.info(f"Log level set to: {args.log_level}")

    # Add OpenRouter-specific logging
    if args.model == "openrouter":
        logger.info("Using OpenRouter API for model access")
        if not OPENROUTER_API_KEY:
            logger.error("OPENROUTER_API_KEY environment variable not set!")
            logger.error("Please set your OpenRouter API key in the .env file")
            exit(1)

    if args.num_questions:
        logger.info(f"Processing {args.num_questions} questions")
    else:
        logger.info("Processing all available questions")

    # Parse the question_ids from the arguments
    question_ids = None
    if args.questions:
        question_ids = [q.strip() for q in args.questions.split(',')]
        logger.info(f"Will process specific questions with IDs: {', '.join(question_ids)}")

    if args.policy_id:
        logger.info(f"Will only process policy with ID: {args.policy_id}")

    # Run the appropriate RAG function with all parameters
    if args.batch:
        run_batch_rag(
            model_provider=args.model,
            model_name=args.model_name,
            max_questions=args.num_questions,
            output_dir=args.output_dir,
            prompt_name=args.prompt,
            use_persona=args.persona,
            question_ids=question_ids,
            policy_id=args.policy_id,
            k=args.k,
            filter_irrelevant=args.filter_irrelevant,
            relevance_prompt_name=args.prompt_relevant,
        )
    else:
        run_rag(
            model_provider=args.model,
            model_name=args.model_name,
            max_questions=args.num_questions,
            output_dir=args.output_dir,
            prompt_name=args.prompt,
            use_persona=args.persona,
            question_ids=question_ids,
            policy_id=args.policy_id,
            k=args.k,
            filter_irrelevant=args.filter_irrelevant,
            relevance_prompt_name=args.prompt_relevant,
        )

    logger.info("RAG pipeline completed successfully")



# End of main.py
# ================================================================================

# File 6/41: models/__init__.py
# --------------------------------------------------------------------------------



# End of models/__init__.py
# ================================================================================

# File 7/41: models/base.py
# --------------------------------------------------------------------------------

# src/models/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, List

class BaseModelClient(ABC):
    @abstractmethod
    def query(self, question: str, context_files: List[str]) -> Dict[str, Any]:
        pass



# End of models/base.py
# ================================================================================

# File 8/41: models/chunking/__init__.py
# --------------------------------------------------------------------------------


# models/chunking/__init__.py
"""
Chunking strategies for insurance policy analysis.
Provides a clean interface for different chunking approaches.
"""

from .base import ChunkingStrategy
from .factory import ChunkingFactory
from .simple_chunker import SimpleChunker
# from .structural_chunker import StructuralChunker
# from .semantic_chunker import SemanticChunker
# from .hierarchical_chunker import HierarchicalChunker
# from .coverage_chunker import CoverageChunker
# from .hybrid_chunker import HybridChunker

__all__ = [
    'ChunkingStrategy',
    'ChunkingFactory',
    'SimpleChunker',
    # 'StructuralChunker',
    # 'SemanticChunker',
    # 'HierarchicalChunker',
    # 'CoverageChunker',
    # 'HybridChunker'
]


# End of models/chunking/__init__.py
# ================================================================================

# File 9/41: models/chunking/base.py
# --------------------------------------------------------------------------------

# models/chunking/base.py

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class ChunkMetadata:
    """Metadata for a single chunk."""
    chunk_id: str
    policy_id: Optional[str] = None
    chunk_type: str = "unknown"
    word_count: int = 0
    has_amounts: bool = False
    has_conditions: bool = False
    has_exclusions: bool = False
    section: Optional[str] = None
    coverage_type: str = "general"
    confidence_score: float = 1.0
    extra_data: Optional[Dict[str, Any]] = None


@dataclass
class ChunkResult:
    """Result of chunking operation."""
    text: str
    metadata: ChunkMetadata


class ChunkingStrategy(ABC):
    """
    Abstract base class for all chunking strategies.

    This interface ensures all chunking approaches have consistent behavior
    and can be easily swapped in the vector store.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the chunking strategy.

        Args:
            config: Optional configuration dictionary for the strategy
        """
        self.config = config or {}
        self.name = self.__class__.__name__.lower().replace('chunker', '')
        logger.info(f"Initialized {self.name} chunking strategy")

    @abstractmethod
    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Chunk the given text using this strategy.

        Args:
            text: The text to chunk
            policy_id: Optional policy identifier for metadata
            max_length: Maximum chunk length (strategy may ignore this)

        Returns:
            List of ChunkResult objects containing text and metadata
        """
        pass

    @abstractmethod
    def get_strategy_info(self) -> Dict[str, Any]:
        """
        Get information about this chunking strategy.

        Returns:
            Dictionary with strategy metadata (name, description, config, etc.)
        """
        pass

    def validate_config(self) -> bool:
        """
        Validate the strategy configuration.

        Returns:
            True if configuration is valid, False otherwise
        """
        return True

    def get_chunk_text_list(self, chunk_results: List[ChunkResult]) -> List[str]:
        """
        Extract just the text from chunk results for backward compatibility.

        Args:
            chunk_results: List of ChunkResult objects

        Returns:
            List of chunk texts
        """
        return [result.text for result in chunk_results]

    def get_metadata_list(self, chunk_results: List[ChunkResult]) -> List[ChunkMetadata]:
        """
        Extract just the metadata from chunk results.

        Args:
            chunk_results: List of ChunkResult objects

        Returns:
            List of ChunkMetadata objects
        """
        return [result.metadata for result in chunk_results]

    def _create_chunk_id(self, policy_id: Optional[str], chunk_index: int) -> str:
        """Create a unique chunk ID."""
        prefix = f"{policy_id}_" if policy_id else ""
        return f"{prefix}{self.name}_{chunk_index}"

    def _analyze_text_properties(self, text: str) -> Dict[str, bool]:
        """Analyze text for common properties."""
        import re

        return {
            'has_amounts': bool(re.search(r'€\s*\d+|CHF\s*\d+|\d+\s*EUR|maximum.*\d+', text)),
            'has_conditions': any(word in text.lower() for word in
                                  ['if', 'when', 'unless', 'provided that', 'subject to']),
            'has_exclusions': any(word in text.lower() for word in
                                  ['not covered', 'excluded', 'exception', 'does not apply'])
        }


class ChunkingError(Exception):
    """Custom exception for chunking-related errors."""
    pass


# End of models/chunking/base.py
# ================================================================================

# File 10/41: models/chunking/factory.py
# --------------------------------------------------------------------------------

# models/chunking/factory.py

from typing import Dict, Any, Optional, Type, List
import logging

from . import SimpleChunker
from .base import ChunkingStrategy, ChunkingError

logger = logging.getLogger(__name__)


class ChunkingFactory:
    """
    Factory for creating chunking strategies.

    Supports easy registration of new strategies and configuration-based creation.
    """

    _strategies: Dict[str, Type[ChunkingStrategy]] = {}
    _default_configs: Dict[str, Dict[str, Any]] = {}

    @classmethod
    def register_strategy(cls, name: str, strategy_class: Type[ChunkingStrategy],
                          default_config: Optional[Dict[str, Any]] = None):
        """
        Register a new chunking strategy.

        Args:
            name: Strategy name (used for selection)
            strategy_class: The strategy class
            default_config: Default configuration for this strategy
        """
        cls._strategies[name] = strategy_class
        cls._default_configs[name] = default_config or {}
        logger.info(f"Registered chunking strategy: {name}")

    @classmethod
    def create_strategy(cls, strategy_name: str,
                        config: Optional[Dict[str, Any]] = None) -> ChunkingStrategy:
        """
        Create a chunking strategy instance.

        Args:
            strategy_name: Name of the strategy to create
            config: Optional configuration to override defaults

        Returns:
            Configured ChunkingStrategy instance

        Raises:
            ChunkingError: If strategy is not found or creation fails
        """
        if strategy_name not in cls._strategies:
            available = list(cls._strategies.keys())
            raise ChunkingError(
                f"Unknown chunking strategy: {strategy_name}. "
                f"Available strategies: {available}"
            )

        strategy_class = cls._strategies[strategy_name]
        default_config = cls._default_configs[strategy_name].copy()

        # Merge provided config with defaults
        if config:
            default_config.update(config)

        try:
            strategy = strategy_class(default_config)

            # Validate configuration
            if not strategy.validate_config():
                raise ChunkingError(f"Invalid configuration for strategy: {strategy_name}")

            logger.info(f"Created {strategy_name} chunking strategy")
            return strategy

        except Exception as e:
            raise ChunkingError(f"Failed to create {strategy_name} strategy: {str(e)}")

    @classmethod
    def get_available_strategies(cls) -> List[str]:
        """Get list of available strategy names."""
        return list(cls._strategies.keys())

    @classmethod
    def get_strategy_info(cls, strategy_name: str) -> Dict[str, Any]:
        """
        Get information about a specific strategy.

        Args:
            strategy_name: Name of the strategy

        Returns:
            Dictionary with strategy information
        """
        if strategy_name not in cls._strategies:
            raise ChunkingError(f"Unknown strategy: {strategy_name}")

        strategy_class = cls._strategies[strategy_name]
        default_config = cls._default_configs[strategy_name]

        # Create temporary instance to get info
        try:
            temp_instance = strategy_class(default_config)
            strategy_info = temp_instance.get_strategy_info()
            strategy_info['default_config'] = default_config
            return strategy_info
        except Exception as e:
            return {
                'name': strategy_name,
                'class': strategy_class.__name__,
                'error': f"Could not get info: {str(e)}",
                'default_config': default_config
            }

    @classmethod
    def get_all_strategies_info(cls) -> Dict[str, Dict[str, Any]]:
        """Get information about all registered strategies."""
        return {
            name: cls.get_strategy_info(name)
            for name in cls._strategies.keys()
        }


# Auto-registration system
def auto_register_strategies():
    """
    Automatically register all available chunking strategies.
    This function is called when the module is imported.
    """
    try:
        # Import and register simple chunker (always available)
        ChunkingFactory.register_strategy(
            'simple',
            SimpleChunker,
            {'max_length': 512, 'overlap': 0, 'preserve_paragraphs': True}
        )

        # Try to register other strategies (may fail if dependencies missing)
        strategy_imports = [
            ('structural', '.structural_chunker', 'StructuralChunker', {}),
            ('semantic', '.semantic_chunker', 'SemanticChunker', {'target_chunks': 15}),
            ('hierarchical', '.hierarchical_chunker', 'HierarchicalChunker', {}),
            ('coverage', '.coverage_chunker', 'CoverageChunker', {}),
            ('hybrid', '.hybrid_chunker', 'HybridChunker', {})
        ]

        for strategy_name, module_name, class_name, default_config in strategy_imports:
            try:
                module = __import__(module_name, fromlist=[class_name], level=1)
                strategy_class = getattr(module, class_name)
                ChunkingFactory.register_strategy(strategy_name, strategy_class, default_config)
            except ImportError as e:
                logger.warning(f"Could not register {strategy_name} strategy: {e}")
            except AttributeError as e:
                logger.warning(f"Could not find {class_name} in {module_name}: {e}")

    except Exception as e:
        logger.error(f"Error during auto-registration: {e}")


# Configuration presets for common use cases
CHUNKING_PRESETS = {
    'fast': {
        'strategy': 'simple',
        'config': {'max_length': 256, 'overlap': 25}
    },
    'balanced': {
        'strategy': 'structural',
        'config': {'max_length': 512, 'preserve_sections': True}
    },
    'comprehensive': {
        'strategy': 'coverage',
        'config': {'include_cross_references': True, 'merge_related': True}
    },
    'research': {
        'strategy': 'hybrid',
        'config': {'enable_all_features': True, 'detailed_metadata': True}
    }
}


def create_preset_strategy(preset_name: str) -> ChunkingStrategy:
    """
    Create a strategy using predefined presets.

    Args:
        preset_name: Name of the preset ('fast', 'balanced', 'comprehensive', 'research')

    Returns:
        Configured ChunkingStrategy instance
    """
    if preset_name not in CHUNKING_PRESETS:
        available = list(CHUNKING_PRESETS.keys())
        raise ChunkingError(f"Unknown preset: {preset_name}. Available: {available}")

    preset = CHUNKING_PRESETS[preset_name]
    return ChunkingFactory.create_strategy(preset['strategy'], preset['config'])


# End of models/chunking/factory.py
# ================================================================================

# File 11/41: models/chunking/simple_chunker.py
# --------------------------------------------------------------------------------

# models/chunking/simple_chunker.py

from typing import List, Dict, Any, Optional
import logging

from .base import ChunkingStrategy, ChunkResult, ChunkMetadata

logger = logging.getLogger(__name__)


class SimpleChunker(ChunkingStrategy):
    """
    Simple paragraph-based chunking strategy.

    This implements your current LocalVectorStore chunking approach adapted
    to follow the ChunkingStrategy interface.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize simple chunker.

        Config options:
        - max_length: Maximum chunk length in characters (default: 512)
        - overlap: Number of characters to overlap between chunks (default: 0)
        - preserve_paragraphs: Whether to prefer paragraph boundaries (default: True)
        """
        super().__init__(config)

        # Configuration with defaults matching your LocalVectorStore
        self.max_length = self.config.get('max_length', 512)
        self.overlap = self.config.get('overlap', 0)
        self.preserve_paragraphs = self.config.get('preserve_paragraphs', True)

        logger.info(f"SimpleChunker configured: max_length={self.max_length}, "
                    f"overlap={self.overlap}, preserve_paragraphs={self.preserve_paragraphs}")

    def chunk_text(self, text: str, policy_id: Optional[str] = None,
                   max_length: int = 512) -> List[ChunkResult]:
        """
        Chunk text using your current LocalVectorStore approach.

        This replicates your existing LocalVectorStore.chunk_text() method
        but returns ChunkResult objects instead of just strings.
        """
        # Use configured max_length unless overridden
        chunk_max_length = self.config.get('max_length', max_length)

        # Apply your exact chunking logic
        chunks = self._chunk_text_simple(text, chunk_max_length)

        # Convert to ChunkResult objects
        chunk_results = []
        for i, chunk_text in enumerate(chunks):
            # Create metadata for each chunk
            metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(policy_id, i),
                policy_id=policy_id,
                chunk_type="simple_paragraph",
                word_count=len(chunk_text.split()),
                has_amounts=self._detect_amounts(chunk_text),
                has_conditions=self._detect_conditions(chunk_text),
                has_exclusions=self._detect_exclusions(chunk_text),
                section=None,  # Simple chunker doesn't identify sections
                coverage_type="general",  # Simple chunker doesn't classify coverage
                confidence_score=1.0,
                extra_data={
                    'chunk_method': 'paragraph_based',
                    'chunk_index': i,
                    'original_length': len(chunk_text)
                }
            )

            chunk_results.append(ChunkResult(
                text=chunk_text,
                metadata=metadata
            ))

        logger.info(f"SimpleChunker created {len(chunk_results)} chunks for policy {policy_id}")
        return chunk_results

    def _chunk_text_simple(self, text: str, max_length: int) -> List[str]:
        """
        Your exact chunking logic from LocalVectorStore.chunk_text()
        """
        if self.preserve_paragraphs:
            return self._paragraph_based_chunking(text, max_length)
        else:
            return self._word_based_chunking(text, max_length)

    def _paragraph_based_chunking(self, text: str, max_length: int) -> List[str]:
        """
        Exact implementation from your LocalVectorStore.
        """
        # Simple paragraph-based chunking
        paragraphs = text.split('\n\n')
        chunks = []

        current_chunk = ""
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) < max_length:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"

        if current_chunk:
            chunks.append(current_chunk.strip())

        # Fallback to word-based chunking if no paragraphs
        if not chunks:
            chunks = self._word_based_chunking(text, max_length)

        return chunks

    def _word_based_chunking(self, text: str, max_length: int) -> List[str]:
        """
        Fallback word-based chunking from your LocalVectorStore.
        """
        words = text.split()
        chunks = []

        current_chunk_words = []
        current_length = 0

        for word in words:
            word_length = len(word) + 1  # +1 for space

            if current_length + word_length <= max_length:
                current_chunk_words.append(word)
                current_length += word_length
            else:
                if current_chunk_words:
                    chunks.append(" ".join(current_chunk_words))
                current_chunk_words = [word]
                current_length = word_length

        if current_chunk_words:
            chunks.append(" ".join(current_chunk_words))

        return chunks if chunks else [text[:max_length]]

    def _detect_amounts(self, text: str) -> bool:
        """Detect if text contains monetary amounts or numbers."""
        import re
        # Look for currency symbols, numbers with currency codes, or percentage
        amount_patterns = [
            r'€\s*\d+',  # Euro amounts
            r'CHF\s*\d+',  # Swiss Franc
            r'\d+\s*EUR',  # EUR suffix
            r'\d+\s*CHF',  # CHF suffix
            r'maximum.*\d+',  # Maximum amounts
            r'\d+%',  # Percentages
            r'\d+\.\d+',  # Decimal numbers
        ]
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in amount_patterns)

    def _detect_conditions(self, text: str) -> bool:
        """Detect if text contains conditional language."""
        condition_words = [
            'if', 'when', 'unless', 'provided that', 'subject to',
            'conditional', 'depends on', 'only if', 'in case of',
            'se', 'quando', 'solo se', 'purché'  # Italian equivalents
        ]
        text_lower = text.lower()
        return any(word in text_lower for word in condition_words)

    def _detect_exclusions(self, text: str) -> bool:
        """Detect if text contains exclusion language."""
        exclusion_words = [
            'not covered', 'excluded', 'exception', 'does not apply',
            'limitation', 'restriction', 'not eligible', 'shall not',
            'non coperto', 'escluso', 'eccezione', 'limitazione'  # Italian
        ]
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in exclusion_words)

    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this chunking strategy."""
        return {
            "name": "simple",
            "description": "Simple paragraph-based chunking adapted from LocalVectorStore",
            "type": "rule_based",
            "complexity": "low",
            "performance": "fast",
            "config": self.config,
            "features": [
                "paragraph_splitting",
                "configurable_max_length",
                "word_based_fallback",
                "basic_content_detection"
            ],
            "best_for": [
                "quick_testing",
                "baseline_comparison",
                "simple_documents",
                "fast_processing",
                "compatibility_with_existing_system"
            ],
            "matches_original": "LocalVectorStore.chunk_text()"
        }

    def validate_config(self) -> bool:
        """Validate configuration."""
        if self.max_length <= 0:
            logger.error("max_length must be positive")
            return False

        if self.overlap < 0:
            logger.error("overlap cannot be negative")
            return False

        if self.overlap >= self.max_length:
            logger.error("overlap must be less than max_length")
            return False

        return True


# End of models/chunking/simple_chunker.py
# ================================================================================

# File 12/41: models/factory.py
# --------------------------------------------------------------------------------

# src/models/factory.py

from config import DOCUMENT_DIR, is_qwen_model, is_phi_model, is_openrouter_model
from models.hf_model import HuggingFaceModelClient
from models.openai_model import OpenAIModelClient
from models.qwen_model import QwenModelClient
from models.openrouter_model import OpenRouterModelClient
from models.shared_client import SharedModelClient
from utils import list_policy_paths


def get_model_client(provider: str, model_name: str, sys_prompt: str):
    """
    Create appropriate model client based on provider and model name.

    Args:
        provider: Model provider ("openai", "hf", "qwen", "openrouter")
        model_name: Name of the model to use
        sys_prompt: System prompt for the model

    Returns:
        Appropriate model client instance
    """
    file_paths = list_policy_paths(DOCUMENT_DIR)

    if provider == "openai":
        return OpenAIModelClient(model_name, sys_prompt, file_paths)
    elif provider == "openrouter":
        return OpenRouterModelClient(model_name, sys_prompt)
    elif provider in ["hf", "qwen"]:
        # Auto-detect model type and use appropriate client
        if is_openrouter_model(model_name):
            # If user specified hf/qwen but model looks like OpenRouter, suggest correction
            print(f"Warning: Model '{model_name}' appears to be an OpenRouter model. Consider using --model openrouter")
            return OpenRouterModelClient(model_name, sys_prompt)
        elif is_qwen_model(model_name):
            return QwenModelClient(model_name, sys_prompt)
        elif is_phi_model(model_name):
            return HuggingFaceModelClient(model_name, sys_prompt)
        else:
            return HuggingFaceModelClient(model_name, sys_prompt)
    else:
        raise ValueError(f"Unknown provider: {provider}. Supported: openai, hf, qwen, openrouter")


def get_shared_relevance_client(base_client, relevance_prompt: str):
    """Create a shared model client for relevance filtering."""
    return SharedModelClient(base_client, relevance_prompt)



# End of models/factory.py
# ================================================================================

# File 13/41: models/hf_model.py
# --------------------------------------------------------------------------------

# src/models/hf_model.py

"""
HuggingFace model client implementation for insurance policy analysis.
"""
import logging
import os
from typing import List, Dict, Any

from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from config import HUGGINGFACE_TOKEN, get_local_model_path, get_model_config
from models.base import BaseModelClient
from models.persona.extractor import PersonaExtractor
from models.json_utils.extractors import JSONExtractor

# Login with your token
login(token=HUGGINGFACE_TOKEN)

logger = logging.getLogger(__name__)


class HuggingFaceModelClient(BaseModelClient):
    """
    Client for using HuggingFace models to analyze insurance policies.
    Handles model initialization, inference, and result formatting.
    """

    def __init__(self, model_name: str, sys_prompt: str):
        """
        Initialize a HuggingFace model client.

        Args:
            model_name: Name of the model to use
            sys_prompt: System prompt for the model
        """
        logger.info(f"Loading HuggingFace model: {model_name}")
        self._log_cache_locations()
        model_name = self._check_model_in_scratch(model_name)

        self.pipe = self._initialize_pipeline(model_name)
        self.base_prompt = sys_prompt.strip()
        self.json_format = self._get_json_format_template()

        # Initialize extractors
        self.persona_extractor = PersonaExtractor(self.pipe)
        self.json_extractor = JSONExtractor()

    def _log_cache_locations(self) -> None:
        """Log HuggingFace cache locations to verify they're set."""
        logger.info(f"HF_HUB_CACHE: {os.environ.get('HF_HUB_CACHE', 'Not set')}")
        logger.info(f"HF_ASSETS_CACHE: {os.environ.get('HF_ASSETS_CACHE', 'Not set')}")

    def _check_model_in_scratch(self, model_name: str) -> str:
        """Check if the model exists locally and return appropriate path."""
        return get_local_model_path(model_name)

    def _initialize_pipeline(self, model_name: str):
        """Initialize the HuggingFace pipeline with the specified model."""
        try:
            logger.info(f"Initializing pipeline with model: {model_name}")
            model_config = get_model_config(model_name)

            tokenizer = self._load_tokenizer(model_name)
            model = self._load_model(model_name, model_config)

            # Create pipeline with loaded model and tokenizer
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer
            )

            logger.info("Pipeline successfully initialized")
            return pipe

        except Exception as e:
            logger.error(f"Error initializing pipeline: {e}")
            raise

    def _load_tokenizer(self, model_name: str):
        """Load the tokenizer for the specified model."""
        return AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )

    def _load_model(self, model_name: str, model_config: dict):
        """Load the model with configuration from config.py."""
        return AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=model_config["torch_dtype"],
            device_map=model_config["device_map"],
            trust_remote_code=model_config["trust_remote_code"],
            low_cpu_mem_usage=model_config["low_cpu_mem_usage"]
        )

    def _get_json_format_template(self) -> str:
        """Return the JSON format template for model output."""
        return """
            Format the output EXACTLY in the following JSON schema WITHOUT ANY ADDITIONAL TEXT BEFORE OR AFTER:
            {
              "answer": {
                "eligibility": "Yes | No - Unrelated event | No - condition(s) not met",
                "eligibility_policy": "Quoted text from policy",
                "amount_policy": "Amount like '1000 CHF' or null"
              }
            }
            """

    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """
        Create a formatted error response.

        Args:
            error_message: The error message to include

        Returns:
            Formatted error response dictionary
        """
        return {
            "answer": {
                "eligibility": "Error",
                "eligibility_policy": error_message,
                "amount_policy": None,
                "amount_policy_line": None
            }
        }

    def _format_context_text(self, context_files: List[str]) -> str:
        """
        Format context information from policy files.

        Args:
            context_files: List of context file contents

        Returns:
            Formatted context text
        """
        if not context_files or len(context_files) == 0:
            return ""

        context_text = "\n\nRelevant policy information:\n\n"
        context_text += ("""
                        WARNING: Some of the following context may contain "SECTION DEFINITIONS" of terms or general "
                        "information that does not directly indicate coverage. Please carefully distinguish "
                        "between definitions, exclusions and actual coverage provisions.\n\n
                        """)

        for i, ctx in enumerate(context_files, 1):
            context_text += f"Policy context {i}:\n{ctx.strip()}\n\n"

        return context_text

    def _build_prompt(self, question: str, context_text: str, persona_text: str) -> str:
        """
        Build the full prompt using model-specific format.
        - Phi-4: Simple concatenated format (original working version)
        - Qwen: Chat template format
        """
        # Get the model path to determine which format to use
        model_path = getattr(self.pipe.model, 'name_or_path', '')
        model_name_lower = model_path.lower()

        # Check if this is a Qwen model
        is_qwen = any(indicator in model_name_lower for indicator in ["qwen", "qwen2", "qwen2.5"])

        if is_qwen:
            # QWEN FORMAT: Use chat template
            return self._build_qwen_chat_prompt(question, context_text, persona_text)
        else:
            # PHI-4 FORMAT: Use original simple format that was working
            return self._build_phi_simple_prompt(question, context_text, persona_text)

    def _build_phi_simple_prompt(self, question: str, context_text: str, persona_text: str) -> str:
        """
        Build the full prompt for Phi-4 using the original simple format that was working.
        """
        prompt_final = "\nThen the JSON Solution is:\n\n"

        if persona_text:
            full_prompt = f"{self.base_prompt}\n\n{context_text}\n\nQuestion: {question}\n\n{persona_text}\n\n{prompt_final}"
        else:
            full_prompt = f"{self.base_prompt}\n\n{context_text}\n\nQuestion: {question}\n\n{prompt_final}"

        logger.debug(f"Phi-4 prompt: {full_prompt}")
        return full_prompt

    def _build_qwen_chat_prompt(self, question: str, context_text: str, persona_text: str) -> str:
        """
        Build the full prompt for Qwen using chat template format.
        """
        if persona_text:
            user_content = f"{context_text}\n\nQuestion: {question}\n\n{persona_text}"
        else:
            user_content = f"{context_text}\n\nQuestion: {question}"

        # Use Qwen chat template format - DO NOT repeat JSON schema in user message
        # system_message = f"<|im_start|>system\n{self.base_prompt}<|im_end|>\n"
        user_message = f"{user_content}\n"
        # assistant_start = "<|im_start|>assistant\n"

        full_prompt = user_message
        logger.debug(f"Qwen chat template prompt: {full_prompt}")
        return full_prompt

    def _get_appropriate_json_format(self) -> str:
        """
        Get the appropriate JSON format based on the current prompt type.

        Returns:
            The appropriate JSON format string
        """
        # Check if this is a relevance filtering prompt
        if "INSURANCE-POLICY RELEVANCE FILTER" in self.base_prompt:
            return """
                Return exactly this JSON:
                {
                  "is_relevant": true/false,
                  "reason": "Brief explanation (≤ 25 words)"
                }
                """
        else:
            # Default to insurance analysis format
            return self.json_format

    def _generate_model_output(self, prompt: str) -> Dict[str, Any]:
        """
        Generate output from the model.

        Args:
            prompt: The prompt to send to the model

        Returns:
            Parsed JSON response or default "No - condition(s) not met" response
        """
        try:
            model_config = get_model_config(self.pipe.model.name_or_path)
            # Generate text with appropriate parameters for JSON output
            outputs = self.pipe(
                prompt,
                max_new_tokens=model_config["max_new_tokens"],
                do_sample=model_config["do_sample"],
                num_return_sequences=1,
                return_full_text=False
            )

            # Get the generated text
            generated = outputs[0]["generated_text"]
            logger.debug(f"Model output: {generated}")

            # Try to extract JSON from the output
            extracted_json = self.json_extractor.extract_json(generated)

            if extracted_json:
                return extracted_json

            # If all extraction methods fail, return default "No - condition(s) not met" response
            logger.error("All JSON extraction methods failed - returning default 'No - condition(s) not met' response")
            return self._create_default_no_conditions_response()

        except Exception as e:
            error_msg = f"Error during model inference: {str(e)}"
            logger.error(error_msg)
            return self._create_default_no_conditions_response()

    def _create_default_no_conditions_response(self) -> Dict[str, Any]:
        """
        Create a default 'No - condition(s) not met' response when JSON extraction fails.

        Returns:
            Default structured response indicating conditions not met
        """
        return {
            "answer": {
                "eligibility": "",
                "eligibility_policy": "",
                "amount_policy": ""
            }
        }

    def query(self, question: str, context_files: List[str], use_persona: bool = False) -> Dict[str, Any]:
        """
        Process a query and return the model's response.

        Args:
            question: The question to answer
            context_files: List of relevant policy files
            use_persona: Whether to extract and use persona information (default: False)

        Returns:
            Dictionary containing the answer
        """
        logger.info(f"Querying model with question: {question}")

        # Extract personas from the question if use_persona is True
        persona_text = ""
        if use_persona:
            personas_info = self.persona_extractor.extract_personas(question)
            logger.info(f"Extracted personas: {personas_info}")
            # Format the persona information
            persona_text = self.persona_extractor.format_persona_text(personas_info)
            logger.info("Using persona information in prompt")
        else:
            logger.info("Skipping persona extraction (--persona flag not used)")

        # Format context information
        context_text = self._format_context_text(context_files)

        # Build the full prompt
        full_prompt = self._build_prompt(question, context_text, persona_text)

        # Generate and process the model output
        return self._generate_model_output(full_prompt)


# End of models/hf_model.py
# ================================================================================

# File 14/41: models/json_utils/__init__.py
# --------------------------------------------------------------------------------



# End of models/json_utils/__init__.py
# ================================================================================

# File 15/41: models/json_utils/extractors.py
# --------------------------------------------------------------------------------

# src/models/json_utils/extractors.py
"""
JSON extraction utilities.
Functions for extracting valid JSON from model outputs.
"""
import json
import re
import logging
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class JSONExtractor:
    """
    Extracts and validates JSON from model outputs.
    Supports multiple extraction methods and validation.
    """

    def extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from text, handling Qwen thinking tags and malformed JSON.
        """
        # Remove thinking tags that interfere with JSON extraction
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

        # Remove any text before the first { and after the last }
        # This helps with models that add explanatory text
        first_brace = text.find('{')
        last_brace = text.rfind('}')

        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            text = text[first_brace:last_brace + 1]

        # QUICK FIX: Replace escaped quotes in policy text with single quotes
        text = re.sub(r'\\"([^"]*)\\"', r"'\1'", text)

        # Try multiple extraction methods
        extracted = self._try_all_json_extraction_methods(text)
        if extracted:
            normalized = self._normalize_json_fields(extracted)
            logger.info(f"Successfully extracted and normalized JSON: {normalized}")
            return normalized
        return None

    def _normalize_json_fields(self, json_obj: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhanced normalization to handle Qwen model variations.
        """
        if "answer" in json_obj and isinstance(json_obj["answer"], dict):
            answer = json_obj["answer"]
            normalized_answer = {}

            # Enhanced eligibility field normalization
            eligibility_value = ""
            eligibility_fields = [
                "eligibility", "elgibility", "eligiblity", "eligibilty",
                "eligibilaty", "eligiblity", "eligibilit", "eligibilyt",
                "eligible"  # Add this common Qwen variant
            ]
            for field in eligibility_fields:
                if field in answer:
                    eligibility_value = answer[field]
                    break
            normalized_answer["eligibility"] = eligibility_value

            # Enhanced eligibility_policy field normalization
            policy_value = ""
            policy_fields = [
                "eligibility_policy", "elgibility_policy", "eligibility_text",
                "eligiblity_policy", "eligibilty_policy", "policy_text",
                "eligible_policy",  # Add this common Qwen variant
                "description", "text", "justification"
            ]
            for field in policy_fields:
                if field in answer:
                    value = answer[field]
                    if isinstance(value, list):
                        policy_value = value[0] if value else ""
                    else:
                        policy_value = str(value) if value else ""
                    break
            normalized_answer["eligibility_policy"] = policy_value

            # Amount field normalization (unchanged)
            amount_value = None
            amount_fields = [
                "amount_policy", "amount_policey", "amount_polcy",
                "amount_policy_line", "coverage", "coverage_amount", "amount", "payment"
            ]
            for field in amount_fields:
                if field in answer:
                    amount_value = answer[field]
                    break
            normalized_answer["amount_policy"] = amount_value

            json_obj["answer"] = normalized_answer

        return json_obj

    def _try_all_json_extraction_methods(self, generated: str) -> Optional[Dict[str, Any]]:
        """
        Try multiple JSON extraction methods in sequence.
        Args:
            generated: The generated text to extract JSON from
        Returns:
            Extracted JSON or None if all methods failed
        """
        # Try solution block extraction (specific to Phi pattern)
        solution_json = self._extract_json_from_solution_block(generated)
        if solution_json:
            logger.info("Successfully parsed JSON from solution block format")
            return solution_json

        # Try code block extraction
        code_block_json = self._extract_json_from_code_block(generated)
        if code_block_json:
            logger.info("Successfully parsed JSON from code block")
            return code_block_json

        # Try generic JSON extraction
        generic_json = self._extract_json_from_text(generated)
        if generic_json:
            logger.info("Successfully parsed JSON using generic extraction")
            return generic_json

        return None

    def _extract_json_from_solution_block(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from Phi's "Solution 1" block format.
        Args:
            text: Generated text containing the solution block
        Returns:
            Parsed JSON dictionary or None if not found
        """
        solution_pattern = r'## Solution 1:.*?```json\s*(.*?)\s*```'
        match = re.search(solution_pattern, text, re.DOTALL)
        if match:
            json_str = match.group(1).strip()
            try:
                parsed = json.loads(json_str)
                logger.debug(f"Successfully extracted JSON from solution block: {json_str}")
                return parsed
            except json.JSONDecodeError as e:
                logger.warning(f"Found solution block but JSON is invalid: {e}")
        return None

    def _extract_json_from_code_block(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from markdown code blocks.
        Args:
            text: Generated text that might contain code blocks
        Returns:
            Parsed JSON dictionary or None if not found
        """
        code_block_pattern = r'```(?:json)?\s*(.*?)\s*```'
        matches = re.findall(code_block_pattern, text, re.DOTALL)
        for block in matches:
            try:
                parsed = json.loads(block.strip())
                if self._is_valid_answer_json_flexible(parsed):
                    return parsed
            except json.JSONDecodeError:
                continue
        return None

    def _extract_json_from_text(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract valid JSON from text that might contain additional content.
        Args:
            text: The generated text that should contain JSON
        Returns:
            The parsed JSON dictionary or None if not found
        """
        json_candidates = self._extract_json_candidate_strings(text)
        if not json_candidates:
            logger.warning("No JSON-like patterns found in the text")
            return None

        for json_candidate in json_candidates:
            json_obj = self._parse_json_candidate(json_candidate)
            if json_obj:
                return json_obj

        logger.warning("No valid JSON with expected structure found")
        return None

    def _extract_json_candidate_strings(self, text: str) -> List[str]:
        """
        Enhanced JSON candidate extraction.
        """
        if not text:
            logger.warning("Empty text provided for JSON extraction")
            return []

        # First, try to find JSON objects with flexible key matching
        patterns = [
            # Standard JSON pattern
            r'\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\}',
            # More flexible pattern for malformed JSON
            r'\{[^{}]*["\'](?:answer|eligibility|eligible)["\'][^{}]*\}',
        ]

        all_matches = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            all_matches.extend(matches)

        # Remove duplicates while preserving order
        seen = set()
        unique_matches = []
        for match in all_matches:
            if match not in seen:
                seen.add(match)
                unique_matches.append(match)

        # Sort by length (longest first)
        unique_matches.sort(key=len, reverse=True)
        return unique_matches

    def _parse_json_candidate(self, json_candidate: str) -> Optional[Dict[str, Any]]:
        """
        Try to parse a JSON candidate string.
        Args:
            json_candidate: The string to parse as JSON
        Returns:
            Parsed JSON object or None if parsing failed
        """
        try:
            # Attempt to parse the candidate
            json_obj = json.loads(json_candidate)
            if self._is_valid_answer_json_flexible(json_obj):
                return json_obj
        except json.JSONDecodeError:
            # Try to fix common JSON issues and try again
            try:
                # Replace single quotes with double quotes
                fixed_json = json_candidate.replace("'", '"')
                json_obj = json.loads(fixed_json)
                if self._is_valid_answer_json_flexible(json_obj):
                    logger.info("Successfully parsed JSON after fixing quotes")
                    return json_obj
            except json.JSONDecodeError:
                pass
        return None

    def _is_valid_answer_json_flexible(self, json_obj: Dict[str, Any]) -> bool:
        """
        Very flexible validation - accepts almost any JSON with recognizable structure.
        Args:
            json_obj: The JSON object to check
        Returns:
            True if the JSON has some recognizable structure, False otherwise
        """
        # Check for the answer structure (very flexible)
        if "answer" in json_obj and isinstance(json_obj["answer"], dict):
            return True

        # Check for the personas structure for persona extraction
        if "personas" in json_obj and isinstance(json_obj["personas"], dict):
            return True

        # Check for relevance filtering structure
        if "is_relevant" in json_obj and "reason" in json_obj:
            return True

        # If it has at least one of the key fields we care about
        if isinstance(json_obj, dict):
            answer_keys = ["eligibility", "elgibility", "eligiblity", "eligibilty"]
            if any(key in json_obj for key in answer_keys):
                return True

        return False

    def _is_valid_answer_json(self, json_obj: Dict[str, Any]) -> bool:
        """
        Legacy method - kept for compatibility but now calls the flexible version
        """
        return self._is_valid_answer_json_flexible(json_obj)


# End of models/json_utils/extractors.py
# ================================================================================

# File 16/41: models/openai_model.py
# --------------------------------------------------------------------------------

# src/models/openai_model.py
from typing import List, Dict, Any

from .base import BaseModelClient
from assistant_manager import client, create_vector_store, create_assistant, update_assistant_vector
import json, time, logging

logger = logging.getLogger(__name__)

class OpenAIModelClient(BaseModelClient):
    def __init__(self, model_name: str, sys_prompt: str, file_paths: List[str]):
        self.vector_store = create_vector_store("RAG_VectorStore", file_paths)
        self.assistant = create_assistant("Insurance Expert Assistant", sys_prompt, model_name)
        update_assistant_vector(self.assistant.id, self.vector_store.id)
        self.thread = client.beta.threads.create()

    def query(self, question: str, context_files: List[str]) -> Dict[str, Any]:
        prompt = f"Question: \"{question}\". Give a Yes/No, quote the supporting text, and mention the amount if relevant."
        client.beta.threads.messages.create(thread_id=self.thread.id, role="user", content=prompt)
        run = client.beta.threads.runs.create(thread_id=self.thread.id, assistant_id=self.assistant.id)

        while client.beta.threads.runs.retrieve(thread_id=self.thread.id, run_id=run.id).status != "completed":
            time.sleep(1)

        messages = client.beta.threads.messages.list(thread_id=self.thread.id)
        for msg in reversed(messages.data):
            if msg.role == "assistant":
                return json.loads(msg.content[0].text.value.strip())
        return {}



# End of models/openai_model.py
# ================================================================================

# File 17/41: models/openrouter_model.py
# --------------------------------------------------------------------------------

# =============================================================================
# Enhanced OpenRouter Model with Complete Prompt and Output Logging
# =============================================================================

"""
OpenRouter model client implementation for insurance policy analysis.
Uses OpenRouter's API to access Qwen and other models through a unified endpoint.
Enhanced with comprehensive logging to show prompts and outputs.
"""
import logging
import os
import json
from typing import List, Dict, Any

from openai import OpenAI

from config import OPENROUTER_API_KEY, OPENROUTER_SITE_URL, OPENROUTER_SITE_NAME
from models.base import BaseModelClient
from models.json_utils.extractors import JSONExtractor

logger = logging.getLogger(__name__)


class OpenRouterModelClient(BaseModelClient):
    """
    Client for using OpenRouter API to access various models including Qwen.
    Handles model initialization, inference, and result formatting.
    Enhanced with comprehensive logging.
    """

    def __init__(self, model_name: str, sys_prompt: str):
        """
        Initialize an OpenRouter model client.

        Args:
            model_name: Name of the model to use (e.g., "qwen/qwen-2.5-72b-instruct")
            sys_prompt: System prompt for the model
        """
        logger.info(f"Loading OpenRouter model: {model_name}")

        # Log the system prompt being used
        logger.info(f"System prompt: {sys_prompt}")

        # Initialize OpenAI client with OpenRouter endpoint
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=OPENROUTER_API_KEY,
        )

        self.model_name = model_name
        self.base_prompt = sys_prompt.strip()

        # Initialize JSON extractor
        self.json_extractor = JSONExtractor()

        logger.info(f"OpenRouter client initialized successfully for model: {model_name}")

    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """Create a formatted error response."""
        return {
            "answer": {
                "eligibility": "Error",
                "eligibility_policy": error_message,
                "amount_policy": None
            }
        }

    def _format_context_text(self, context_files: List[str]) -> str:
        """Format context information from policy files."""
        if not context_files or len(context_files) == 0:
            return ""

        context_text = "\n\nRelevant policy information:\n\n"
        context_text += """
                        WARNING: Some of the following context may contain "SECTION DEFINITIONS" of terms or general 
                        information that does not directly indicate coverage. Please carefully distinguish 
                        between definitions, exclusions and actual coverage provisions.\n\n
                        """

        for i, ctx in enumerate(context_files, 1):
            context_text += f"Policy context {i}:\n{ctx.strip()}\n\n"

        # Log the formatted context
        logger.debug(f"Formatted context text (length: {len(context_text)} chars)")
        logger.debug(f"Context preview: {context_text[:500]}...")

        return context_text

    def _extract_persona_via_api(self, question: str) -> str:
        """Extract persona information using the API model."""
        try:
            persona_prompt = f"""
            Analyze this insurance query and extract ONLY information about the people involved and their locations:

            Query: "{question}"

            1. Who is making the insurance claim (the primary policy user/policyholder)?
            2. Who actually experienced the event, accident, health issue, loss, or damage?
            3. WHERE was the affected person when the event occurred?
            4. What is the relationship between the policyholder and the affected person?
            5. Is the affected person covered by the policy?
            6. Who else is mentioned but NOT making the claim?
            7. Total number of people in the scenario?
            8. Number of people actually covered by the insurance?

            Return JSON format:
            {{
              "personas": {{
                "policy_user": "Who is making the claim/policyholder",
                "affected_person": "Who experienced the event",
                "location": "Where the event occurred",
                "is_abroad": true or false,
                "relationship_to_policyholder": "Relationship",
                "is_affected_covered": true or false,
                "mentioned_people": "Others mentioned",
                "total_count": number,
                "claimant_count": number,
                "relationship": "Relationships between people"
              }}
            }}
            """

            # Log the persona extraction prompt
            logger.debug("=== PERSONA EXTRACTION PROMPT ===")
            logger.debug(persona_prompt)

            # Create extra headers for OpenRouter
            extra_headers = {}
            if OPENROUTER_SITE_URL:
                extra_headers["HTTP-Referer"] = OPENROUTER_SITE_URL
            if OPENROUTER_SITE_NAME:
                extra_headers["X-Title"] = OPENROUTER_SITE_NAME

            response = self.client.chat.completions.create(
                extra_headers=extra_headers,
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You extract persona information from insurance queries."},
                    {"role": "user", "content": persona_prompt}
                ],
                max_tokens=512,
                temperature=0.1
            )

            persona_response = response.choices[0].message.content
            logger.debug(f"=== PERSONA EXTRACTION RESPONSE ===")
            logger.debug(f"Raw persona response: {persona_response}")

            extracted_json = self.json_extractor.extract_json(persona_response)

            if extracted_json and "personas" in extracted_json:
                from models.persona.formatters import format_persona_text
                formatted_persona = format_persona_text(extracted_json)
                logger.info(f"Successfully extracted persona: {formatted_persona}")
                return formatted_persona
            else:
                logger.warning("Failed to extract valid persona JSON from API response")
                return ""

        except Exception as e:
            logger.warning(f"API persona extraction failed: {str(e)}")
            return ""

    def _build_messages(self, question: str, context_text: str, persona_text: str) -> List[Dict[str, str]]:
        """Build the message array for the OpenRouter API call."""
        if persona_text:
            user_content = f"{context_text}\n\nQuestion: {question}\n\n{persona_text}"
        else:
            user_content = f"{context_text}\n\nQuestion: {question}"

        messages = [
            {"role": "system", "content": self.base_prompt},
            {"role": "user", "content": user_content}
        ]

        # Log the complete messages being sent to the API
        logger.info("=== MAIN QUERY MESSAGES ===")
        logger.info(f"System message: {messages[0]['content']}")
        logger.info(f"User message length: {len(messages[1]['content'])} characters")
        logger.debug(f"Full user message: {messages[1]['content']}")

        return messages

    def _generate_model_output(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """Generate output from the OpenRouter API."""
        try:
            # Create extra headers for OpenRouter
            extra_headers = {}
            if OPENROUTER_SITE_URL:
                extra_headers["HTTP-Referer"] = OPENROUTER_SITE_URL
            if OPENROUTER_SITE_NAME:
                extra_headers["X-Title"] = OPENROUTER_SITE_NAME

            logger.info(f"Making API call to model: {self.model_name}")
            logger.debug(f"Extra headers: {extra_headers}")

            response = self.client.chat.completions.create(
                extra_headers=extra_headers,
                model=self.model_name,
                messages=messages,
                max_tokens=2048,
                temperature=0.1,
                top_p=0.9
            )

            generated = response.choices[0].message.content

            # Log the complete raw output
            logger.info("=== MODEL OUTPUT ===")
            logger.info(f"Raw model response: {generated}")

            # Log token usage if available
            if hasattr(response, 'usage'):
                logger.info(f"Token usage: {response.usage}")

            extracted_json = self.json_extractor.extract_json(generated)

            if extracted_json:
                logger.info("=== EXTRACTED JSON ===")
                logger.info(f"Parsed JSON: {json.dumps(extracted_json, indent=2)}")
                return extracted_json

            logger.error("JSON extraction failed - returning default response")
            logger.error(f"Failed to extract JSON from: {generated}")
            return self._create_default_no_conditions_response()

        except Exception as e:
            error_msg = f"Error during OpenRouter API call: {str(e)}"
            logger.error(error_msg)
            return self._create_error_response(error_msg)

    def _create_default_no_conditions_response(self) -> Dict[str, Any]:
        """Create a default response when JSON extraction fails."""
        default_response = {
            "answer": {
                "eligibility": "",
                "eligibility_policy": "",
                "amount_policy": ""
            }
        }
        logger.warning(f"Using default response: {default_response}")
        return default_response

    def query(self, question: str, context_files: List[str], use_persona: bool = False) -> Dict[str, Any]:
        """Process a query and return the model's response."""
        logger.info("=" * 60)
        logger.info(f"NEW QUERY STARTED")
        logger.info(f"Model: {self.model_name}")
        logger.info(f"Question: {question}")
        logger.info(f"Context files count: {len(context_files) if context_files else 0}")
        logger.info(f"Use persona: {use_persona}")
        logger.info("=" * 60)

        persona_text = ""
        if use_persona:
            logger.info("Starting persona extraction...")
            persona_text = self._extract_persona_via_api(question)
            if persona_text:
                logger.info("Successfully extracted persona information")
                logger.debug(f"Persona text: {persona_text}")
            else:
                logger.info("No persona information extracted")
        else:
            logger.info("Skipping persona extraction")

        context_text = self._format_context_text(context_files)
        messages = self._build_messages(question, context_text, persona_text)

        result = self._generate_model_output(messages)

        logger.info("=== FINAL RESULT ===")
        logger.info(f"Final result: {json.dumps(result, indent=2)}")
        logger.info("=" * 60)

        return result


# =============================================================================
# LOGGING CONFIGURATION HELPER
# =============================================================================

def setup_detailed_logging():
    """
    Setup logging configuration to see all the prompt and output details.
    Call this at the start of your application.
    """
    logging.basicConfig(
        level=logging.DEBUG,  # Set to DEBUG to see all logs
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),  # Console output
            logging.FileHandler('openrouter_model.log')  # File output
        ]
    )

    # You can also set specific loggers to different levels
    logging.getLogger('openai').setLevel(logging.WARNING)  # Reduce OpenAI client noise
    logging.getLogger('httpx').setLevel(logging.WARNING)  # Reduce HTTP client noise


# =============================================================================
# USAGE EXAMPLE
# =============================================================================

if __name__ == "__main__":
    # Setup logging first
    setup_detailed_logging()

    # Example usage
    client = OpenRouterModelClient(
        model_name="qwen/qwen-2.5-72b-instruct",
        sys_prompt="You are an insurance policy analyzer..."
    )

    result = client.query(
        question="Is my trip to Spain covered?",
        context_files=["Policy section about international travel..."],
        use_persona=True
    )


# End of models/openrouter_model.py
# ================================================================================

# File 18/41: models/persona/__init__.py
# --------------------------------------------------------------------------------



# End of models/persona/__init__.py
# ================================================================================

# File 19/41: models/persona/extractor.py
# --------------------------------------------------------------------------------

# src/models/persona/extractor.py

"""
Main persona extraction functionality.
Coordinates rule-based and LLM-based extraction methods.
"""
import logging
from typing import Dict, Any

from models.persona.rule_based import RuleBasedExtractor
from models.persona.llm_based import LLMBasedExtractor
from models.persona.formatters import format_persona_text

logger = logging.getLogger(__name__)


class PersonaExtractor:
    """
    Extracts persona information from insurance queries.
    Combines rule-based and LLM-based approaches.
    """

    def __init__(self, pipe):
        """
        Initialize persona extractors.

        Args:
            pipe: HuggingFace pipeline for LLM-based extraction
        """
        self.rule_based_extractor = RuleBasedExtractor()
        self.llm_based_extractor = LLMBasedExtractor(pipe)

    def extract_personas(self, question: str) -> Dict[str, Any]:
        """
        Extract persona information from a question.

        Args:
            question: The insurance query to analyze

        Returns:
            Dictionary with persona information
        """
        logger.info(f"Extracting personas from question: {question}")

        # First try rule-based extraction
        rule_based_result = self.rule_based_extractor.extract(question)

        # Try LLM-based approach as a backup
        llm_result = self.llm_based_extractor.extract(question)

        # Use LLM result if available, otherwise use rule-based
        if llm_result:
            logger.info("Using LLM-extracted persona information")
            return llm_result
        else:
            logger.info("Using rule-based persona extraction")
            return rule_based_result

    def format_persona_text(self, personas_info: Dict[str, Any]) -> str:
        """
        Format persona information for inclusion in a prompt.

        Args:
            personas_info: Dictionary with persona information

        Returns:
            Formatted persona text
        """
        return format_persona_text(personas_info)

# End of models/persona/extractor.py
# ================================================================================

# File 20/41: models/persona/formatters.py
# --------------------------------------------------------------------------------

# src/models/persona/formatters.py

"""
Formatting functions for persona information.
Converts extracted persona information into text for prompts.
"""
from typing import Dict, Any


def format_persona_text(personas_info: Dict[str, Any]) -> str:
    """
    Format persona information for inclusion in the prompt, with enhanced focus
    on the affected person and their location during the event.

    Args:
        personas_info: Dictionary with persona information

    Returns:
        Formatted persona text
    """
    persona_text = "IMPORTANT PERSONA INFORMATION FROM THE QUESTION:\n"

    try:
        # Extract persona details
        policy_user = personas_info["personas"]["policy_user"]
        affected_person = personas_info["personas"].get("affected_person", f"{policy_user} (inferred)")
        location = personas_info["personas"].get("location", "unspecified location")
        is_abroad = personas_info["personas"].get("is_abroad", False)
        relationship_to_policyholder = personas_info["personas"].get("relationship_to_policyholder",
                                                                     "self (inferred)")
        is_affected_covered = personas_info["personas"].get("is_affected_covered", True)
        mentioned_people = personas_info["personas"]["mentioned_people"]
        total_count = personas_info["personas"]["total_count"]
        claimant_count = personas_info["personas"]["claimant_count"]
        relationship = personas_info["personas"]["relationship"]

        # Build detailed persona text
        persona_text += f"- Primary policy user/claimant: {policy_user}\n"
        persona_text += f"- Person who experienced the event/accident: {affected_person}\n"
        persona_text += f"- Location where the event occurred: {location}\n"
        persona_text += f"- Event occurred abroad: {'Yes' if is_abroad else 'No'}\n"
        persona_text += f"- Relationship to policyholder: {relationship_to_policyholder}\n"

        # Add coverage information for affected person
        if is_affected_covered:
            persona_text += f"- The affected person is likely covered under this policy\n"
        else:
            persona_text += f"- The affected person may NOT be covered under this policy\n"

        persona_text += f"- Other people mentioned (not policy users): {mentioned_people}\n"
        persona_text += f"- Total number of people mentioned: {total_count}\n"
        persona_text += f"- Number of people actually claiming/covered: {claimant_count}\n"
        persona_text += f"- Relationships between all people: {relationship}\n\n"

        # Add location-specific guidance
        if is_abroad:
            persona_text += "When determining coverage, check if the policy covers events occurring abroad and any special conditions or exclusions for international coverage.\n"

        if "airport" in location.lower():
            persona_text += "When determining coverage, focus on travel-related provisions, especially those related to baggage, delays, or airport incidents.\n"

        elif "hotel" in location.lower() or "resort" in location.lower():
            persona_text += "When determining coverage, check if accommodation-related incidents are covered and under what conditions.\n"

        elif "hospital" in location.lower() or "medical" in location.lower():
            persona_text += "When determining coverage, focus on medical coverage provisions, including emergency treatment and hospitalization.\n"

        elif "home" in location.lower() or "domestic" in location.lower():
            persona_text += "When determining coverage, verify if domestic incidents are covered, as some travel policies only apply when traveling.\n"

        # Add person-specific guidance
        if relationship_to_policyholder == "self":
            persona_text += "Also check provisions that apply to the policyholder directly.\n"
        elif relationship_to_policyholder in ["spouse/partner", "child/dependent"]:
            persona_text += "Also verify if family members/dependents are covered and under what conditions.\n"
        else:
            persona_text += "Also carefully verify if non-family members are covered under this policy.\n"

        persona_text += "Take into account all of these factors when assessing eligibility for coverage.\n"
    except (KeyError, TypeError):
        persona_text += "Unable to extract detailed persona information. Consider who is actually making the claim vs. who experienced the event vs. who is just mentioned and where the event occurred.\n"

    return persona_text


# End of models/persona/formatters.py
# ================================================================================

# File 21/41: models/persona/llm_based.py
# --------------------------------------------------------------------------------

# src/models/persona/llm_based.py

"""
LLM-based persona extraction.
Uses large language models to extract persona information.
"""
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


class LLMBasedExtractor:
    """
    LLM-based extractor for persona information.
    Uses language models to extract information.
    """

    def __init__(self, pipe):
        """
        Initialize the LLM-based extractor.

        Args:
            pipe: HuggingFace pipeline for inference
        """
        self.pipe = pipe

    def extract(self, question: str) -> Optional[Dict[str, Any]]:
        """
        Extract persona information using LLM-based approach.

        Args:
            question: The insurance query to analyze

        Returns:
            Dictionary with persona information or None if extraction failed
        """
        try:
            # Construct a persona-focused prompt
            persona_prompt = self._create_persona_prompt(question)

            # Use a shorter context window
            outputs = self.pipe(
                persona_prompt,
                max_new_tokens=200,
                do_sample=False,
                num_return_sequences=1,
                return_full_text=False
            )

            # Get the generated text
            generated = outputs[0]["generated_text"]
            logger.debug(f"Persona extraction output: {generated}")

            # Try to extract JSON from the output
            from models.json_utils.extractors import JSONExtractor
            json_extractor = JSONExtractor()
            llm_result = json_extractor.extract_json(generated)

            # Return the result if valid
            if llm_result and "personas" in llm_result:
                # Validate and fix values if needed
                llm_result = self._validate_llm_persona_result(llm_result)
                logger.info("Using LLM-extracted persona information")
                return llm_result

            return None

        except Exception as e:
            logger.warning(f"LLM persona extraction failed: {str(e)}")
            return None

    def _create_persona_prompt(self, question: str) -> str:
        """
        Create a prompt for persona extraction with enhanced focus on affected person
        and their location during the event.

        Args:
            question: The insurance query to analyze

        Returns:
            Formatted prompt for the LLM
        """
        return f"""
        Analyze this insurance query and extract ONLY information about the people involved and their locations:

        Query: "{question}"

        1. Who is making the insurance claim (the primary policy user/policyholder)?
        2. Who actually experienced the event, accident, health issue, loss, or damage that is the subject of the claim?
        3. WHERE was the affected person when the event occurred? (e.g., at home, abroad, at the airport, in a hotel)
        4. What is the relationship between the policyholder and the affected person?
        5. Is the affected person covered by the policy? (Usually yes if they are the policyholder, spouse, or dependent)
        6. Who else is mentioned but NOT making the claim or experiencing the event?
        7. Total number of people in the scenario?
        8. Number of people actually covered by the insurance or making claims?

        IMPORTANT: Carefully distinguish between these roles:
        - The POLICYHOLDER (who owns the policy and usually makes the claim)
        - The AFFECTED PERSON (who actually experienced the event, accident, illness, loss, or damage)
        - OTHER PEOPLE who are merely mentioned but not directly involved

        Pay special attention to LOCATION information, which is often critical for insurance claims:
        - Was the event domestic or international?
        - Was the person in transit (airport, train station, etc.)?
        - Was the person at a specific venue (hotel, resort, hospital)?
        - Was the person in their home country or abroad?

        Examples to consider:
        - "At the airport my baggage was lost" → Location: airport
        - "During our vacation in Spain my daughter got sick" → Location: abroad (Spain)
        - "My house was damaged by a storm" → Location: home/domestic
        - "While staying at the hotel, my wallet was stolen" → Location: hotel

        IMPORTANT: The answer must ONLY be valid JSON in this EXACT format:
        {{
          "personas": {{
            "policy_user": "Who is making the claim/policyholder",
            "affected_person": "Who experienced the event/accident/illness/loss/damage",
            "location": "Where the affected person was when the event occurred",
            "is_abroad": true or false (whether the event occurred outside home country),
            "relationship_to_policyholder": "Relationship between affected person and policyholder (self, spouse, child, etc.)",
            "is_affected_covered": true or false (whether the affected person is likely covered),
            "mentioned_people": "Who else is mentioned but not a claimant or affected",
            "total_count": number of ALL people mentioned,
            "claimant_count": number of people actually claiming/using the insurance,
            "relationship": "Relationships between all mentioned people"
          }}
        }}
        """

    def _validate_llm_persona_result(self, llm_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate and fix persona extraction results, including location information.

        Args:
            llm_result: The LLM extraction result to validate

        Returns:
            Validated and corrected result
        """
        # Ensure the personas key exists
        if "personas" not in llm_result:
            llm_result["personas"] = {}

        # VALIDATION: Ensure the counts make sense
        if "total_count" in llm_result["personas"]:
            # Get the count from LLM
            llm_count = llm_result["personas"]["total_count"]
            # Ensure it's reasonable (1-5) for most queries
            if not isinstance(llm_count, int) or llm_count < 1 or llm_count > 5:
                # If unreasonable, use a default count
                llm_result["personas"]["total_count"] = 1
                logger.warning(f"Adjusted implausible LLM total_count from {llm_count} to 1")

        # Make sure claimant_count is not greater than total_count
        if "claimant_count" in llm_result["personas"] and "total_count" in llm_result["personas"]:
            if llm_result["personas"]["claimant_count"] > llm_result["personas"]["total_count"]:
                llm_result["personas"]["claimant_count"] = llm_result["personas"]["total_count"]
                logger.warning("Adjusted claimant_count to not exceed total_count")

        # Ensure claimant_count exists
        if "claimant_count" not in llm_result["personas"]:
            llm_result["personas"]["claimant_count"] = 1
            logger.warning(f"Added missing claimant_count: 1")

        # Ensure policy_user exists
        if "policy_user" not in llm_result["personas"]:
            llm_result["personas"]["policy_user"] = "policyholder (individual)"
            logger.warning("Added missing policy_user with default value")

        # If affected_person is missing, use policy_user as a fallback
        if "affected_person" not in llm_result["personas"] and "policy_user" in llm_result["personas"]:
            llm_result["personas"]["affected_person"] = f"{llm_result['personas']['policy_user']} (inferred)"
            logger.warning("Added missing affected_person, inferred from policy_user")

        # If location is missing, infer from context or set to unknown
        if "location" not in llm_result["personas"]:
            # Try to infer from other fields
            affected_person_desc = llm_result["personas"].get("affected_person", "").lower()
            context = affected_person_desc + " " + llm_result["personas"].get("relationship", "").lower()

            if "airport" in context:
                llm_result["personas"]["location"] = "airport"
            elif "hotel" in context or "resort" in context:
                llm_result["personas"]["location"] = "hotel/resort"
            elif "hospital" in context or "clinic" in context:
                llm_result["personas"]["location"] = "hospital/medical facility"
            elif "home" in context or "house" in context:
                llm_result["personas"]["location"] = "home"
            elif any(place in context for place in ["abroad", "foreign", "overseas", "international"]):
                llm_result["personas"]["location"] = "abroad (unspecified location)"
            else:
                llm_result["personas"]["location"] = "unspecified location"

            logger.warning(f"Added missing location: {llm_result['personas']['location']}")

        # If is_abroad is missing, infer from location
        if "is_abroad" not in llm_result["personas"]:
            location = llm_result["personas"].get("location", "").lower()
            is_abroad = any(term in location for term in ["abroad", "foreign", "overseas", "international"])

            # Also check for specific country or region names that would indicate abroad
            country_indicators = ["europe", "asia", "america", "africa", "australia"]
            if any(country in location for country in country_indicators):
                is_abroad = True

            llm_result["personas"]["is_abroad"] = is_abroad
            logger.warning(f"Added missing is_abroad: {is_abroad}")

        # Add relationship_to_policyholder if missing
        if "relationship_to_policyholder" not in llm_result["personas"]:
            # If affected person is the same as policy user, relationship is "self"
            if (llm_result["personas"].get("affected_person", "").lower() ==
                    llm_result["personas"].get("policy_user", "").lower() or
                    "self" in llm_result["personas"].get("affected_person", "").lower()):
                llm_result["personas"]["relationship_to_policyholder"] = "self"
            else:
                # Otherwise infer from context
                affected = llm_result["personas"].get("affected_person", "").lower()
                if any(term in affected for term in ["spouse", "wife", "husband", "partner"]):
                    llm_result["personas"]["relationship_to_policyholder"] = "spouse/partner"
                elif any(term in affected for term in ["child", "son", "daughter"]):
                    llm_result["personas"]["relationship_to_policyholder"] = "child/dependent"
                elif any(term in affected for term in ["parent", "father", "mother"]):
                    llm_result["personas"]["relationship_to_policyholder"] = "parent"
                else:
                    llm_result["personas"]["relationship_to_policyholder"] = "unknown (inferred)"
            logger.warning(
                f"Added missing relationship_to_policyholder: {llm_result['personas']['relationship_to_policyholder']}")

        # Add is_affected_covered if missing
        if "is_affected_covered" not in llm_result["personas"]:
            # Default assumption based on relationship
            rel = llm_result["personas"].get("relationship_to_policyholder", "").lower()
            if any(r in rel for r in ["self", "spouse", "partner", "child", "dependent"]):
                llm_result["personas"]["is_affected_covered"] = True
            else:
                llm_result["personas"]["is_affected_covered"] = False
            logger.warning(f"Added missing is_affected_covered: {llm_result['personas']['is_affected_covered']}")

        return llm_result


# End of models/persona/llm_based.py
# ================================================================================

# File 22/41: models/persona/location.py
# --------------------------------------------------------------------------------

#  src/models/persona/location.py

"""
Location detection for insurance queries.
Identifies where an event occurred.
"""
import re
from typing import Tuple


class LocationDetector:
    """
    Detects location information in insurance queries.
    """

    def identify_location(self, question: str) -> Tuple[str, bool]:
        """
        Identify location information from the insurance query.

        Args:
            question: The query to analyze

        Returns:
            Tuple of (location, is_abroad)
        """
        # Common location patterns
        airport_patterns = [
            r'(?:at|in)\s+(?:the)?\s+airport',
            r'baggage.{1,30}(?:lost|delayed|missing|derouted)',
            r'luggage.{1,30}(?:lost|delayed|missing|derouted)',
            r'airport.{1,30}(?:lost|delayed|missing)',
            r'flight.{1,30}(?:lost|delayed|missing)',
            r'check-in.{1,30}(?:lost|delayed|missing)',
        ]

        hotel_patterns = [
            r'(?:at|in)\s+(?:the|my|our)?\s+hotel',
            r'(?:at|in)\s+(?:the|a)?\s+resort',
            r'staying\s+(?:at|in)',
            r'during\s+(?:my|our)\s+stay',
            r'accommodation'
        ]

        hospital_patterns = [
            r'(?:at|in)\s+(?:the|a)?\s+hospital',
            r'medical\s+(?:facility|center|centre)',
            r'emergency\s+room',
            r'clinic',
            r'doctor',
            r'medical\s+treatment'
        ]

        abroad_patterns = [
            r'(?:abroad|overseas|internationally)',
            r'(?:in|to|from|at)\s+(?!home|my home|our home)([A-Z][a-z]+)',  # Country/city names
            r'foreign\s+(?:country|place|location)',
            r'outside\s+(?:my|the|our)\s+country',
            r'international\s+(?:trip|travel|journey)',
            r'vacation\s+(?:in|to|at)'
        ]

        domestic_patterns = [
            r'(?:at|in)\s+(?:my|our)\s+home',
            r'at\s+home',
            r'domestic',
            r'within\s+(?:my|the|our)\s+country'
        ]

        transportation_patterns = [
            r'(?:on|in)\s+(?:the|a)?\s+(?:train|bus|car|taxi|subway|metro)',
            r'driving',
            r'during\s+(?:the|my|our)?\s+(?:journey|trip|travel|transit)',
            r'(?:train|bus|subway|car|taxi)\s+(?:station|terminal|stop)'
        ]

        # First check if it's abroad (this should be checked first to combine with location)
        is_abroad = any(re.search(pattern, question, re.IGNORECASE) for pattern in abroad_patterns)

        # Check for specific locations by type
        if any(re.search(pattern, question, re.IGNORECASE) for pattern in airport_patterns):
            return "airport", is_abroad
        elif any(re.search(pattern, question, re.IGNORECASE) for pattern in hotel_patterns):
            return "hotel/resort", is_abroad
        elif any(re.search(pattern, question, re.IGNORECASE) for pattern in hospital_patterns):
            return "hospital/medical facility", is_abroad
        elif any(re.search(pattern, question, re.IGNORECASE) for pattern in transportation_patterns):
            return "during transportation", is_abroad
        elif any(re.search(pattern, question, re.IGNORECASE) for pattern in domestic_patterns):
            return "at home/domestic", False  # Domestic patterns override is_abroad

        # If no specific location but abroad is detected
        if is_abroad:
            return "abroad (unspecified location)", True

        # Default if no clear patterns
        return "unspecified location", False


# End of models/persona/location.py
# ================================================================================

# File 23/41: models/persona/rule_based.py
# --------------------------------------------------------------------------------

# src/models/persona/rule_based.py

"""
Rule-based persona extraction.
Extracts persona information using pattern matching and heuristics.
"""
import re
import logging
from typing import Dict, Any, List, Tuple, Optional

from models.persona.location import LocationDetector

logger = logging.getLogger(__name__)


class RuleBasedExtractor:
    """
    Rule-based extractor for persona information.
    Uses regex patterns and heuristics to extract information.
    """

    def __init__(self):
        """Initialize the rule-based extractor."""
        self.location_detector = LocationDetector()

    def extract(self, question: str) -> Dict[str, Any]:
        """
        Extract persona information using rule-based approach.

        Args:
            question: The insurance query to analyze

        Returns:
            Dictionary with persona information
        """
        # Initialize defaults
        policy_user = None
        affected_person = None
        mentioned_people = []
        relationship = None
        total_count = 1
        claimant_count = 1

        # Extract family relationships
        found_relationships, family_mentioned = self._extract_family_relationships(question)
        mentioned_people.extend(family_mentioned)

        # Extract non-family relationships
        other_relationships, other_mentioned = self._extract_non_family_relationships(question)
        found_relationships.extend(other_relationships)
        mentioned_people.extend(other_mentioned)

        # Try to identify who experienced the event
        affected_person = self._identify_affected_person(question)

        # Extract location information
        location, is_abroad = self.location_detector.identify_location(question)

        # Handle special cases
        (special_policy_user,
         special_mentioned,
         special_relationship,
         special_total_count,
         special_claimant_count,
         special_affected_person) = self._handle_special_case_personas(question, found_relationships)

        if special_policy_user:
            policy_user = special_policy_user
        if special_mentioned:
            mentioned_people = special_mentioned
        if special_relationship:
            relationship = special_relationship
        if special_total_count:
            total_count = special_total_count
        if special_claimant_count:
            claimant_count = special_claimant_count
        if special_affected_person:
            affected_person = special_affected_person

        # If not a special case, determine defaults
        if not policy_user:
            (policy_user,
             relationship,
             default_total,
             default_claimant,
             relationship) = self._determine_default_persona_info(
                question,
                found_relationships
            )

            # Only use defaults if not set by special cases
            if not special_total_count:
                total_count = default_total
            if not special_claimant_count:
                claimant_count = default_claimant

        # If there are specific people mentioned, update total count
        if mentioned_people:
            mentioned_count = len(mentioned_people)
            total_count = max(total_count, mentioned_count)

        # If affected_person is still None, default to policy_user
        if not affected_person:
            affected_person = f"{policy_user} (inferred)"

        # Determine relationship to policyholder
        relationship_to_policyholder = self._determine_relationship_to_policyholder(policy_user, affected_person)

        # Determine if affected person is likely covered
        is_affected_covered = self._is_likely_covered(relationship_to_policyholder)

        # Format the result
        return {
            "personas": {
                "policy_user": policy_user,
                "affected_person": affected_person,
                "location": location,
                "is_abroad": is_abroad,
                "relationship_to_policyholder": relationship_to_policyholder,
                "is_affected_covered": is_affected_covered,
                "mentioned_people": ", ".join(mentioned_people) if mentioned_people else "None specifically mentioned",
                "total_count": total_count,
                "claimant_count": claimant_count,
                "relationship": relationship if relationship else "Not clearly specified"
            }
        }

    def _extract_family_relationships(self, question: str) -> Tuple[List[str], List[str]]:
        """
        Extract family relationship terms from the question.

        Args:
            question: The question to analyze

        Returns:
            Tuple of (found_relationships, mentioned_people)
        """
        found_relationships = []
        mentioned_people = []

        # Family relationship terms
        family_terms = {
            'daughter': 'daughter',
            'son': 'son',
            'child': 'child',
            'children': 'children',
            'wife': 'wife',
            'husband': 'husband',
            'spouse': 'spouse',
            'partner': 'partner',
            'mother': 'mother',
            'father': 'father',
            'parent': 'parent',
            'parents': 'parents',
            'grandparent': 'grandparent',
            'grandmother': 'grandmother',
            'grandfather': 'grandfather',
            'sister': 'sister',
            'brother': 'brother',
            'sibling': 'sibling',
            'aunt': 'aunt',
            'uncle': 'uncle',
            'cousin': 'cousin',
            'niece': 'niece',
            'nephew': 'nephew',
            'family': 'family member'
        }

        # Patterns for indirect references
        indirect_references = [
            r'my\s+(?:business\s+)?partner\s+had',
            r'my\s+(?:family\s+)?member\s+(?:had|was|got)',
            r'my\s+(?:colleague|coworker)\s+(?:had|was|got)',
            r'(?:death|illness|injury)\s+of\s+(?:my|the)',
        ]

        # Check for family members and relationships
        for term, relationship_type in family_terms.items():
            if re.search(r'\b' + term + r'\b', question, re.IGNORECASE):
                found_relationships.append(relationship_type)

                # Check if this is mentioned as a non-claimant
                is_indirect = any(
                    re.search(pattern + r'.*\b' + term + r'\b', question, re.IGNORECASE)
                    for pattern in indirect_references
                )

                # Add to mentioned people
                if is_indirect:
                    mentioned_people.append(f"{relationship_type} (not a claimant)")
                else:
                    mentioned_people.append(relationship_type)

        return found_relationships, mentioned_people

    def _extract_non_family_relationships(self, question: str) -> Tuple[List[str], List[str]]:
        """
        Extract non-family relationship terms from the question.

        Args:
            question: The question to analyze

        Returns:
            Tuple of (found_relationships, mentioned_people)
        """
        found_relationships = []
        mentioned_people = []

        # Non-family relationship terms
        other_terms = {
            'friend': 'friend',
            'friends': 'friends',
            'colleague': 'colleague',
            'coworker': 'coworker',
            'business partner': 'business partner',
            'neighbor': 'neighbor',
            'guest': 'guest',
            'traveler': 'fellow traveler'
        }

        # Patterns for indirect references
        indirect_references = [
            r'my\s+(?:business\s+)?partner\s+had',
            r'my\s+(?:family\s+)?member\s+(?:had|was|got)',
            r'my\s+(?:colleague|coworker)\s+(?:had|was|got)',
            r'(?:death|illness|injury)\s+of\s+(?:my|the)',
        ]

        # Check for non-family relationships
        for term, relationship_type in other_terms.items():
            if re.search(r'\b' + term + r'\b', question, re.IGNORECASE):
                found_relationships.append(relationship_type)

                # Check if this is mentioned as a non-claimant
                is_indirect = any(
                    re.search(pattern + r'.*\b' + term + r'\b', question, re.IGNORECASE)
                    for pattern in indirect_references
                )

                # Add to mentioned people
                if is_indirect:
                    mentioned_people.append(f"{relationship_type} (not a claimant)")
                else:
                    mentioned_people.append(relationship_type)

        return found_relationships, mentioned_people

    def _count_people_from_numeric_mentions(self, question: str) -> int:
        """
        Extract explicit numeric mentions of people in the question.

        Args:
            question: The question to analyze

        Returns:
            The explicitly mentioned count of people, or 0 if none found
        """
        # Pattern for phrases like "family of X", "group of X", "X of us", etc.
        patterns = [
            r'family\s+of\s+(\d+)',
            r'group\s+of\s+(\d+)',
            r'(\d+)\s+of\s+us',
            r'(\d+)\s+people',
            r'(\d+)\s+persons',
            r'(\d+)\s+travelers',
            r'(\d+)\s+members',
            r'for\s+(\d+)'
        ]

        # Look for matches
        for pattern in patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            if matches:
                # Return the first numeric match as an integer
                try:
                    return int(matches[0])
                except (ValueError, IndexError):
                    continue

        return 0  # No explicit numeric mentions found

    def _count_people_from_pronouns(self, question: str) -> int:
        """
        Count distinct people based on pronoun usage, with improved contextual understanding.

        Args:
            question: The question to analyze

        Returns:
            Estimated count of distinct people
        """
        # Check if "our" is being used in institutional context rather than indicating multiple people
        institutional_our_pattern = r'\b(our|at our) (hotel|resort|company|office|facility|premises|building|property|organization|institution)\b'
        has_institutional_our = bool(re.search(institutional_our_pattern, question, re.IGNORECASE))

        # Check for different types of pronouns
        has_first_person_singular = bool(re.search(r'\b(I|me|my)\b', question, re.IGNORECASE))

        # Modified first person plural check to exclude institutional "our"
        has_first_person_plural = bool(re.search(r'\b(we|us)\b', question, re.IGNORECASE))
        if not has_first_person_plural and re.search(r'\bour\b', question, re.IGNORECASE) and not has_institutional_our:
            has_first_person_plural = True

        has_second_person = bool(re.search(r'\b(you|your)\b', question, re.IGNORECASE))
        has_third_person_singular_male = bool(re.search(r'\b(he|him|his)\b', question, re.IGNORECASE))
        has_third_person_singular_female = bool(re.search(r'\b(she|her)\b', question, re.IGNORECASE))
        has_third_person_plural = bool(re.search(r'\b(they|them|their)\b', question, re.IGNORECASE))

        # Count distinct people based on pronoun types
        distinct_people_count = 0

        if has_first_person_singular:
            distinct_people_count += 1  # The speaker/policy holder

        if has_first_person_plural and not has_institutional_our:
            # 'We' implies at least 2 people, but only if not institutional
            distinct_people_count = max(distinct_people_count, 2)

        # Only count 'you' as another person when it's clearly referring to a different individual
        # and not the customer service or entity being addressed
        if has_second_person and re.search(r'\b(you|your) (?:and|with|also|too)\b', question, re.IGNORECASE):
            distinct_people_count += 1

        # More contextual checks for second person
        # Don't count 'you' in service requests like "can you help"
        if has_second_person and not re.search(r'(?:can|could|would|will|please)\s+you', question, re.IGNORECASE):
            distinct_people_count += 1

        if has_third_person_singular_male:
            distinct_people_count += 1

        if has_third_person_singular_female:
            distinct_people_count += 1

        if has_third_person_plural:
            # 'They' implies at least 2 more people
            if distinct_people_count == 0:
                distinct_people_count = 2  # Minimum for 'they'
            else:
                distinct_people_count += 1  # Add at least one more person

        # If no pronouns were found, default to 1 person (the claimant)
        if distinct_people_count == 0:
            distinct_people_count = 1

        return distinct_people_count

    def _determine_default_persona_info(
            self,
            question: str,
            found_relationships: List[str]
    ) -> Tuple[str, str, int, int, str]:
        """
        Determine default persona information based on pronouns and context.

        Args:
            question: The question to analyze
            found_relationships: List of relationships found in the question

        Returns:
            Tuple of (policy_user, relationship, total_count, claimant_count, relationship)
        """
        # Default based on pronouns
        if re.search(r'\b(I|me|my)\b', question, re.IGNORECASE) and not found_relationships:
            policy_user = "policyholder (individual)"
            relationship = "none mentioned"
            claimant_count = 1

        elif re.search(r'\b(we|us|our)\b', question, re.IGNORECASE) and not found_relationships:
            policy_user = "policyholder (group)"
            relationship = "group (unspecified)"
            claimant_count = 2  # At least 2 people

        elif found_relationships:
            # If relationships found but no clear policy user, assume relationship is claimant
            policy_user = found_relationships[0]
            relationship = "family member of policyholder"
            claimant_count = 1

        else:
            # Default if nothing could be determined
            policy_user = "undetermined"
            relationship = "Not clearly specified"
            claimant_count = 1

        # Get people count from pronouns
        pronoun_count = self._count_people_from_pronouns(question)

        # Get explicit numeric mentions of people count
        numeric_count = self._count_people_from_numeric_mentions(question)

        # Use the larger of the two counts
        total_count = max(pronoun_count, numeric_count)

        # If we have an explicit numeric count and we're in a family/group context,
        # update the claimant count accordingly
        if numeric_count > 0 and re.search(r'\b(we|us|our|family|group)\b', question, re.IGNORECASE):
            claimant_count = numeric_count

        return policy_user, relationship, total_count, claimant_count, relationship


    def _identify_affected_person(self, question: str) -> Optional[str]:
        """
        Identify who experienced the event/accident/illness in the query.

        Args:
            question: The query to analyze

        Returns:
            String describing who experienced the event or None if unclear
        """
        # Patterns for first-person event experiencing
        first_person_patterns = [
            r'I\s+(?:had|have|got|experienced|suffered|am suffering|was diagnosed with|developed)\s+(?:a|an)?\s*(?:illness|sickness|disease|condition|injury|accident|problem)',
            r'I\s+(?:broke|injured|hurt|damaged|lost)\s+my',
            r'I\s+(?:am|was|feel|felt)\s+(?:sick|ill|unwell|not well|injured)',
            r'my\s+(?:illness|sickness|disease|condition|injury|accident|problem)',
            r'I\s+need\s+(?:a|to see)\s+(?:doctor|medical|physician|hospital)',
            r'I\s+(?:had|have)\s+(?:pain|discomfort|symptoms)',
        ]

        # Patterns for family members experiencing events
        family_patterns = {
            'child': r'(?:my|our)\s+(?:child|kid|son|daughter)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'spouse': r'(?:my|our)\s+(?:spouse|husband|wife|partner)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'parent': r'(?:my|our)\s+(?:parent|father|mother|dad|mom)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'family': r'(?:my|our)\s+(?:family member|relative|brother|sister|sibling|uncle|aunt|cousin|nephew|niece)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
        }

        # Check for first person as affected
        for pattern in first_person_patterns:
            if re.search(pattern, question, re.IGNORECASE):
                return "policyholder (self)"

        # Check for family members as affected
        for family_type, pattern in family_patterns.items():
            if re.search(pattern, question, re.IGNORECASE):
                return f"{family_type} of policyholder"

        # Check for others as affected
        other_patterns = {
            'friend': r'(?:my|our)\s+friend\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'colleague': r'(?:my|our)\s+(?:colleague|coworker|co-worker)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
            'travel companion': r'(?:my|our)\s+(?:travel companion|fellow traveler|traveling partner)\s+(?:has|had|is|was|got|became|fell|is suffering|started)',
        }

        for other_type, pattern in other_patterns.items():
            if re.search(pattern, question, re.IGNORECASE):
                return f"{other_type} of policyholder"

        # Default to None if no clear matches
        return None

    def _handle_special_case_personas(
            self,
            question: str,
            found_relationships: List[str]
    ) -> Tuple[Optional[str], Optional[List[str]], Optional[str], Optional[int], Optional[int], Optional[str]]:
        """
        Handle special case persona scenarios, now including affected person.

        Args:
            question: The question to analyze
            found_relationships: List of relationships found in the question

        Returns:
            Tuple of (policy_user, mentioned_people, relationship, total_count, claimant_count, affected_person)
            Returns None for any values that couldn't be determined by special cases
        """
        policy_user = None
        mentioned_people = None
        relationship = None
        total_count = None
        claimant_count = None
        affected_person = None

        # Define family relation terms for consistent use
        family_terms = [
            'daughter', 'son', 'child', 'children', 'wife', 'husband', 'spouse', 'partner',
            'mother', 'father', 'parent', 'parents', 'grandparent', 'grandmother', 'grandfather',
            'sister', 'brother', 'sibling', 'aunt', 'uncle', 'cousin', 'niece', 'nephew',
            'family', 'relative'
        ]

        # Create regex pattern for family relationships
        family_pattern = r'\b(' + '|'.join(family_terms) + r')\b'

        # Special case: business partner scenario
        if re.search(r'my\s+(?:business\s+)?partner\s+had', question, re.IGNORECASE):
            if re.search(r'I\s+am\s+traveling\s+alone', question, re.IGNORECASE):
                policy_user = "policyholder (individual traveler)"
                mentioned_people = ["business partner (not traveling/insured)"]
                relationship = "business relationship (but not traveling together)"
                total_count = 2
                claimant_count = 1
                affected_person = "business partner (not a claimant)"
            else:
                policy_user = "policyholder (individual)"
                mentioned_people = ["business partner"]
                relationship = "business relationship"
                total_count = 2
                claimant_count = 1
                affected_person = "business partner"

        # Special case: my family member scenarios (using any family term)
        elif re.search(r'\bmy\s+' + family_pattern, question, re.IGNORECASE):
            policy_user = "policyholder (family member)"
            relationship = "family - " + ", ".join(found_relationships)

            # Check if family member is experiencing the event
            affected_match = re.search(
                r'my\s+(' + '|'.join(family_terms) + r')\s+(?:is|was|had|got|needs|has been|suffered|experienced)',
                question, re.IGNORECASE)
            if affected_match:
                affected_person = affected_match.group(1)

            # Extract all mentioned family members
            all_family_matches = re.findall(r'my\s+(' + '|'.join(family_terms) + r')', question, re.IGNORECASE)
            mentioned_people = [match for match in all_family_matches if match]

            # Check for explicit family size
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0

            if family_size > 0:
                total_count = family_size
                # Check if all family members are claiming
                if re.search(r'(?:all|everyone|each|the whole family|all \d+ of us|for all of us)', question,
                             re.IGNORECASE):
                    claimant_count = family_size
                else:
                    # Check if only family member is a claimant or multiple
                    if affected_person and not re.search(r'(?:all|everyone|each|all \d+ of us)', question,
                                                         re.IGNORECASE):
                        claimant_count = 1  # Just the affected family member
                    else:
                        claimant_count = min(family_size, len(mentioned_people) + 1)  # +1 for policyholder
            else:
                # No explicit family size
                total_count = max(2, len(mentioned_people) + 1)  # At least policyholder + mentioned people
                claimant_count = 1 if affected_person else total_count  # Either just affected or all

        # Special case: our family member scenario (using any family term)
        elif re.search(r'\bour\s+' + family_pattern, question, re.IGNORECASE):
            policy_user = "policyholder (joint policy with family)"
            relationship = "family - " + ", ".join(found_relationships)

            # Check for explicit family size
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0

            # Check who is actually affected/claiming
            affected_terms = [
                'all of us', 'everyone', 'the whole family',
                r'all \d+ of us', r'for the \d+ of us', 'for all of us'
            ]

            # Extract the specific affected family member if mentioned
            affected_match = re.search(r'(?:my|our)\s+(' + '|'.join(
                family_terms) + r')\s+(?:is|was|had|got|needs|has been|suffered|experienced)', question, re.IGNORECASE)
            if affected_match:
                affected_person = affected_match.group(1)

            # If the question indicates everyone is affected/claiming
            everyone_affected = any(re.search(pattern, question, re.IGNORECASE) for pattern in affected_terms)

            if family_size > 0:
                total_count = family_size

                # Determine claimant count based on context
                if everyone_affected:
                    claimant_count = family_size
                    # If we found a specific affected person but everyone is claiming
                    if affected_person:
                        affected_person = f"{affected_person} (but whole family of {family_size} is claiming)"
                elif affected_person:
                    # Check if only the affected person is claiming
                    if re.search(r'(?:only|just)\s+(?:my|our|the)\s+' + affected_person, question, re.IGNORECASE):
                        claimant_count = 1
                    else:
                        # If reimbursement is for all
                        if re.search(r'reimburse\s+(?:the|our|all).*(trip|vacation|holiday)', question, re.IGNORECASE):
                            claimant_count = family_size
                            affected_person = f"{affected_person} (but whole family of {family_size} is claiming)"
                        else:
                            claimant_count = 1
                else:
                    # Default to all family members if no specific affected person
                    claimant_count = family_size
            else:
                # No explicit family size, use "we/our" to imply at least 2
                total_count = max(2, len(found_relationships) + 1)
                claimant_count = 1 if affected_person and not everyone_affected else total_count

        # Special case: we/family scenario without explicit "our family" phrase
        elif re.search(r'\b(we|us)\b', question, re.IGNORECASE) and any(
                term in question.lower() for term in family_terms):
            policy_user = "policyholder (family group)"
            relationship = "family group"

            # Extract all mentioned family members
            all_family_matches = re.findall(r'(?:the|my|our)\s+(' + '|'.join(family_terms) + r')', question,
                                            re.IGNORECASE)
            mentioned_people = [match for match in all_family_matches if match]

            # Check for explicit family size
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0

            # Look for affected person
            affected_match = re.search(r'(?:my|our|the)\s+(' + '|'.join(
                family_terms) + r')\s+(?:is|was|had|got|needs|has been|suffered|experienced)', question, re.IGNORECASE)
            if affected_match:
                affected_person = affected_match.group(1)

            # Determine counts
            if family_size > 0:
                total_count = family_size

                # Check if everyone is claiming
                if re.search(r'(?:reimburse|refund|cover)\s+(?:the|our|all|us|everyone).*(trip|vacation|holiday|stay)',
                             question, re.IGNORECASE):
                    claimant_count = family_size
                    # If specific person affected but all claiming
                    if affected_person:
                        affected_person = f"{affected_person} (but all {family_size} travelers claiming)"
                elif affected_person:
                    # Default to just affected person claiming unless specified
                    claimant_count = 1
                else:
                    claimant_count = family_size
            else:
                # No explicit size, determine from context
                total_count = max(2, len(mentioned_people) + 1)  # At least 2 for "we"
                claimant_count = 1 if affected_person else total_count

        # Special case: traveling alone but mentioning business partner
        elif re.search(r'I\s+am\s+traveling\s+alone', question, re.IGNORECASE) and 'partner' in question.lower():
            policy_user = "policyholder (individual traveler)"
            mentioned_people = ["business partner (not traveling/insured)"]
            relationship = "business relationship"
            total_count = 2
            claimant_count = 1
            affected_person = "policyholder (self)"

        # Special case: personal item theft or loss
        elif re.search(r'(my|I).*(handbag|purse|bag|phone|document|wallet|passport|luggage)', question, re.IGNORECASE):
            policy_user = "policyholder (individual)"
            mentioned_people = []
            relationship = "none mentioned"

            # Check for group/family mentions
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0
            if family_size > 0 and re.search(r'\b(we|us|our|family)\b', question, re.IGNORECASE):
                total_count = family_size

                # Check if other people's items are also affected
                if re.search(r'(?:our|all our|everyone\'s).*(handbag|purse|bag|phone|document|wallet|passport|luggage)',
                             question, re.IGNORECASE):
                    claimant_count = family_size
                    affected_person = "entire group's belongings"
                else:
                    claimant_count = 1  # Usually only the owner of the item is claiming
                    affected_person = "policyholder's belongings"
            else:
                total_count = 1
                claimant_count = 1
                affected_person = "policyholder (self)"

        # Special case: illness or medical condition
        elif re.search(r'(I had|I got|I am|I was).*?(sick|ill|poisoning|diarrhea|vomiting|stomach|pain|fever|injured)',
                       question, re.IGNORECASE):
            policy_user = "policyholder (individual)"
            mentioned_people = []
            relationship = "none mentioned"

            # Check for group/family mentions
            family_size = self._count_people_from_numeric_mentions(question) if hasattr(self,
                                                                                        '_count_people_from_numeric_mentions') else 0
            if family_size > 0 and re.search(r'\b(we|us|our|family)\b', question, re.IGNORECASE):
                total_count = family_size

                # Check if only the person is sick or everyone
                if re.search(r'(we all|all of us).*?(sick|ill|injured|affected)', question, re.IGNORECASE):
                    claimant_count = family_size
                    affected_person = "entire family/group"
                else:
                    # Check for trip cancellation for everyone
                    if re.search(
                            r'(?:reimburse|refund|cover)\s+(?:the|our|all|us|everyone).*(trip|vacation|holiday|stay)',
                            question, re.IGNORECASE):
                        claimant_count = family_size
                        affected_person = "policyholder (but whole group of " + str(family_size) + " is claiming)"
                    else:
                        claimant_count = 1
                        affected_person = "policyholder (self)"
            else:
                total_count = 1
                claimant_count = 1
                affected_person = "policyholder (self)"

        return policy_user, mentioned_people, relationship, total_count, claimant_count, affected_person

    def _determine_relationship_to_policyholder(self, policy_user: str, affected_person: str) -> str:
        """
        Determine the relationship between the affected person and policyholder.

        Args:
            policy_user: The identified policy user
            affected_person: The identified affected person

        Returns:
            String describing relationship
        """
        # If they appear to be the same person
        if policy_user.lower() in affected_person.lower() or affected_person.lower() in policy_user.lower():
            return "self"

        # Check for keywords in the affected person description
        affected_lower = affected_person.lower()
        if any(term in affected_lower for term in ["spouse", "wife", "husband", "partner"]):
            return "spouse/partner"
        elif any(term in affected_lower for term in ["child", "son", "daughter"]):
            return "child/dependent"
        elif any(term in affected_lower for term in ["parent", "father", "mother"]):
            return "parent"
        elif any(term in affected_lower for term in ["brother", "sister", "sibling"]):
            return "sibling"
        elif any(term in affected_lower for term in ["friend", "colleague", "coworker"]):
            return "non-family relation"
        elif "family" in affected_lower:
            return "family member"

        # Default if no clear relationship
        return "unknown"

    def _is_likely_covered(self, relationship: str) -> bool:
        """
        Determine if the affected person is likely covered based on their relationship.

        Args:
            relationship: Relationship to policyholder

        Returns:
            Boolean indicating likely coverage
        """
        # These relationships are typically covered
        covered_relationships = [
            "self", "spouse/partner", "child/dependent", "family member"
        ]

        # Check if relationship is in typically covered list
        return relationship in covered_relationships


# End of models/persona/rule_based.py
# ================================================================================

# File 24/41: models/qwen_model.py
# --------------------------------------------------------------------------------

# src/models/qwen_model.py

"""
Qwen model client implementation for insurance policy analysis.
Based on the HuggingFace model client but optimized for Qwen models.
"""
import logging
import os
import re
from typing import Dict, Any

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from config import get_local_model_path, get_model_config
from models.hf_model import HuggingFaceModelClient
from models.json_utils.extractors import JSONExtractor
from models.persona.extractor import PersonaExtractor

logger = logging.getLogger(__name__)


class QwenModelClient(HuggingFaceModelClient):
    """
    Specialized client for Qwen models.
    Inherits from HuggingFaceModelClient but with Qwen-specific optimizations.
    """

    def __init__(self, model_name: str, sys_prompt: str):
        """
        Initialize a Qwen model client.

        Args:
            model_name: Name of the Qwen model to use
            sys_prompt: System prompt for the model
        """
        logger.info(f"Loading Qwen model: {model_name}")
        self._log_cache_locations()

        # Check for local model first
        model_path = get_local_model_path(model_name)
        self.model_config = get_model_config(model_name)

        self.pipe = self._initialize_qwen_pipeline(model_path)
        self.base_prompt = sys_prompt.strip()

        self.persona_extractor = PersonaExtractor(self.pipe)
        self.json_extractor = JSONExtractor()

    def _check_qwen_model_in_scratch(self, model_name: str) -> str:
        """Check if Qwen model exists in scratch directory."""
        # Support both 32B and 7B models
        model_mappings = {
            "Qwen/Qwen2.5-32B": "qwen2.5-32b",
            "qwen2.5-32b": "qwen2.5-32b",
            "Qwen/Qwen2.5-7B": "qwen2.5-7b",  # Add 7B support
            "qwen2.5-7b": "qwen2.5-7b"  # Add 7B support
        }

        if model_name in model_mappings:
            scratch_model_path = os.path.join(
                "/cluster/scratch",
                os.environ.get("USER", ""),
                "models",
                model_mappings[model_name]
            )

            if os.path.exists(scratch_model_path):
                logger.info(f"Found Qwen model in scratch: {scratch_model_path}")
                return scratch_model_path
            else:
                logger.warning(f"Qwen model not found in scratch directory.")

        return model_name

    def _initialize_qwen_pipeline(self, model_path: str):
        """Initialize the Qwen pipeline with optimized settings."""
        try:
            logger.info(f"Initializing Qwen pipeline with model: {model_path}")

            # Load tokenizer with Qwen-specific settings
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="left"  # Qwen often works better with left padding
            )

            # Ensure pad token is set
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # Load model with Qwen-optimized settings
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=self.model_config["torch_dtype"],
                device_map=self.model_config["device_map"],
                trust_remote_code=self.model_config["trust_remote_code"],
                low_cpu_mem_usage=self.model_config["low_cpu_mem_usage"]
            )

            # Create pipeline
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer
            )

            logger.info("Qwen pipeline successfully initialized")
            return pipe

        except Exception as e:
            logger.error(f"Error initializing Qwen pipeline: {e}")
            raise

    def _generate_model_output(self, prompt: str) -> Dict[str, Any]:
        """
        Generate output from Qwen model with optimized parameters.
        """
        try:
            # Qwen-specific generation parameters
            generation_params = {
                "max_new_tokens": self.model_config.get("max_new_tokens", 512),
                "temperature": None,
                "do_sample": self.model_config.get("do_sample", False),
                "repetition_penalty": self.model_config.get("repetition_penalty", 1.1),
                "num_return_sequences": 1,
                "return_full_text": False,
                "pad_token_id": self.pipe.tokenizer.eos_token_id,
                "eos_token_id": self.pipe.tokenizer.eos_token_id,
                "top_p": None,
                "top_k": None,
                "no_repeat_ngram_size": self.model_config.get("no_repeat_ngram_size", 3),
            }

            # Generate text
            outputs = self.pipe(prompt, **generation_params)

            # Get the generated text
            generated = outputs[0]["generated_text"]

            generated = self._clean_qwen_output(generated)
            logger.debug(f"Qwen model output: {generated}")

            # Try to extract JSON from the output
            extracted_json = self.json_extractor.extract_json(generated)

            if extracted_json:
                return extracted_json

            # If extraction fails, return formatted error
            logger.error("Failed to extract valid JSON from Qwen model output")
            return self._create_error_response("Failed to extract valid JSON from model output")

        except Exception as e:
            error_msg = f"Error during Qwen model inference: {str(e)}"
            logger.error(error_msg)
            return self._create_error_response(error_msg)

    def _clean_qwen_output(self, text: str) -> str:
        """
        Clean Qwen output to improve JSON extraction.
        """
        # Remove thinking tags
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

        # Remove any explanatory text before JSON
        lines = text.split('\n')
        json_started = False
        cleaned_lines = []

        for line in lines:
            if '{' in line and not json_started:
                json_started = True
            if json_started:
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines) if cleaned_lines else text

    def _build_prompt(self, question: str, context_text: str, persona_text: str) -> str:
        """
        Build the full prompt for Qwen model using proper chat template.
        """
        # Build user content - NO JSON schema
        if persona_text:
            user_content = f"{context_text}\n\nQuestion: {question}\n\n{persona_text}"
        else:
            user_content = f"{context_text}\n\nQuestion: {question}"

        # Use Qwen's apply_chat_template if available
        if hasattr(self.pipe.tokenizer, 'apply_chat_template'):
            try:
                messages = [
                    # {"role": "system", "content": },
                    {"role": "user", "content": (self.base_prompt + user_content)}
                ]

                formatted_prompt = self.pipe.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=True
                )
                logger.debug(f"Qwen chat template prompt: {formatted_prompt}")
                return formatted_prompt

            except Exception as e:
                logger.warning(f"Failed to use chat template: {e}, falling back to manual formatting")

        return ""



# End of models/qwen_model.py
# ================================================================================

# File 25/41: models/shared_client.py
# --------------------------------------------------------------------------------

# src/models/shared_client.py

"""
Shared model client for memory-efficient relevance filtering.
Uses the same model pipeline with different prompts to avoid loading multiple models.
"""
import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)


class SharedModelClient:
    """
    A wrapper that uses the same model pipeline with different prompts.
    This avoids loading multiple model instances and saves GPU memory.
    """

    def __init__(self, base_client, relevance_prompt: str):
        """
        Initialize the shared model client.

        Args:
            base_client: The main model client (HuggingFaceModelClient or OpenAIModelClient)
            relevance_prompt: The prompt to use for relevance filtering
        """
        self.base_client = base_client
        self.relevance_prompt = relevance_prompt
        self.original_prompt = base_client.base_prompt
        self.original_json_format = getattr(base_client, 'json_format', None)
        logger.info(f"Initialized SharedModelClient for relevance filtering")

    def query(self, question: str, context_files: List[str]) -> Dict[str, Any]:
        """
        Query the model using the relevance prompt.
        Temporarily switches the base client's prompt, runs the query, then restores.

        Args:
            question: The question to analyze for relevance
            context_files: List of relevant policy files

        Returns:
            Dictionary containing relevance analysis result
        """
        # Store original values
        original_prompt = self.base_client.base_prompt
        original_json_format = getattr(self.base_client, 'json_format', None)

        try:
            # Temporarily switch to relevance prompt
            self.base_client.base_prompt = self.relevance_prompt.strip()

            # Also temporarily switch JSON format if the base client has one
            if hasattr(self.base_client, 'json_format'):
                self.base_client.json_format = """
                    Return exactly this JSON:
                    {
                      "is_relevant": true/false,
                      "reason": "Brief explanation (≤ 25 words)"
                    }
                    """

            logger.debug(f"Switched to relevance prompt for query: {question}")

            # Query with relevance prompt (persona extraction not needed for relevance checks)
            result = self.base_client.query(question, context_files, use_persona=False)

            logger.debug(f"Relevance filtering result: {result}")
            return result

        except Exception as e:
            logger.error(f"Error in shared model relevance query: {str(e)}")
            # Return a safe default that assumes relevance
            return {
                "is_relevant": True,
                "reason": f"Error in relevance check: {str(e)}"
            }
        finally:
            # Always restore original values
            self.base_client.base_prompt = original_prompt
            if hasattr(self.base_client, 'json_format') and original_json_format is not None:
                self.base_client.json_format = original_json_format
            logger.debug("Restored original prompt and JSON format after relevance check")


# End of models/shared_client.py
# ================================================================================

# File 26/41: models/vector_store.py
# --------------------------------------------------------------------------------

# models/vector_store.py (Enhanced Version)

import logging
import re
import numpy as np
import os
from typing import List, Dict, Any, Optional, Union
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader

from config import CACHE_EMBEDDINGS, EMBEDDINGS_DIR

# Import chunking components
from models.chunking.base import ChunkingStrategy, ChunkResult, ChunkMetadata
from models.chunking.factory import ChunkingFactory, create_preset_strategy, auto_register_strategies

logger = logging.getLogger(__name__)


class EnhancedLocalVectorStore:
    """
    Enhanced vector store with pluggable chunking strategies.

    Supports multiple chunking approaches through the Strategy pattern,
    making it easy to test and compare different chunking methods.
    """

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 chunking_strategy: str = "simple",
                 chunking_config: Optional[Dict[str, Any]] = None):
        """
        Initialize enhanced vector store.

        Args:
            model_name: Sentence transformer model for embeddings
            chunking_strategy: Strategy name ('simple', 'structural', 'semantic', etc.)
            chunking_config: Optional configuration for the chunking strategy
        """
        logger.info(f"Initializing EnhancedLocalVectorStore with {chunking_strategy} chunking")

        # Auto-register strategies on first use
        auto_register_strategies()

        # Initialize sentence transformer
        try:
            self.model = SentenceTransformer(model_name)
            logger.info(f"Successfully loaded sentence-transformers model")
        except Exception as e:
            logger.error(f"Error loading embedding model {model_name}: {e}")
            raise RuntimeError(f"Failed to load embedding model: {e}")

        # Initialize chunking strategy
        self.chunking_strategy = self._create_chunking_strategy(
            chunking_strategy, chunking_config
        )

        # Storage for embeddings and chunks
        self.embeddings = None
        self.text_chunks = []
        self.chunk_metadata = []
        self.indexed_files = {}

        logger.info(f"EnhancedLocalVectorStore initialized successfully")

    def _create_chunking_strategy(self, strategy_name: str,
                                  config: Optional[Dict[str, Any]]) -> ChunkingStrategy:
        """Create the chunking strategy."""
        try:
            # Check if it's a preset
            if strategy_name in ['fast', 'balanced', 'comprehensive', 'research']:
                return create_preset_strategy(strategy_name)
            else:
                return ChunkingFactory.create_strategy(strategy_name, config)
        except Exception as e:
            logger.warning(f"Failed to create {strategy_name} strategy: {e}")
            logger.info("Falling back to simple chunking strategy")
            return ChunkingFactory.create_strategy('simple', {'max_length': 512})

    def get_chunking_info(self) -> Dict[str, Any]:
        """Get information about the current chunking strategy."""
        strategy_info = self.chunking_strategy.get_strategy_info()
        strategy_info.update({
            'total_chunks': len(self.text_chunks),
            'total_files_indexed': len(self.indexed_files),
            'has_embeddings': self.embeddings is not None,
            'embedding_model': self.model.get_sentence_embedding_dimension()
        })
        return strategy_info

    def switch_chunking_strategy(self, new_strategy: str,
                                 config: Optional[Dict[str, Any]] = None,
                                 reindex: bool = False):
        """
        Switch to a different chunking strategy.

        Args:
            new_strategy: Name of the new strategy
            config: Optional configuration for the new strategy
            reindex: Whether to re-index existing files with new strategy
        """
        logger.info(f"Switching from {self.chunking_strategy.name} to {new_strategy}")

        old_strategy = self.chunking_strategy
        self.chunking_strategy = self._create_chunking_strategy(new_strategy, config)

        if reindex and self.indexed_files:
            logger.info("Re-indexing files with new chunking strategy")
            file_paths = list(self.indexed_files.keys())
            self.index_documents(file_paths)
        else:
            logger.info("Strategy switched. Use reindex=True to re-process existing files.")

    def extract_policy_id(self, path: str) -> str:
        """Extract policy ID from filename."""
        filename = os.path.basename(path)
        match = re.match(r'^(\d+)_', filename)
        if match:
            return match.group(1)
        else:
            logger.warning(f"Could not extract policy ID from filename: {filename}")
            return os.path.splitext(filename)[0]

    def get_file_type(self, path: str) -> str:
        """Determine file type based on extension."""
        _, ext = os.path.splitext(path.lower())
        if ext == '.pdf':
            return 'pdf'
        elif ext == '.txt':
            return 'txt'
        else:
            return 'unknown'

    def extract_text_from_file(self, path: str) -> str:
        """Extract text from a file based on its type."""
        file_type = self.get_file_type(path)

        if file_type == 'pdf':
            return self._extract_text_from_pdf(path)
        elif file_type == 'txt':
            return self._extract_text_from_txt(path)
        else:
            logger.warning(f"Unsupported file type for {path}. Supported types: PDF, TXT")
            return ""

    def _extract_text_from_pdf(self, path: str) -> str:
        """Extract text from a PDF file."""
        try:
            reader = PdfReader(path)
            return "\n".join([page.extract_text() or "" for page in reader.pages])
        except Exception as e:
            logger.error(f"Error extracting text from PDF {path}: {e}")
            return ""

    def _extract_text_from_txt(self, path: str, encoding: str = 'utf-8') -> str:
        """Extract text from a TXT file."""
        try:
            with open(path, 'r', encoding=encoding) as file:
                return file.read()
        except UnicodeDecodeError:
            # Try with different encodings if utf-8 fails
            for fallback_encoding in ['latin-1', 'cp1252', 'iso-8859-1']:
                try:
                    with open(path, 'r', encoding=fallback_encoding) as file:
                        logger.info(f"Successfully read {path} using {fallback_encoding} encoding")
                        return file.read()
                except UnicodeDecodeError:
                    continue
            logger.error(f"Could not decode {path} with any common encoding")
            return ""
        except Exception as e:
            logger.error(f"Error extracting text from TXT {path}: {e}")
            return ""

    def index_documents(self, file_paths: Union[List[str], str]):
        """Index documents using the current chunking strategy."""
        if isinstance(file_paths, str):
            file_paths = [file_paths]

        logger.info(f"Indexing {len(file_paths)} documents using {self.chunking_strategy.name} strategy")

        # Reset storage
        self.text_chunks = []
        self.chunk_metadata = []
        self.indexed_files = {}

        all_chunk_results = []

        for path in file_paths:
            try:
                policy_id = self.extract_policy_id(path)
                file_type = self.get_file_type(path)
                logger.info(f"Processing {file_type.upper()} document: {path}")

                text = self.extract_text_from_file(path)
                if not text:
                    logger.warning(f"No text extracted from {path}")
                    continue

                # Use chunking strategy to process text
                chunk_results = self.chunking_strategy.chunk_text(
                    text=text,
                    policy_id=policy_id,
                    max_length=512  # Default, strategy may ignore
                )

                # Store file information
                self.indexed_files[path] = {
                    'policy_id': policy_id,
                    'file_type': file_type,
                    'chunk_count': len(chunk_results),
                    'strategy_used': self.chunking_strategy.name
                }

                all_chunk_results.extend(chunk_results)

                logger.info(f"Created {len(chunk_results)} chunks for policy {policy_id}")

            except Exception as e:
                logger.error(f"Error processing document {path}: {e}")

        if not all_chunk_results:
            logger.warning("No chunks created from documents")
            return

        # Extract texts and metadata
        self.text_chunks = self.chunking_strategy.get_chunk_text_list(all_chunk_results)
        self.chunk_metadata = self.chunking_strategy.get_metadata_list(all_chunk_results)

        logger.info(f"Creating embeddings for {len(self.text_chunks)} chunks")
        self.embeddings = self.embed(self.text_chunks)

        logger.info(f"Successfully indexed {len(self.text_chunks)} chunks using {self.chunking_strategy.name} strategy")

    def embed(self, texts: List[str]) -> np.ndarray:
        """Create embeddings for a list of text chunks."""
        if not texts:
            return np.array([])

        try:
            embeddings = self.model.encode(texts, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            return np.array([])

    def cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """Compute cosine similarity between query and document vectors."""
        if a.size == 0 or b.size == 0:
            return np.array([])

        a_norm = np.linalg.norm(a, axis=1, keepdims=True)
        b_norm = np.linalg.norm(b, axis=1, keepdims=True)

        a_norm = np.where(a_norm == 0, 1e-10, a_norm)
        b_norm = np.where(b_norm == 0, 1e-10, b_norm)

        a_normalized = a / a_norm
        b_normalized = b / b_norm

        return np.dot(a_normalized, b_normalized.T)

    def retrieve(self, query: str, k: int = 1, policy_id: Optional[str] = None) -> List[str]:
        """Retrieve the k most relevant text chunks for a query."""
        if self.embeddings is None or self.embeddings.size == 0:
            logger.warning("No embeddings available for retrieval")
            return []

        if not self.text_chunks:
            logger.warning("No text chunks available for retrieval")
            return []

        logger.info(f"Retrieving top {k} chunks for query using {self.chunking_strategy.name} strategy")
        query_embedding = self.embed([query])

        if query_embedding.size == 0:
            logger.warning("Failed to create query embedding")
            return []

        similarities = self.cosine_similarity(query_embedding, self.embeddings)
        if similarities.size == 0:
            logger.warning("Failed to compute similarities")
            return []

        similarities = similarities[0]

        # Filter by policy if specified
        if policy_id:
            policy_mask = np.array([
                meta.policy_id == policy_id for meta in self.chunk_metadata
            ])
            if np.any(policy_mask):
                filtered_similarities = np.where(policy_mask, similarities, -1)
                similarities = filtered_similarities

        # Get top k indices
        k = min(k, len(similarities))
        top_indices = np.argsort(similarities)[-k:][::-1]

        return [self.text_chunks[i] for i in top_indices]

    def get_chunk_statistics(self) -> Dict[str, Any]:
        """Get detailed statistics about the chunks."""
        if not self.chunk_metadata:
            return {"error": "No chunks available"}

        stats = {
            "strategy": self.chunking_strategy.name,
            "total_chunks": len(self.chunk_metadata),
            "chunk_types": {},
            "coverage_types": {},
            "avg_word_count": 0,
            "chunks_with_amounts": 0,
            "chunks_with_conditions": 0,
            "chunks_with_exclusions": 0,
            "policies_indexed": len(set(meta.policy_id for meta in self.chunk_metadata if meta.policy_id))
        }

        total_words = 0
        for meta in self.chunk_metadata:
            # Count chunk types
            chunk_type = meta.chunk_type
            stats["chunk_types"][chunk_type] = stats["chunk_types"].get(chunk_type, 0) + 1

            # Count coverage types
            coverage_type = meta.coverage_type
            stats["coverage_types"][coverage_type] = stats["coverage_types"].get(coverage_type, 0) + 1

            # Accumulate word count
            total_words += meta.word_count

            # Count special characteristics
            if meta.has_amounts:
                stats["chunks_with_amounts"] += 1
            if meta.has_conditions:
                stats["chunks_with_conditions"] += 1
            if meta.has_exclusions:
                stats["chunks_with_exclusions"] += 1

        stats["avg_word_count"] = total_words / len(self.chunk_metadata) if self.chunk_metadata else 0

        return stats

    def get_available_strategies(self) -> List[str]:
        """Get list of available chunking strategies."""
        return ChunkingFactory.get_available_strategies()

    def compare_strategies(self, strategies: List[str], sample_text: str,
                           policy_id: str = "test") -> Dict[str, Dict[str, Any]]:
        """
        Compare different chunking strategies on sample text.

        Args:
            strategies: List of strategy names to compare
            sample_text: Text to chunk for comparison
            policy_id: Policy ID for testing

        Returns:
            Dictionary mapping strategy names to their results
        """
        comparison_results = {}

        for strategy_name in strategies:
            try:
                # Create strategy instance
                strategy = ChunkingFactory.create_strategy(strategy_name)

                # Chunk the sample text
                chunks = strategy.chunk_text(sample_text, policy_id)

                # Analyze results
                comparison_results[strategy_name] = {
                    "chunk_count": len(chunks),
                    "avg_chunk_length": np.mean([len(chunk.text) for chunk in chunks]),
                    "chunk_types": list(set(chunk.metadata.chunk_type for chunk in chunks)),
                    "coverage_types": list(set(chunk.metadata.coverage_type for chunk in chunks)),
                    "chunks_with_amounts": sum(1 for chunk in chunks if chunk.metadata.has_amounts),
                    "strategy_info": strategy.get_strategy_info()
                }

            except Exception as e:
                comparison_results[strategy_name] = {
                    "error": str(e),
                    "chunk_count": 0
                }

        return comparison_results


# Backward compatibility: alias to existing LocalVectorStore interface
LocalVectorStore = EnhancedLocalVectorStore


# End of models/vector_store.py
# ================================================================================

# File 27/41: output_formatter.py
# --------------------------------------------------------------------------------

from config import DOCUMENT_DIR, is_qwen_model, is_phi_model, is_openrouter_model
from models.hf_model import HuggingFaceModelClient
from models.openai_model import OpenAIModelClient
from models.qwen_model import QwenModelClient
from models.openrouter_model import OpenRouterModelClient
from models.shared_client import SharedModelClient
from utils import list_policy_paths


def get_model_client(provider: str, model_name: str, sys_prompt: str):
    """
    Create appropriate model client based on provider and model name.

    Args:
        provider: Model provider ("openai", "hf", "qwen", "openrouter")
        model_name: Name of the model to use
        sys_prompt: System prompt for the model

    Returns:
        Appropriate model client instance
    """
    file_paths = list_policy_paths(DOCUMENT_DIR)

    if provider == "openai":
        return OpenAIModelClient(model_name, sys_prompt, file_paths)
    elif provider == "openrouter":
        return OpenRouterModelClient(model_name, sys_prompt)
    elif provider in ["hf", "qwen"]:
        # Auto-detect model type and use appropriate client
        if is_openrouter_model(model_name):
            # If user specified hf/qwen but model looks like OpenRouter, suggest correction
            print(f"Warning: Model '{model_name}' appears to be an OpenRouter model. Consider using --model openrouter")
            return OpenRouterModelClient(model_name, sys_prompt)
        elif is_qwen_model(model_name):
            return QwenModelClient(model_name, sys_prompt)
        elif is_phi_model(model_name):
            return HuggingFaceModelClient(model_name, sys_prompt)
        else:
            return HuggingFaceModelClient(model_name, sys_prompt)
    else:
        raise ValueError(f"Unknown provider: {provider}. Supported: openai, hf, qwen, openrouter")


def get_shared_relevance_client(base_client, relevance_prompt: str):
    """Create a shared model client for relevance filtering."""
    return SharedModelClient(base_client, relevance_prompt)


# =============================================================================
# FILE 4: src/output_formatter.py (REPLACE EXISTING CONTENT)
# =============================================================================

import os
import re
import json
import logging
import datetime
from typing import Dict, List, Any

from config import get_clean_model_name

logger = logging.getLogger(__name__)


def extract_policy_id(file_path: str) -> str:
    """
    Extract policy ID from PDF or TXT filename.
    Examples:
    - "10_nobis_policy.pdf" -> "10"
    - "18_medical_coverage.txt" -> "18"
    - "10-1_AXA 20220316 DIP AGGIUNTIVO ALI@TOP.pdf" -> "10-1"
    """
    filename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(filename)[0]

    # Extract policy ID from the filename using regex
    match = re.match(r'^([\d-]+)_', name_without_ext)
    if match:
        return match.group(1)
    else:
        logger.warning(f"Could not extract policy ID from filename: {filename}")
        return name_without_ext


def format_results_as_json(policy_path: str, question_results: List[List[str]]) -> Dict[str, Any]:
    """
    Format question results for a single policy as a JSON object.

    Args:
        policy_path: Path to the policy PDF
        question_results: List of question results in the format:
                         [model_name, q_id, question, eligibility, eligibility_policy, amount_policy]

    Returns:
        A JSON-serializable dictionary for the policy
    """
    policy_id = extract_policy_id(policy_path)

    # Initialize the policy JSON structure
    policy_json = {
        "policy_id": policy_id,
        "questions": []
    }

    # Add each question result to the policy JSON
    for result in question_results:
        _, q_id, question, eligibility, eligibility_policy, amount_policy = result

        question_json = {
            "request_id": q_id,
            "question": question,
            "outcome": eligibility,
            "outcome_justification": eligibility_policy,
            "payment_justification": amount_policy,
        }

        policy_json["questions"].append(question_json)

    return policy_json


def create_model_specific_output_dir(base_output_dir: str, model_name: str) -> str:
    """
    Create a model-specific output directory.

    Args:
        base_output_dir: Base output directory
        model_name: Full model name (e.g., "microsoft/phi-4", "qwen/qwen-2.5-72b-instruct")

    Returns:
        Path to the model-specific directory
    """
    clean_model_name = get_clean_model_name(model_name)
    model_output_dir = os.path.join(base_output_dir, clean_model_name)
    os.makedirs(model_output_dir, exist_ok=True)
    logger.info(f"Created model-specific output directory: {model_output_dir}")
    return model_output_dir


def save_policy_json(policy_json: Dict[str, Any], output_dir: str, model_name: str) -> str:
    """
    Save the policy JSON to a model-specific file.

    Args:
        policy_json: JSON-serializable dictionary
        output_dir: Base output directory
        model_name: Model name for creating subdirectory

    Returns:
        Path to the saved file
    """
    # Create model-specific output directory
    model_output_dir = create_model_specific_output_dir(output_dir, model_name)

    timestamp = datetime.datetime.now().strftime("%d-%m-%Y_%H-%M-%S")
    policy_id = policy_json["policy_id"]

    # Include clean model name in filename for clarity
    clean_model_name = get_clean_model_name(model_name)
    output_filename = f"policy_id-{policy_id}__{clean_model_name}__{timestamp}.json"
    output_path = os.path.join(model_output_dir, output_filename)

    # Save JSON to file
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(policy_json, f, indent=2, ensure_ascii=False)

    logger.info(f"Saved policy JSON to {output_path}")
    return output_path


def process_policy_results(policy_paths: List[str], all_results: List[List[str]],
                           output_dir: str, model_name: str) -> List[str]:
    """
    Process all results and create a JSON file for each policy in model-specific directory.

    Args:
        policy_paths: List of paths to policy PDFs
        all_results: List of all question results
        output_dir: Base output directory
        model_name: Model name for organizing outputs

    Returns:
        List of paths to the saved JSON files
    """
    saved_files = []

    # Convert to absolute path if needed
    if not os.path.isabs(output_dir):
        output_dir = os.path.abspath(output_dir)

    # Group results by policy ID
    for policy_path in policy_paths:
        policy_id = extract_policy_id(policy_path)
        logger.info(f"Processing results for policy ID: {policy_id}")

        # Filter results for this policy (you may need to modify this based on your data structure)
        policy_results = all_results

        # Format and save policy JSON
        policy_json = format_results_as_json(policy_path, policy_results)
        saved_path = save_policy_json(policy_json, output_dir, model_name)
        saved_files.append(saved_path)

    return saved_files


# End of output_formatter.py
# ================================================================================

# File 28/41: preprocessing_pdf.py
# --------------------------------------------------------------------------------

import os.path
import fitz  # PyMuPDF
import camelot
import json
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import pandas as pd
from enum import Enum

from config import DOCUMENT_DIR

try:
    from pymupdf4llm import to_markdown

    PYMUPDF4LLM_AVAILABLE = True
except ImportError:
    PYMUPDF4LLM_AVAILABLE = False
    logging.warning("PyMuPDF4LLM not available. Using basic text extraction.")


class DocumentType(Enum):
    """Supported document types."""
    INSURANCE_POLICY = "insurance_policy"
    CONTRACT = "contract"
    LEGAL_DOCUMENT = "legal_document"
    TECHNICAL_MANUAL = "technical_manual"
    GENERIC = "generic"


@dataclass
class DocumentChunk:
    """A structured chunk of document content for RAG."""
    chunk_id: str
    chunk_type: str
    title: str
    content: str
    page_number: int
    section_path: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    tables: List[Dict[str, Any]] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'chunk_id': self.chunk_id,
            'chunk_type': self.chunk_type,
            'title': self.title,
            'content': self.content,
            'page_number': self.page_number,
            'section_path': self.section_path,
            'metadata': self.metadata,
            'tables': self.tables
        }

    def to_rag_dict(self) -> Dict[str, Any]:
        """Convert to RAG-optimized dictionary."""
        return {
            'chunk_id': self.chunk_id,
            'title': self.title,
            'content': self.content,
            'page_number': self.page_number,
            'section_path': self.section_path,
            'chunk_type': self.chunk_type,
            'language': self.metadata.get('language', 'unknown'),
            'content_type': self.metadata.get('content_type', 'general'),
            'semantic_type': self.metadata.get('semantic_type', 'general'),
            'word_count': self.metadata.get('word_count', 0),
            'has_tables': len(self.tables) > 0,
            'tables': self.tables,
            'quality_score': self.metadata.get('quality_score', 0.0),
            'context_preserved': self.metadata.get('context_preserved', False)
        }


@dataclass
class CompanyProfile:
    """Enhanced company-specific document structure profile."""
    name: str
    document_types: List[DocumentType]
    section_patterns: Dict[int, List[str]]
    table_indicators: List[str] = field(default_factory=list)
    header_patterns: List[str] = field(default_factory=list)
    footer_patterns: List[str] = field(default_factory=list)
    language_hints: List[str] = field(default_factory=list)
    special_formatting: Dict[str, Any] = field(default_factory=dict)
    ignore_patterns: List[str] = field(default_factory=list)
    question_patterns: List[str] = field(default_factory=list)
    semantic_indicators: Dict[str, List[str]] = field(default_factory=dict)
    content_type_patterns: Dict[str, List[str]] = field(default_factory=dict)


@dataclass
class ProcessingConfig:
    """Enhanced configuration for PDF processing pipeline."""
    table_accuracy_threshold: float = 75.0
    preserve_structure: bool = True
    extract_metadata: bool = True
    debug_mode: bool = False

    # Enhanced RAG-optimized settings
    max_chunk_size: int = 1500  # Increased for better context
    min_chunk_size: int = 150  # Increased minimum for better quality
    optimal_chunk_size: int = 1000  # Sweet spot for RAG

    # Context preservation settings
    overlap_size: int = 200  # Character overlap between chunks
    preserve_semantic_units: bool = True
    maintain_context: bool = True

    # Quality settings
    min_quality_score: float = 0.6
    require_complete_sentences: bool = True
    preserve_lists: bool = True
    preserve_qa_pairs: bool = True

    # Document processing
    company_profile: Optional[CompanyProfile] = None
    document_type: DocumentType = DocumentType.GENERIC

    # Enhanced language settings
    primary_language: Optional[str] = None
    auto_detect_language: bool = True
    supported_languages: List[str] = field(default_factory=lambda: [
        'english', 'italian', 'french', 'german', 'spanish', 'portuguese'
    ])

    # Advanced processing
    custom_section_patterns: List[Tuple[int, str]] = field(default_factory=list)
    ignore_patterns: List[str] = field(default_factory=list)
    table_detection_strategy: str = "adaptive"

    # Output formatting
    include_page_numbers: bool = True
    normalize_text: bool = True
    enhance_readability: bool = True


class EnhancedCompanyProfileManager:
    """Enhanced profile manager with improved multi-language support and semantic understanding."""

    def __init__(self):
        self.profiles = self._initialize_enhanced_profiles()

    def _initialize_enhanced_profiles(self) -> Dict[str, CompanyProfile]:
        profiles = {}

        # Enhanced English Insurance Profile
        profiles["english_insurance"] = CompanyProfile(
            name="English Insurance Enhanced",
            document_types=[DocumentType.INSURANCE_POLICY],
            section_patterns={
                # Question-style headers (HIGHEST PRIORITY)
                0: [
                    r'^(What\s+is\s+(?:insured|covered))\?\s*(.*)$',
                    r'^(What\s+is\s+NOT\s+(?:insured|covered))\?\s*(.*)$',
                    r'^(Are\s+there\s+(?:coverage\s+)?limits)\?\s*(.*)$',
                    r'^(What\s+are\s+my\s+obligations)\?\s*(.*)$',
                    r'^(What\s+are\s+the\s+(?:insurer\'?s\s+)?obligations)\?\s*(.*)$',
                    r'^(What\s+to\s+do\s+in\s+case\s+of\s+(?:a\s+)?claim)\?\s*(.*)$',
                    r'^(When\s+and\s+how\s+do\s+I\s+pay)\?\s*(.*)$',
                    r'^(When\s+does\s+(?:the\s+)?cover(?:age)?\s+start\s+and\s+end)\?\s*(.*)$',
                    r'^(How\s+can\s+I\s+cancel\s+(?:the\s+)?policy)\?\s*(.*)$',
                    r'^(Who\s+is\s+this\s+product\s+for)\?\s*(.*)$',
                    r'^(What\s+costs\s+do\s+I\s+have\s+to\s+bear)\?\s*(.*)$',
                    r'^(HOW\s+CAN\s+I\s+COMPLAIN.*RESOLVE.*DISPUTES)\?\s*(.*)$',
                ],
                1: [
                    # Main sections
                    r'^SECTION\s+([A-Z\d]+(?:\s+[IV]+)?)\s*[-–—:]*\s*(.*)$',
                    r'^PART\s+([A-Z\d]+(?:\s+[IV]+)?)\s*[-–—:]*\s*(.*)$',
                    r'^CHAPTER\s+([A-Z\d]+(?:\s+[IV]+)?)\s*[-–—:]*\s*(.*)$',
                    r'^([A-Z][A-Z\s]*[A-Z])\s*$',  # All caps titles
                ],
                2: [
                    # Articles and subsections
                    r'^ARTICLE\s+(\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                    r'^ART\.\s*(\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                    r'^Art\.\s*(\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                    r'^(\d+\.\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                ],
                3: [
                    # Sub-items
                    r'^([a-z])\)\s*(.+)$',
                    r'^([A-Z])\.\s*(.+)$',
                    r'^([A-Z]\d+)\s*[-–—:]*\s*(.+)$',
                    r'^([A-Z]\.\d+)\s*[-–—:]*\s*(.+)$',
                ],
            },
            question_patterns=[
                r'What\s+is\s+(?:insured|covered)\?',
                r'What\s+is\s+NOT\s+(?:insured|covered)\?',
                r'Are\s+there\s+(?:coverage\s+)?limits\?',
                r'What\s+are\s+my\s+obligations\?',
                r'What\s+are\s+the\s+(?:insurer\'?s\s+)?obligations\?',
                r'What\s+to\s+do\s+in\s+case\s+of\s+(?:a\s+)?claim\?',
                r'When\s+and\s+how\s+do\s+I\s+pay\?',
                r'When\s+does\s+(?:the\s+)?cover(?:age)?\s+start\s+and\s+end\?',
                r'How\s+can\s+I\s+cancel\s+(?:the\s+)?policy\?',
                r'Who\s+is\s+this\s+product\s+for\?',
                r'What\s+costs\s+do\s+I\s+have\s+to\s+bear\?',
                r'HOW\s+CAN\s+I\s+COMPLAIN.*RESOLVE.*DISPUTES\?',
            ],
            table_indicators=['COVERAGE', 'LIMITS', 'PREMIUMS', 'CONDITIONS', 'OPTIONS', 'BENEFITS'],
            semantic_indicators={
                'coverage': ['insured', 'covered', 'protection', 'benefit', 'guarantee'],
                'exclusions': ['excluded', 'not covered', 'limitations', 'restrictions'],
                'obligations': ['must', 'shall', 'required', 'obligation', 'duty'],
                'procedures': ['procedure', 'process', 'steps', 'how to', 'method'],
                'definitions': ['means', 'definition', 'refers to', 'defined as', 'shall mean']
            },
            content_type_patterns={
                'coverage': [r'cover(?:age|ed|s)', r'insur(?:ed|ance)', r'protect(?:ion|ed)', r'benefit'],
                'exclusions': [r'exclud(?:ed|es|ing)', r'not\s+cover(?:ed|age)', r'limitation', r'restriction'],
                'claims': [r'claim', r'loss', r'damage', r'incident', r'accident'],
                'premium': [r'premium', r'payment', r'cost', r'fee', r'charge'],
                'assistance': [r'assistance', r'help', r'support', r'service', r'aid']
            },
            header_patterns=[
                r'--- Page \d+ ---',
                r'INSURANCE CONTRACT',
                r'NOBIS GROUP',
                r'POLICY CONDITIONS'
            ],
            ignore_patterns=[
                r'^Last update:.*',
                r'^Nobis Compagnia di Assicurazioni S\.p\.A\.',
                r'^The Legal Representative.*',
                r'^dr\. Giorgio Introvigne.*',
                r'^Conditions Assicurazioni Filo diretto Travel.*',
                r'^Model \d+.*edition.*',
                r'^\s*$',
                r'^Page \d+ of \d+$',
                r'^www\.',
                r'^.*@.*\..*$',  # Email addresses
                r'^\d{2}\.\d{2}\.\d{4}$',  # Dates
            ],
            language_hints=['english', 'insurance', 'policy', 'coverage', 'section', 'article', 'what is']
        )

        # Enhanced Italian Insurance Profile
        profiles["italian_insurance"] = CompanyProfile(
            name="Italian Insurance Enhanced",
            document_types=[DocumentType.INSURANCE_POLICY],
            section_patterns={
                # Question-style headers (HIGHEST PRIORITY) - Enhanced patterns
                0: [
                    r'^(Che\s+cosa\s+(?:è|sono)\s*assicurat[oi])\?\s*(.*)$',
                    r'^(Che\s+cosa\s+NON\s+(?:è|sono)\s*assicurat[oi])\?\s*(.*)$',
                    r'^(Ci\s+sono\s+limiti\s+di\s+copertura)\?\s*(.*)$',
                    r'^(Che\s+obblighi\s+ho)\?\s*(.*)$',
                    r"^(Quali\s+obblighi\s+ha\s+l['’]impresa)\?\s*(.*)$",
                    r'^(Cosa\s+fare\s+in\s+caso\s+di\s+sinistro)\?\s*(.*)$',
                    r'^(Quando\s+e\s+come\s+devo\s+pagare)\?\s*(.*)$',
                    r'^(Quando\s+comincia\s+la\s+copertura\s+e\s+quando\s+finisce)\?\s*(.*)$',
                    r'^(Come\s+posso\s+disdire\s+la\s+polizza)\?\s*(.*)$',
                    r'^(A\s+chi\s+è\s+rivolto\s+questo\s+prodotto)\?\s*(.*)$',
                    r'^(Quali\s+costi\s+devo\s+sostenere)\?\s*(.*)$',
                    r'^(COME\s+POSSO\s+PRESENTARE.*RECLAMI.*CONTROVERSIE)\?\s*(.*)$',
                        # Additional Italian question patterns
                    r'^(Quando\s+comincia.*quando\s+finisce)\?\s*(.*)$',
                    r'^(Come\s+posso\s+disdire)\?\s*(.*)$',
                ],
            1: [
                # Main sections - Enhanced patterns
                r'^SEZIONE\s+([A-Z\d]+(?:\s+[IV]+)?)\s*[-–—:]*\s*(.*)$',
                r'^PARTE\s+([A-Z\d]+(?:\s+[IV]+)?)\s*[-–—:]*\s*(.*)$',
                r'^CAPITOLO\s+([A-Z\d]+(?:\s+[IV]+)?)\s*[-–—:]*\s*(.*)$',
                r'^GARANZIE?\s+([A-Z\d\s]+)\s*$',
                r'^([A-Z][A-ZÀÈÉÌÒÙ\s]*[A-ZÀÈÉÌÒÙ])\s*$',  # All caps with Italian accents
                # Specific Italian insurance sections
                r'^(ASSISTENZA\s+IN\s+VIAGGIO)\s*(.*)$',
                r'^(SPESE\s+MEDICHE)\s*(.*)$',
                r'^(BAGAGLIO)\s*(.*)$',
                r'^(ANNULLAMENTO\s+VIAGGIO)\s*(.*)$',
            ],
            2: [
                # Articles and subsections - Enhanced
                r'^ARTICOLO\s+(\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                r'^ART\.\s*(\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                r'^Art\.\s*(\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                r'^(\d+\.\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                # Italian-specific patterns
                r'^([A-Z]\d*)\s*[-–—:]*\s*(.+)$',  # Like "B1", "C.1"
            ],
            3: [
                # Sub-items - Enhanced
                r'^([a-z])\)\s*(.+)$',
                r'^([A-Z])\.\s*(.+)$',
                r'^(\d+\.\d+(?:\.\d+)*)\s*[-–—:]*\s*(.+)$',
                r'^([A-Z]\d+)\s*[-–—:]*\s*(.+)$',
                r'^([A-Z]\.\d+)\s*[-–—:]*\s*(.+)$',
                # Lettered sub-items common in Italian docs
                r'^([a-z])\s*[-–—:]\s*(.+)$',
            ],
        },
        question_patterns = [
            r'Che\s+cosa\s+(?:è|sono)\s*assicurat[oi]\?',
            r'Che\s+cosa\s+NON\s+(?:è|sono)\s*assicurat[oi]\?',
            r'Ci\s+sono\s+limiti\s+di\s+copertura\?',
            r'Che\s+obblighi\s+ho\?',
            r"Quali\s+obblighi\s+ha\s+l['’]impresa\?",
            r'Cosa\s+fare\s+in\s+caso\s+di\s+sinistro\?',
            r'Quando\s+e\s+come\s+devo\s+pagare\?',
            r'Quando\s+comincia.*copertura.*quando\s+finisce\?',
            r'Come\s+posso\s+disdire.*polizza\?',
            r'A\s+chi\s+è\s+rivolto\s+questo\s+prodotto\?',
            r'Quali\s+costi\s+devo\s+sostenere\?',
            r'COME\s+POSSO\s+PRESENTARE.*RECLAMI\?',
        ],
        table_indicators = ['COPERTURA', 'LIMITI', 'PREMI', 'CONDIZIONI', 'OPZIONI', 'GARANZIE', 'MASSIMALI'],
        semantic_indicators = {
            'copertura': ['assicurato', 'coperto', 'protezione', 'garanzia', 'beneficio'],
            'esclusioni': ['escluso', 'non coperto', 'limitazioni', 'restrizioni', 'non assicurato'],
            'obblighi': ['deve', 'dovrà', 'obbligo', 'dovere', 'tenuto'],
            'procedure': ['procedura', 'processo', 'modalità', 'come fare', 'metodo'],
            'definizioni': ['significa', 'definizione', 'si intende', 'definito come', 'comprende']
        },
        content_type_patterns = {
            'copertura': [r'copertur[ao]', r'assicurat[oi]', r'garanzi[ao]', r'protezion[ei]', r'benefici[oi]'],
            'esclusioni': [r'esclus[oi]', r'non\s+(?:coperto|assicurato)', r'limitazion[ei]', r'restrizion[ei]'],
            'sinistri': [r'sinistro', r'danno', r'perdita', r'incidente', r'evento'],
            'premio': [r'premio', r'pagamento', r'costo', r'tariffa', r'importo'],
            'assistenza': [r'assistenza', r'aiuto', r'supporto', r'servizio', r'soccorso']
        },
        header_patterns = [
            r'Pag\.\s*\d+\s*di\s*\d+',
            r'--- Page \d+ ---',
            r'CONTRATTO ASSICURATIVO',
            r'POLIZZA',
            r'CONDIZIONI DI ASSICURAZIONE'
        ],
        ignore_patterns = [
            r'^Ultimo aggiornamento:.*',
            r'^Inter Partner Assistance S\.A\..*',
            r'^AXA.*Assistance.*',
            r'^Nobis.*',
            r'^Il Rappresentante Legale.*',
            r'^Condizioni.*',
            r'^Modello \d+.*edizione.*',
            r'^www\.',
            r'^\s*$',
            r'^Pag\.\s*\d+\s*di\s*\d+\s*$',
            r'^.*@.*\..*$',  # Email addresses
            r'^\d{2}\.\d{2}\.\d{4}$',  # Dates
            r'^Via\s+.*\d+.*$',  # Addresses
            r'^Codice\s+Fiscale.*$',
            r'^Partita\s+IVA.*$',
        ],
        language_hints = ['italian', 'assicurazione', 'polizza', 'garanzia', 'sezione', 'articolo', 'che cosa',
                          'società']
        )

        return profiles

    def get_profile(self, profile_name: str) -> Optional[CompanyProfile]:
        """Get a specific company profile."""
        return self.profiles.get(profile_name.lower())

    def detect_best_profile(self, text_sample: str) -> str:
        """Enhanced auto-detection with better language recognition."""
        text_lower = text_sample.lower()

        italian_score = 0
        english_score = 0

        # Enhanced Italian indicators with weights
        italian_indicators = {
            'che cosa': 15, 'assicurato': 10, 'polizza': 10, 'garanzia': 8, 'sezione': 5,
            'articolo': 5, 'copertura': 8, 'sinistro': 8, 'premio': 5, 'contratto': 5,
            'viaggio': 3, 'bagaglio': 3, 'spese mediche': 10, 'assistenza': 5,
            'rimborso': 5, 'indennizzo': 5, 'esclusioni': 8, 'società': 3,
            'dell\'assicurato': 12, 'centrale operativa': 10, 'quando comincia': 8,
            'come posso': 8, 'quali obblighi': 8, 'cosa fare': 5
        }

        # Enhanced English indicators with weights
        english_indicators = {
            'what is': 15, 'insured': 10, 'policy': 10, 'coverage': 8, 'section': 5,
            'article': 5, 'insurance': 10, 'contract': 5, 'travel': 3, 'baggage': 3,
            'medical expenses': 10, 'assistance': 5, 'reimbursement': 5, 'claims': 8,
            'exclusions': 8, 'company': 3, 'insured person': 12, 'when does': 8,
            'how can': 8, 'what are my': 8, 'what to do': 5, 'limits': 5
        }

        # Count weighted matches
        for indicator, weight in italian_indicators.items():
            if indicator in text_lower:
                italian_score += text_lower.count(indicator) * weight

        for indicator, weight in english_indicators.items():
            if indicator in text_lower:
                english_score += text_lower.count(indicator) * weight

        # Specific pattern bonuses
        if re.search(r'che cosa (?:è|sono)\s*assicurat[oi]\?', text_lower):
            italian_score += 25
        if re.search(r'what is (?:insured|covered)\?', text_lower):
            english_score += 25

        # Company-specific patterns with higher weights
        if any(pattern in text_lower for pattern in ['axa', 'inter partner assistance', 'centrale operativa']):
            italian_score += 15
        if any(pattern in text_lower for pattern in ['nobis', 'filo diretto']):
            english_score += 10

        # Language-specific character patterns
        italian_chars = (text_lower.count('à') + text_lower.count('è') + text_lower.count('é') +
                         text_lower.count('ì') + text_lower.count('ò') + text_lower.count('ù'))
        italian_score += italian_chars * 5

        # Document structure patterns
        if re.search(r'pag\.\s*\d+\s*di\s*\d+', text_lower):
            italian_score += 10
        if re.search(r'page \d+ of \d+', text_lower):
            english_score += 10

        # Final decision with logging
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Enhanced language detection scores - Italian: {italian_score}, English: {english_score}")

        if italian_score > english_score:
            return 'italian_insurance'
        elif english_score > italian_score:
            return 'english_insurance'
        else:
            # Enhanced tie-breaking
            if any(word in text_lower for word in ['assicurato', 'polizza', 'società', 'garanzia']):
                return 'italian_insurance'
            return 'italian_insurance'  # Default to Italian for insurance docs

    def list_profiles(self) -> List[str]:
        """List available profiles."""
        return list(self.profiles.keys())

class EnhancedAdvancedPDFPreprocessor:
    """Production-ready PDF preprocessing pipeline with enhanced chunk quality."""

    # Enhanced character normalization map
    NORMALIZATION_MAP = {
        "\u200b": "", "\ufeff": "", "\xad": "", "\u00a0": " ",
        "\u2009": " ", "\u202f": " ", "\u2013": "-", "\u2014": "--",
        "\u2018": "'", "\u2019": "'", "\u201c": '"', "\u201d": '"',
        "\u2022": "•", "\u2026": "...", "\u20ac": "€",
        # Preserve Italian accented characters properly
    }

    def __init__(self, config: Optional[ProcessingConfig] = None):
        self.config = config or ProcessingConfig()
        self.profile_manager = EnhancedCompanyProfileManager()
        self.logger = self._setup_logging()

    def _setup_logging(self) -> logging.Logger:
        """Setup comprehensive logging."""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG if self.config.debug_mode else logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def clean_text(self, text: str) -> str:
        """Enhanced text cleaning with better Italian support and readability."""
        if not text:
            return ""

        if not self.config.normalize_text:
            return text.strip()

        # Remove control characters except newlines and tabs
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)

        # Normalize special characters (but preserve Italian accents)
        for char, replacement in self.NORMALIZATION_MAP.items():
            text = text.replace(char, replacement)

        # Fix broken hyphenation while preserving intentional hyphens
        text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)

        # Enhanced whitespace normalization
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)

        # Clean punctuation spacing
        text = re.sub(r'\s+([.,;:!?])', r'\1', text)
        text = re.sub(r'([.,;:!?])\s*\n', r'\1\n', text)

        # Enhanced Italian text fixes
        text = re.sub(r'è\s+assicurat', 'è assicurat', text)
        text = re.sub(r'à\s+([aeiou])', r'à\1', text)
        text = re.sub(r'ò\s+([aeiou])', r'ò\1', text)

        # Fix common formatting issues
        text = re.sub(r'(\w)\s*\n\s*(\w)', r'\1 \2', text)  # Fix broken words across lines
        text = re.sub(r'\n\s*([.,:;])', r'\1', text)  # Fix punctuation on new lines

        # Enhanced readability improvements
        if self.config.enhance_readability:
            # Ensure proper spacing after punctuation
            text = re.sub(r'([.!?])\s*([A-ZÀÈÉÌÒÙ])', r'\1 \2', text)
            # Fix number formatting
            text = re.sub(r'(\d)\s+([.,])\s*(\d)', r'\1\2\3', text)

        return text.strip()

    def extract_with_pymupdf4llm(self, doc: fitz.Document) -> str:
        """Extract text with enhanced fallback strategy."""
        try:
            if PYMUPDF4LLM_AVAILABLE:
                markdown_content = to_markdown(doc)
                return self.clean_text(markdown_content)
            else:
                return self._enhanced_fallback_extraction(doc)
        except Exception as e:
            self.logger.warning(f"Primary extraction failed: {e}")
            return self._enhanced_fallback_extraction(doc)

    def _enhanced_fallback_extraction(self, doc: fitz.Document) -> str:
        """Enhanced fallback text extraction with better page separation."""
        text_blocks = []
        for page_num in range(doc.page_count):
            page = doc[page_num]

            # Try different text extraction methods in order of preference
            text_methods = [
                # Method 1: Preserve ligatures and whitespace
                lambda: page.get_text(flags=fitz.TEXT_PRESERVE_LIGATURES | fitz.TEXT_PRESERVE_WHITESPACE),
                # Method 2: Standard text extraction
                lambda: page.get_text("text"),
                # Method 3: Block-based extraction
                lambda: page.get_text("blocks"),
                # Method 4: Dict-based extraction (most detailed)
                lambda: page.get_text("dict")
            ]

            extracted_text = ""
            for method_idx, method in enumerate(text_methods):
                try:
                    result = method()

                    if isinstance(result, list):  # blocks method
                        result = "\n".join(
                            [block[4] if len(block) > 4 else str(block) for block in result if block])
                    elif isinstance(result, dict):  # dict method
                        result = self._extract_text_from_dict(result)

                    if result and result.strip():
                        extracted_text = result
                        self.logger.debug(f"Page {page_num + 1}: Used extraction method {method_idx + 1}")
                        break

                except Exception as e:
                    self.logger.debug(f"Extraction method {method_idx + 1} failed on page {page_num + 1}: {e}")
                    continue

            if extracted_text.strip():
                if self.config.include_page_numbers:
                    text_blocks.append(f"--- Page {page_num + 1} ---\n{extracted_text}")
                else:
                    text_blocks.append(extracted_text)

        return self.clean_text("\n\n".join(text_blocks))

    def _extract_text_from_dict(self, text_dict: dict) -> str:
        """Extract text from PyMuPDF dict format while preserving structure."""
        text_parts = []

        if 'blocks' in text_dict:
            for block in text_dict['blocks']:
                if 'lines' in block:
                    for line in block['lines']:
                        if 'spans' in line:
                            line_text = ''
                            for span in line['spans']:
                                if 'text' in span:
                                    line_text += span['text']
                            if line_text.strip():
                                text_parts.append(line_text)

        return '\n'.join(text_parts)

    def _enhanced_fix_concatenated_markers(self, text: str) -> str:
        """Enhanced fix for concatenated page markers with better pattern recognition."""

        # Enhanced patterns for both languages
        patterns_to_fix = [
            # English patterns
            (r'(--- Page \d+ ---)\s*([A-Z])', r'\1\n\2'),
            (r'(--- Page \d+ ---)\s*(SECTION\s+[A-Z\d]+)', r'\1\n\2'),
            (r'(--- Page \d+ ---)\s*(ARTICLE\s+\d+)', r'\1\n\2'),
            (r'(--- Page \d+ ---)\s*(What\s+)', r'\1\n\2'),

            # Italian patterns - Enhanced
            (r'(Pag\.\s*\d+\s*di\s*\d+)\s*([A-ZÀÈÉÌÒÙ][a-zàèéìòù])', r'\1\n\2'),
            (r'(Pag\.\s*\d+\s*di\s*\d+)\s*(Che\s+cosa)', r'\1\n\2'),
            (r'(--- Page \d+ ---)\s*(Che\s+cosa)', r'\1\n\2'),
            (r'(\d+\s*di\s*\d+)\s*([A-ZÀÈÉÌÒÙ][a-zàèéìòù])', r'\1\n\2'),
            (r'(Pag\.\s*\d+\s*di\s*\d+)\s*(SEZIONE)', r'\1\n\2'),
            (r'(Pag\.\s*\d+\s*di\s*\d+)\s*(ARTICOLO)', r'\1\n\2'),

            # Fix spacing in question patterns (Italian)
            (r'Che\s+cosa\s+è\s*assicurat', 'Che cosa è assicurat'),
            (r'Che\s+cosa\s+NON\s+è\s*assicurat', 'Che cosa NON è assicurat'),
            (r'Ci\s+sono\s+limiti\s+di\s*copertura', 'Ci sono limiti di copertura'),

            # Fix spacing in question patterns (English)
            (r'What\s+is\s+insured', 'What is insured'),
            (r'What\s+is\s+NOT\s+insured', 'What is NOT insured'),
            (r'Are\s+there\s+coverage\s*limits', 'Are there coverage limits'),

            # General improvements
            (r'([.!?])\s*([A-ZÀÈÉÌÒÙ])', r'\1\n\2'),  # New sentence, new line
            (r'(\w)\s*\n\s*([a-zàèéìòù])', r'\1 \2'),  # Fix broken words
        ]

        for pattern, replacement in patterns_to_fix:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE | re.MULTILINE)

        # Ensure proper page markers
        text = re.sub(r'--- Page (\d+) --([^-])', r'--- Page \1 ---\n\2', text)
        text = re.sub(r'(Pag\.\s*\d+\s*di\s*\d+)([A-ZÀÈÉÌÒÙ])', r'\1\n\2', text)

        self.logger.debug("Enhanced concatenated marker fixes applied")
        return text

    def _enhanced_detect_structure_element(self, line: str) -> Tuple[int, str, str]:
        """Enhanced structure detection with better semantic understanding."""
        line = line.strip()

        if not line or not self.config.company_profile:
            return 0, "", line

        # Check all pattern levels in priority order
        for level in sorted(self.config.company_profile.section_patterns.keys()):
            patterns = self.config.company_profile.section_patterns[level]

            for pattern in patterns:
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    groups = match.groups()

                    if len(groups) >= 2:
                        section_id = groups[0].strip()
                        title_part = groups[1].strip() if groups[1] else ""
                    elif len(groups) == 1:
                        # For patterns that capture the entire title
                        if level == 0:  # Question patterns
                            section_id = ""
                            title_part = groups[0].strip()
                        else:
                            section_id = groups[0].strip()
                            title_part = ""
                    else:
                        section_id = ""
                        title_part = line.strip()

                    # Enhanced title cleaning
                    if title_part:
                        title_part = re.sub(r'^[-–—:\s]+', '', title_part)
                        title_part = re.sub(r'[-–—:\s]+$', '', title_part)
                        title_part = self.clean_text(title_part)

                    # Build enhanced full title
                    if level == 0:  # Question patterns - use as-is
                        full_title = title_part if title_part else line.strip()
                        # Ensure question mark is included
                        if not full_title.endswith('?') and '?' in line:
                            full_title += '?'
                    elif section_id and title_part:
                        full_title = f"{section_id} - {title_part}"
                    elif section_id:
                        full_title = section_id
                    elif title_part:
                        full_title = title_part
                    else:
                        full_title = line.strip()

                    # Adjust level for question patterns
                    actual_level = 1 if level == 0 else level

                    self.logger.info(
                        f"🎯 Enhanced structure detected: Level {actual_level}, Pattern: '{pattern[:50]}...', Title: '{full_title}'")
                    return actual_level, section_id, full_title

        return 0, "", line

    def _enhanced_should_ignore_line(self, line: str) -> bool:
        """Enhanced line filtering with better semantic understanding."""
        if not self.config.company_profile or not line.strip():
            return True

        line_clean = line.strip()

        # Check ignore patterns
        for pattern in self.config.company_profile.ignore_patterns:
            if re.match(pattern, line_clean, re.IGNORECASE):
                self.logger.debug(f"Ignoring line (pattern match): {line_clean[:50]}...")
                return True

        # Enhanced filters
        if len(line_clean) < 3:
            return True

        # Ignore pure numbers
        if re.match(r'^\d+\s*$', line_clean):
            return True

        # Ignore pure punctuation
        if re.match(r'^[^\w\s]*$', line_clean):
            return True

        # Ignore URLs and email addresses
        if re.match(r'^(?:https?://|www\.|.*@.*\..*)$', line_clean, re.IGNORECASE):
            return True

        # Ignore pure dates
        if re.match(r'^\d{1,2}[./]\d{1,2}[./]\d{2,4}$', line_clean):
            return True

        return False

    def create_enhanced_smart_chunks(self, text: str, tables_by_page: Dict[int, List[Dict]]) -> List[DocumentChunk]:
        """Enhanced smart chunking with guaranteed high-quality results."""
        self.logger.info("Starting enhanced smart chunking with quality optimization")

        # Apply enhanced preprocessing
        text = self._enhanced_fix_concatenated_markers(text)

        # Try structured chunking with enhanced quality
        structured_chunks = self._try_enhanced_structured_chunking(text, tables_by_page)

        if structured_chunks and len(structured_chunks) > 1:
            # Validate and enhance chunk quality
            quality_chunks = self._enhance_chunk_quality(structured_chunks)
            if quality_chunks:
                self.logger.info(f"✅ Successfully created {len(quality_chunks)} enhanced quality chunks")
                return quality_chunks

        # Enhanced fallback chunking
        self.logger.warning("Structure detection suboptimal, using enhanced intelligent fallback")
        fallback_chunks = self._create_enhanced_intelligent_fallback_chunks(text, tables_by_page)
        quality_chunks = self._enhance_chunk_quality(fallback_chunks)

        return quality_chunks if quality_chunks else fallback_chunks

    def _try_enhanced_structured_chunking(self, text: str, tables_by_page: Dict[int, List[Dict]]) -> List[
        DocumentChunk]:
        """Enhanced structured chunking with better semantic preservation."""
        try:
            hierarchical_structure = self._enhanced_parse_document_structure(text)

            if not hierarchical_structure:
                self.logger.warning("No hierarchical structure detected")
                return []

            chunks = self._create_enhanced_chunks_from_structure(hierarchical_structure, tables_by_page)
            valid_chunks = [chunk for chunk in chunks if self._validate_enhanced_chunk(chunk)]

            if len(valid_chunks) < len(chunks):
                self.logger.warning(f"Enhanced validation filtered out {len(chunks) - len(valid_chunks)} chunks")

            return valid_chunks

        except Exception as e:
            self.logger.error(f"Enhanced structured chunking failed: {e}")
            return []

    def _enhanced_parse_document_structure(self, text: str) -> List[Dict[str, Any]]:
        """Enhanced document structure parsing with better semantic understanding."""
        try:
            self.logger.info("Starting enhanced document structure parsing")
            structural_elements = self._enhanced_parse_text_with_page_tracking(text)

            self.logger.info(f"Identified {len(structural_elements)} structural elements")

            structure_count = sum(1 for e in structural_elements if e['type'] == 'structure')
            content_count = sum(1 for e in structural_elements if e['type'] == 'content')
            self.logger.info(f"Structure elements: {structure_count}, Content elements: {content_count}")

            if structure_count == 0:
                self.logger.warning("No structural elements found")
                return []

            hierarchical_structure = self._build_enhanced_hierarchy(structural_elements)
            self.logger.info(
                f"Built enhanced hierarchical structure with {len(hierarchical_structure)} top-level sections")

            return hierarchical_structure

        except Exception as e:
            self.logger.error(f"Enhanced document structure parsing failed: {e}")
            return []

    def _enhanced_parse_text_with_page_tracking(self, text: str) -> List[Dict[str, Any]]:
        """Enhanced text parsing with better semantic understanding and page tracking."""
        elements = []
        current_page = 1
        current_content = []

        if not text or not isinstance(text, str):
            self.logger.warning("Invalid text input")
            return []

        lines = text.split('\n')
        self.logger.debug(f"Processing {len(lines)} lines")

        i = 0
        while i < len(lines):
            original_line = lines[i]
            line = original_line.strip()

            if not line:
                if current_content:
                    current_content.append("")
                i += 1
                continue

            # Check for page markers
            page_patterns = [
                r'^---\s*Page\s+(\d+)\s*---\s*$',  # English
                r'^Pag\.\s*(\d+)\s*di\s*\d+\s*$',  # Italian
            ]

            page_found = False
            for pattern in page_patterns:
                page_match = re.match(pattern, line)
                if page_match:
                    new_page = int(page_match.group(1))
                    if new_page != current_page:
                        current_page = new_page
                        self.logger.debug(f"📄 Found page marker: Page {current_page}")
                    page_found = True
                    break

            if page_found:
                i += 1
                continue

            # Skip ignored patterns
            if self._enhanced_should_ignore_line(line):
                i += 1
                continue

            # Detect structural elements
            level, section_id, title = self._enhanced_detect_structure_element(line)

            if level > 0:
                self.logger.info(f"🎯 Structure detected on page {current_page}: Level {level}, '{line}'")

                # Save previous content if substantial
                if current_content:
                    content_text = '\n'.join(current_content).strip()
                    if content_text and len(content_text) >= self.config.min_chunk_size:
                        elements.append({
                            'type': 'content',
                            'level': 999,
                            'title': 'Content Block',
                            'content': content_text,
                            'page': current_page,
                            'section_id': '',
                            'full_line': '',
                            'semantic_type': self._detect_content_semantic_type(content_text)
                        })
                    current_content = []

                # Look ahead for continuation content
                continuation_content = []
                j = i + 1
                while j < len(lines) and j < i + 10:  # Look ahead max 10 lines
                    next_line = lines[j].strip()
                    if not next_line:
                        j += 1
                        continue

                    # If it's another structural element or page marker, stop
                    if (self._enhanced_detect_structure_element(next_line)[0] > 0 or
                            any(re.match(pattern, next_line) for pattern in page_patterns)):
                        break

                    # If it's not ignored, add to continuation
                    if not self._enhanced_should_ignore_line(next_line):
                        continuation_content.append(lines[j].rstrip())
                    j += 1

                # Add structural element with any immediate continuation
                structure_content = '\n'.join(continuation_content).strip() if continuation_content else ''

                elements.append({
                    'type': 'structure',
                    'level': level,
                    'title': title,
                    'content': structure_content,
                    'page': current_page,
                    'section_id': section_id,
                    'full_line': line,
                    'semantic_type': self._detect_content_semantic_type(title + ' ' + structure_content)
                })

                # Skip the processed continuation lines
                i = j

            else:
                # Accumulate content
                current_content.append(original_line.rstrip())
                i += 1

        # Save remaining content
        if current_content:
            content_text = '\n'.join(current_content).strip()
            if content_text and len(content_text) >= self.config.min_chunk_size:
                elements.append({
                    'type': 'content',
                    'level': 999,
                    'title': 'Content Block',
                    'content': content_text,
                    'page': current_page,
                    'section_id': '',
                    'full_line': '',
                    'semantic_type': self._detect_content_semantic_type(content_text)
                })

        return elements

    def _detect_content_semantic_type(self, content: str) -> str:
        """Detect semantic type of content for better categorization."""
        if not content:
            return 'general'

        content_lower = content.lower()

        # Use profile-specific patterns if available
        if self.config.company_profile and hasattr(self.config.company_profile, 'content_type_patterns'):
            for content_type, patterns in self.config.company_profile.content_type_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, content_lower, re.IGNORECASE):
                        return content_type

        # Fallback to general patterns
        if re.search(r'esclus|exclud', content_lower):
            return "exclusions"
        elif re.search(r'copertur|coverage|garanzia', content_lower):
            return "coverage"
        elif re.search(r'limite|limit', content_lower):
            return "limits"
        elif re.search(r'premio|premium|pagamento|payment', content_lower):
            return "premium"
        elif re.search(r'sinistro|claim|denuncia', content_lower):
            return "claims"
        elif re.search(r'assistenza|assistance', content_lower):
            return "assistance"
        elif re.search(r'rimborso|reimbursement', content_lower):
            return "reimbursement"
        elif re.search(r'definizione|definition|significa|means', content_lower):
            return "definitions"
        elif re.search(r'procedura|procedure|modalità|method', content_lower):
            return "procedures"
        else:
            return "general"

    def _build_enhanced_hierarchy(self, elements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Build enhanced hierarchical structure with better semantic organization."""
        hierarchy = []
        stack = []

        for element in elements:
            if element['type'] == 'structure':
                level = element['level']

                # Pop stack until we find the right parent level
                while stack and stack[-1]['level'] >= level:
                    popped = stack.pop()
                    self.logger.debug(f"Popped from stack: {popped['title']} (level {popped['level']})")

                # Create enhanced section
                section = {
                    'level': level,
                    'title': element['title'],
                    'section_id': element['section_id'],
                    'content': element['content'],
                    'page': element['page'],
                    'subsections': [],
                    'full_path': [],
                    'semantic_type': element.get('semantic_type', 'general'),
                    'quality_indicators': self._assess_section_quality(element)
                }

                # Build full path
                section['full_path'] = [s['title'] for s in stack] + [element['title']]

                # Add to parent or root
                if stack:
                    parent = stack[-1]
                    parent['subsections'].append(section)
                    self.logger.debug(f"Added '{element['title']}' as child of '{parent['title']}'")
                else:
                    hierarchy.append(section)
                    self.logger.debug(f"Added '{element['title']}' as root section")

                stack.append(section)

            elif element['type'] == 'content':
                if stack:
                    current_section = stack[-1]
                    if current_section['content']:
                        current_section['content'] += '\n\n' + element['content']
                    else:
                        current_section['content'] = element['content']

                    # Update semantic type if more specific
                    if element.get('semantic_type', 'general') != 'general':
                        current_section['semantic_type'] = element['semantic_type']

                    self.logger.debug(f"Added content to section '{current_section['title']}'")
                else:
                    # Create enhanced default section for orphaned content
                    section = {
                        'level': 1,
                        'title': 'Document Content',
                        'section_id': '',
                        'content': element['content'],
                        'page': element['page'],
                        'subsections': [],
                        'full_path': ['Document Content'],
                        'semantic_type': element.get('semantic_type', 'general'),
                        'quality_indicators': {'is_orphaned': True}
                    }
                    hierarchy.append(section)
                    stack.append(section)
                    self.logger.debug("Created enhanced default section for orphaned content")

        return hierarchy

    def _assess_section_quality(self, element: Dict[str, Any]) -> Dict[str, Any]:
        """Assess the quality of a section for better chunk creation."""
        indicators = {
            'has_title': bool(element.get('title', '').strip()),
            'title_length': len(element.get('title', '')),
            'has_content': bool(element.get('content', '').strip()),
            'content_length': len(element.get('content', '')),
            'has_section_id': bool(element.get('section_id', '').strip()),
            'is_question': element.get('title', '').endswith('?'),
            'semantic_type': element.get('semantic_type', 'general'),
            'page_number': element.get('page', 0)
        }

        # Calculate quality score
        score = 0.0
        if indicators['has_title']:
            score += 0.3
        if indicators['has_content']:
            score += 0.3
        if indicators['has_section_id']:
            score += 0.1
        if indicators['is_question']:
            score += 0.1
        if indicators['semantic_type'] != 'general':
            score += 0.1
        if 50 <= indicators['title_length'] <= 200:
            score += 0.1

        indicators['quality_score'] = score
        return indicators

    def _create_enhanced_chunks_from_structure(self, hierarchy: List[Dict[str, Any]],
                                               tables_by_page: Dict[int, List[Dict]]) -> List[DocumentChunk]:
        """Create enhanced chunks from hierarchical structure with better quality control."""
        chunks = []
        chunk_counter = 0

        def process_section_enhanced(section: Dict[str, Any], parent_path: List[str] = None) -> None:
            nonlocal chunk_counter

            parent_path = parent_path or []
            section_title = section['title']
            current_path = parent_path + [section_title]
            page_num = section.get('page', 1)

            # Enhanced content processing
            if section.get('content', '').strip():
                content = section['content'].strip()

                # Intelligent content splitting with semantic preservation
                content_chunks = self._enhanced_split_content_intelligently(content, section_title,
                                                                            section.get('semantic_type', 'general'))

                for i, chunk_content in enumerate(content_chunks):
                    if len(chunk_content.strip()) >= self.config.min_chunk_size:
                        title = section_title if len(content_chunks) == 1 else f"{section_title} (Part {i + 1})"

                        chunk = self._create_enhanced_quality_chunk(
                            chunk_counter, title, chunk_content, page_num,
                            current_path, tables_by_page, "enhanced_structured",
                            section.get('semantic_type', 'general'),
                            section.get('quality_indicators', {})
                        )
                        chunks.append(chunk)
                        chunk_counter += 1

            # Process subsections
            for subsection in section.get('subsections', []):
                process_section_enhanced(subsection, current_path)

        for section in hierarchy:
            process_section_enhanced(section)

        return chunks

    def _enhanced_split_content_intelligently(self, content: str, title: str, semantic_type: str) -> List[str]:
        """Enhanced content splitting that preserves semantic units and context."""

        # If content is already optimal size, return as-is
        if len(content) <= self.config.optimal_chunk_size:
            return [content]

        # Detect if this is a special semantic unit that shouldn't be split
        if self._is_protected_semantic_unit(content, semantic_type):
            # If it's too large but protected, try minimal splitting
            if len(content) > self.config.max_chunk_size:
                return self._minimal_split_protected_content(content)
            else:
                return [content]

        chunks = []

        # Step 1: Split by enhanced semantic boundaries
        semantic_blocks = self._split_by_enhanced_semantic_boundaries(content, semantic_type)

        current_chunk = ""

        for block in semantic_blocks:
            # Try to add entire semantic block
            test_chunk = (current_chunk + "\n\n" + block).strip() if current_chunk else block

            if len(test_chunk) <= self.config.max_chunk_size:
                current_chunk = test_chunk
            else:
                # Save current chunk if substantial
                if current_chunk and len(current_chunk.strip()) >= self.config.min_chunk_size:
                    chunks.append(current_chunk.strip())

                # Handle oversized block
                if len(block) > self.config.max_chunk_size:
                    block_chunks = self._safe_split_large_semantic_block(block, semantic_type)
                    chunks.extend(block_chunks[:-1])
                    current_chunk = block_chunks[-1] if block_chunks else ""
                else:
                    current_chunk = block

        # Add final chunk
        if current_chunk and len(current_chunk.strip()) >= self.config.min_chunk_size:
            chunks.append(current_chunk.strip())

        # Enhanced context preservation with semantic overlap
        return self._add_enhanced_semantic_overlap(chunks, semantic_type)

    def _is_protected_semantic_unit(self, content: str, semantic_type: str) -> bool:
        """Check if content represents a protected semantic unit that shouldn't be split."""

        # Lists with multiple items (preserve complete lists)
        if re.search(r'^\s*[•\-\*\d+\.\)]\s+.*(\n\s*[•\-\*\d+\.\)]\s+.*){2,}', content, re.MULTILINE):
            return True

        # Q&A pairs (preserve question-answer structure)
        if re.search(r'[^?]*\?[^\n]*\n[^\n?]*[^\?]\n', content):
            return True

        # Tables or structured data
        if content.count('|') >= 4 or content.count('\t') >= 4:
            return True

        # Definitions (based on semantic type and content patterns)
        if semantic_type == 'definitions' or re.search(
                r'(significa|significa che|definisce|definition|means|refers to)', content, re.IGNORECASE):
            return True

        # Procedures or step-by-step instructions
        if semantic_type == 'procedures' or re.search(r'(procedura|procedure|modalità|step|fase|point)', content,
                                                      re.IGNORECASE):
            return True

        # Legal clauses or conditions
        if re.search(r'(condizione|condition|clausola|clause|termine|term)', content, re.IGNORECASE):
            return True

        return False

    def _minimal_split_protected_content(self, content: str) -> List[str]:
        """Minimally split protected content while preserving semantic integrity."""

        # Try to find natural break points that don't break semantic units

        # Split at double line breaks (paragraph boundaries)
        paragraphs = re.split(r'\n\s*\n', content)
        if len(paragraphs) > 1:
            chunks = []
            current_chunk = ""

            for para in paragraphs:
                test_chunk = (current_chunk + "\n\n" + para).strip() if current_chunk else para

                if len(test_chunk) <= self.config.max_chunk_size:
                    current_chunk = test_chunk
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para

            if current_chunk:
                chunks.append(current_chunk.strip())

            return [chunk for chunk in chunks if len(chunk.strip()) >= self.config.min_chunk_size]

        # If no paragraph breaks, return as single chunk (better to have oversized than broken)
        return [content]

    def _split_by_enhanced_semantic_boundaries(self, content: str, semantic_type: str) -> List[str]:
        """Split content by enhanced semantic boundaries specific to content type."""

        # Enhanced patterns based on semantic type
        if semantic_type == 'exclusions':
            # For exclusions, split by numbered items or bullet points
            if re.search(r'^\s*\d+\.\s+', content, re.MULTILINE):
                blocks = re.split(r'(?=^\s*\d+\.\s+)', content, flags=re.MULTILINE)
            elif re.search(r'^\s*[•\-]\s+', content, re.MULTILINE):
                blocks = re.split(r'(?=^\s*[•\-]\s+)', content, flags=re.MULTILINE)
            else:
                blocks = re.split(r'\n\s*\n', content)

        elif semantic_type == 'coverage':
            # For coverage, split by benefit types or coverage areas
            blocks = re.split(r'(?=\n[A-ZÀÈÉÌÒÙ][A-ZÀÈÉÌÒÙa-zàèéìòù\s]*:?\n)', content)
            if len(blocks) == 1:
                blocks = re.split(r'\n\s*\n', content)

        elif semantic_type == 'procedures':
            # For procedures, preserve step sequences
            if re.search(r'^\s*\d+\.\s+', content, re.MULTILINE):
                # Keep numbered steps together in logical groups
                blocks = re.split(r'(?=^\s*1\.\s+)', content, flags=re.MULTILINE)
            else:
                blocks = re.split(r'\n\s*\n', content)

        else:
            # Default semantic splitting
            blocks = re.split(r'\n\s*\n+', content)

        # Clean and filter blocks
        refined_blocks = []
        for block in blocks:
            block = block.strip()
            if not block:
                continue

            # Further semantic refinement if needed
            if len(block) > self.config.max_chunk_size * 1.5:
                # Only split if really necessary and safe
                sub_blocks = self._safe_semantic_subsplit(block, semantic_type)
                refined_blocks.extend(sub_blocks)
            else:
                refined_blocks.append(block)

        return [block for block in refined_blocks if block.strip()]

    def _safe_semantic_subsplit(self, block: str, semantic_type: str) -> List[str]:
        """Safely split a large semantic block while preserving meaning."""

        # Try sentence-based splitting first
        sentences = self._enhanced_split_into_sentences(block)

        if len(sentences) <= 1:
            return [block]  # Can't split safely

        chunks = []
        current_chunk = ""

        for sentence in sentences:
            test_chunk = (current_chunk + " " + sentence).strip() if current_chunk else sentence

            if len(test_chunk) <= self.config.max_chunk_size:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def _enhanced_split_into_sentences(self, text: str) -> List[str]:
        """Enhanced sentence splitting for multilingual content with better accuracy."""

        # Enhanced sentence boundary patterns for Italian and English
        patterns = [
            # Standard sentence endings followed by capital letters
            r'([.!?]+)\s+(?=[A-ZÀÈÉÌÒÙ])',
            # Italian specific patterns
            r'([.!?]+)\s+(?=(?:Il|La|Lo|Gli|Le|Un|Una|Che|Come|Quando|Dove|Perché|Se)\s)',
            # English specific patterns
            r'([.!?]+)\s+(?=(?:The|A|An|What|How|When|Where|Why|If)\s)',
            # List item boundaries (preserve list structure)
            r';\s*(?=\n|\s*[A-ZÀÈÉÌÒÙ])',
        ]

        # Try each pattern and use the one that gives the best split
        best_sentences = [text]

        for pattern in patterns:
            candidate_sentences = re.split(pattern, text)

            # Reconstruct sentences with punctuation
            reconstructed = []
            i = 0
            while i < len(candidate_sentences):
                sentence = candidate_sentences[i].strip()

                # Add back punctuation if it was captured
                if i + 1 < len(candidate_sentences) and re.match(r'^[.!?]+$', candidate_sentences[i + 1]):
                    sentence += candidate_sentences[i + 1]
                    i += 2
                else:
                    i += 1

                if sentence and len(sentence) > 10:  # Avoid tiny fragments
                    reconstructed.append(sentence)

            # Use this split if it's better than current best
            if len(reconstructed) > len(best_sentences) and len(reconstructed) > 1:
                # Check quality of split
                avg_length = sum(len(s) for s in reconstructed) / len(reconstructed)
                if 20 < avg_length < 500:  # Reasonable sentence lengths
                    best_sentences = reconstructed

        return best_sentences

    def _safe_split_large_semantic_block(self, block: str, semantic_type: str) -> List[str]:
        """Safely split a large semantic block while preserving context."""

        # First try enhanced semantic subsplitting
        sub_blocks = self._safe_semantic_subsplit(block, semantic_type)

        if all(len(sb) <= self.config.max_chunk_size for sb in sub_blocks):
            return sub_blocks

        # If still too large, use enhanced sentence-based splitting
        final_chunks = []

        for sub_block in sub_blocks:
            if len(sub_block) <= self.config.max_chunk_size:
                final_chunks.append(sub_block)
            else:
                # Enhanced sentence splitting with clause preservation
                sentence_chunks = self._split_by_enhanced_clauses(sub_block)
                final_chunks.extend(sentence_chunks)

        return final_chunks

    def _split_by_enhanced_clauses(self, text: str) -> List[str]:
        """Split text by enhanced clause detection while preserving meaning."""

        if len(text) <= self.config.max_chunk_size:
            return [text]

        # Enhanced clause boundary markers for Italian and English
        clause_patterns = [
            # Italian clause markers
            r',\s+(?=(?:che|il quale|la quale|dove|quando|come|perché|mentre|anche se|tuttavia|inoltre|quindi|pertanto)\s)',
            r';\s+',
            r':\s+(?=[A-ZÀÈÉÌÒÙ])',
            r'\s+(?:e|o|ma|però|tuttavia|inoltre|quindi|pertanto|mentre)\s+',
            r'\s+–\s+',
            r'\s+—\s+',

            # English clause markers
            r',\s+(?=(?:which|who|where|when|how|because|while|although|however|furthermore|therefore|thus)\s)',
            r'\s+(?:and|or|but|however|furthermore|therefore|thus|while)\s+',
        ]

        best_split = None
        best_balance = float('inf')

        for pattern in clause_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))

            for match in matches:
                split_pos = match.end()
                left_part = text[:split_pos].strip()
                right_part = text[split_pos:].strip()

                # Check if split creates reasonable chunks
                if (self.config.min_chunk_size <= len(left_part) <= self.config.max_chunk_size and
                        len(right_part) >= self.config.min_chunk_size):

                    # Prefer splits that create balanced chunks
                    balance = abs(len(left_part) - len(right_part))
                    if balance < best_balance:
                        best_balance = balance
                        best_split = (left_part, right_part)

        if best_split:
            left_part, right_part = best_split
            chunks = [left_part]

            # Recursively split right part if still too long
            if len(right_part) > self.config.max_chunk_size:
                chunks.extend(self._split_by_enhanced_clauses(right_part))
            else:
                chunks.append(right_part)

            return chunks

        # Last resort: preserve as much semantic meaning as possible
        return self._preserve_semantic_meaning_split(text)

    def _preserve_semantic_meaning_split(self, text: str) -> List[str]:
        """Last resort splitting that tries to preserve semantic meaning."""

        # If we must split, try to do it at the most meaningful boundary

        # Try to split at paragraph boundaries first
        paragraphs = text.split('\n\n')
        if len(paragraphs) > 1:
            return self._group_paragraphs_by_size(paragraphs)

        # Try to split at sentence boundaries
        sentences = self._enhanced_split_into_sentences(text)
        if len(sentences) > 1:
            return self._group_sentences_by_size(sentences)

        # Absolute last resort: split but add clear continuation markers
        mid_point = len(text) // 2

        # Try to find a word boundary near the midpoint
        best_split = mid_point
        for offset in range(0, 100):  # Look within 100 characters
            for pos in [mid_point - offset, mid_point + offset]:
                if 0 < pos < len(text) and text[pos].isspace():
                    best_split = pos
                    break
            if best_split != mid_point:
                break

        left_part = text[:best_split].strip()
        right_part = text[best_split:].strip()

        # Add continuation markers to indicate the split
        if not left_part.endswith(('.', '!', '?', ';', ':')):
            left_part += "..."

        return [left_part, right_part]

    def _group_paragraphs_by_size(self, paragraphs: List[str]) -> List[str]:
        """Group paragraphs into chunks of appropriate size."""
        chunks = []
        current_chunk = ""

        for para in paragraphs:
            test_chunk = (current_chunk + "\n\n" + para).strip() if current_chunk else para

            if len(test_chunk) <= self.config.max_chunk_size:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())

                # Handle oversized paragraph
                if len(para) > self.config.max_chunk_size:
                    chunks.extend(self._preserve_semantic_meaning_split(para))
                else:
                    current_chunk = para

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def _group_sentences_by_size(self, sentences: List[str]) -> List[str]:
        """Group sentences into chunks of appropriate size."""
        chunks = []
        current_chunk = ""

        for sentence in sentences:
            test_chunk = (current_chunk + " " + sentence).strip() if current_chunk else sentence

            if len(test_chunk) <= self.config.max_chunk_size:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())

                # Handle oversized sentence
                if len(sentence) > self.config.max_chunk_size:
                    chunks.extend(self._preserve_semantic_meaning_split(sentence))
                else:
                    current_chunk = sentence

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def _add_enhanced_semantic_overlap(self, chunks: List[str], semantic_type: str) -> List[str]:
        """Add enhanced semantic overlap between chunks to preserve context."""

        if len(chunks) <= 1:
            return chunks

        # Adjust overlap size based on semantic type
        if semantic_type in ['definitions', 'procedures', 'coverage']:
            overlap_size = min(self.config.overlap_size * 2, 300)  # More overlap for important content
        elif semantic_type == 'exclusions':
            overlap_size = min(self.config.overlap_size, 150)  # Less overlap for exclusions
        else:
            overlap_size = self.config.overlap_size

        overlapped_chunks = []

        for i, chunk in enumerate(chunks):
            if i == 0:
                overlapped_chunks.append(chunk)
            else:
                # Extract meaningful overlap from previous chunk
                prev_chunk = chunks[i - 1]
                overlap_text = self._extract_semantic_overlap(prev_chunk, overlap_size, semantic_type)

                if overlap_text:
                    # Add context separator
                    separator = "\n\n[Continued from previous section]\n"
                    overlapped_chunk = f"{overlap_text.strip()}{separator}{chunk}"
                    overlapped_chunks.append(overlapped_chunk)
                else:
                    overlapped_chunks.append(chunk)

        return overlapped_chunks

    def _extract_semantic_overlap(self, text: str, max_overlap_size: int, semantic_type: str) -> str:
        """Extract semantically meaningful overlap from text."""

        if len(text) <= max_overlap_size:
            return ""  # Don't overlap if the entire chunk is small

        # Get the last portion of text for overlap
        tail = text[-max_overlap_size * 2:]  # Look at more text to find good boundary

        # Find semantic boundaries for different content types
        if semantic_type == 'definitions':
            # For definitions, try to include the complete definition
            sentences = self._enhanced_split_into_sentences(tail)
            if len(sentences) > 1:
                # Include last complete definition/explanation
                return sentences[-1] if len(sentences[-1]) <= max_overlap_size else ""

        elif semantic_type == 'procedures':
            # For procedures, include the last complete step or instruction
            if re.search(r'\d+\.\s+', tail):
                # Find last numbered step
                steps = re.split(r'(?=\d+\.\s+)', tail)
                if steps and len(steps[-1]) <= max_overlap_size:
                    return steps[-1].strip()

        elif semantic_type in ['coverage', 'exclusions']:
            # For coverage/exclusions, include last complete item or clause
            if re.search(r'[•\-\*]\s+', tail):
                items = re.split(r'(?=[•\-\*]\s+)', tail)
                if items and len(items[-1]) <= max_overlap_size:
                    return items[-1].strip()

        # Default: extract last complete sentence(s)
        sentences = self._enhanced_split_into_sentences(tail)
        if len(sentences) > 1:
            last_sentence = sentences[-1]
            if len(last_sentence) <= max_overlap_size:
                return last_sentence
            elif len(sentences) > 2:
                # Try last two sentences if they fit
                last_two = sentences[-2] + " " + sentences[-1]
                if len(last_two) <= max_overlap_size:
                    return last_two

        return ""

    def _create_enhanced_intelligent_fallback_chunks(self, text: str, tables_by_page: Dict[int, List[Dict]]) -> \
    List[DocumentChunk]:
        """Enhanced intelligent fallback chunking with better quality control."""
        chunks = []
        chunk_counter = 0

        elements = self._enhanced_parse_text_with_page_tracking(text)

        if not elements:
            return self._create_enhanced_basic_page_chunks(text, tables_by_page)

        current_section_title = "Document Content"
        current_section_content = []
        current_page = 1
        current_semantic_type = "general"

        for element in elements:
            if element['type'] == 'structure':
                # Save previous content
                if current_section_content:
                    content_text = '\n'.join(current_section_content).strip()
                    if len(content_text) >= self.config.min_chunk_size:
                        chunk = self._create_enhanced_quality_chunk(
                            chunk_counter, current_section_title, content_text,
                            current_page, [current_section_title], tables_by_page,
                            "enhanced_intelligent_fallback", current_semantic_type
                        )
                        chunks.append(chunk)
                        chunk_counter += 1

                # Start new section
                current_section_title = element['title']
                current_section_content = []
                current_page = element['page']
                current_semantic_type = element.get('semantic_type', 'general')

            elif element['type'] == 'content':
                current_section_content.append(element['content'])
                current_page = element['page']
                if element.get('semantic_type', 'general') != 'general':
                    current_semantic_type = element['semantic_type']

        # Save final content
        if current_section_content:
            content_text = '\n'.join(current_section_content).strip()
            if len(content_text) >= self.config.min_chunk_size:
                chunk = self._create_enhanced_quality_chunk(
                    chunk_counter, current_section_title, content_text,
                    current_page, [current_section_title], tables_by_page,
                    "enhanced_intelligent_fallback", current_semantic_type
                )
                chunks.append(chunk)
                chunk_counter += 1

        self.logger.info(f"Created {len(chunks)} enhanced intelligent fallback chunks")
        return chunks

    def _create_enhanced_basic_page_chunks(self, text: str, tables_by_page: Dict[int, List[Dict]]) -> List[
        DocumentChunk]:
        """Enhanced basic page-based chunking with better semantic preservation."""
        chunks = []
        chunk_counter = 0
        current_page = 1
        current_content = []

        lines = text.split('\n')

        for line in lines:
            # Check for page markers
            page_patterns = [
                r'^---\s*Page\s+(\d+)\s*---\s*$',
                r'^Pag\.\s*(\d+)\s*di\s*\d+\s*$',
            ]

            page_found = False
            for pattern in page_patterns:
                page_match = re.match(pattern, line.strip())
                if page_match:
                    # Save previous page content
                    if current_content:
                        content_text = '\n'.join(current_content).strip()
                        if len(content_text) >= self.config.min_chunk_size:
                            semantic_type = self._detect_content_semantic_type(content_text)
                            chunk = self._create_enhanced_quality_chunk(
                                chunk_counter, f"Page {current_page}", content_text,
                                current_page, [f"Page {current_page}"], tables_by_page,
                                "enhanced_basic_page", semantic_type
                            )
                            chunks.append(chunk)
                            chunk_counter += 1

                    # Start new page
                    current_page = int(page_match.group(1))
                    current_content = []
                    page_found = True
                    break

            if not page_found:
                current_content.append(line)

        # Save final page
        if current_content:
            content_text = '\n'.join(current_content).strip()
            if len(content_text) >= self.config.min_chunk_size:
                semantic_type = self._detect_content_semantic_type(content_text)
                chunk = self._create_enhanced_quality_chunk(
                    chunk_counter, f"Page {current_page}", content_text,
                    current_page, [f"Page {current_page}"], tables_by_page,
                    "enhanced_basic_page", semantic_type
                )
                chunks.append(chunk)
                chunk_counter += 1

        self.logger.info(f"Created {len(chunks)} enhanced basic page chunks")
        return chunks

    def _determine_level_from_title(self, title: str) -> int:
        """Determine hierarchical level from title with enhanced pattern recognition."""
        if not title:
            return 999

        title_clean = title.strip()

        # Level 0: Questions (highest priority semantic sections)
        if title_clean.endswith('?'):
            return 0

        # Level 1: Main sections (SECTION, SEZIONE, etc.)
        if re.search(r'^(?:SECTION|SEZIONE|PARTE|PART|CAPITOLO|CHAPTER)\s+[A-Z\d]+', title_clean, re.IGNORECASE):
            return 1

        # Level 1: All caps titles (likely main sections)
        if re.match(r'^[A-ZÀÈÉÌÒÙ][A-ZÀÈÉÌÒÙ\s]*[A-ZÀÈÉÌÒÙ]$', title_clean):
            return 1

        # Level 1: Italian insurance specific sections
        italian_main_sections = [
            r'^(?:ASSISTENZA\s+IN\s+VIAGGIO|SPESE\s+MEDICHE|BAGAGLIO|ANNULLAMENTO\s+VIAGGIO)',
            r'^(?:GARANZIE?|COPERTURE?|ESCLUSIONI|CONDIZIONI)'
        ]
        for pattern in italian_main_sections:
            if re.search(pattern, title_clean, re.IGNORECASE):
                return 1

        # Level 2: Articles (ARTICLE, ARTICOLO, ART.)
        if re.search(r'^(?:ARTICLE|ARTICOLO|ART\.)\s+\d+', title_clean, re.IGNORECASE):
            return 2

        # Level 2: Numbered sections (1.0, 2.0, etc.)
        if re.match(r'^\d+\.\d+\s*[-–—:]*\s*.+$', title_clean):
            return 2

        # Level 2: Letter-number combinations (A1, B.1, etc.)
        if re.match(r'^[A-Z]\d*\s*[-–—:]*\s*.+$', title_clean):
            return 2

        # Level 3: Detailed subsections (1.2.3, A.1.2, etc.)
        if re.match(r'^\d+\.\d+\.\d+', title_clean):
            return 3

        # Level 3: Letter-dot-number (A.1, B.2, etc.)
        if re.match(r'^[A-Z]\.\d+\s*[-–—:]*\s*.+$', title_clean):
            return 3

        # Level 4: Lettered sub-items (a), b), etc.)
        if re.match(r'^[a-z]\)\s*.+$', title_clean):
            return 4

        # Level 4: Single letter subsections (A., B., etc.)
        if re.match(r'^[A-Z]\.\s*.+$', title_clean):
            return 4

        # Default: content level
        return 999

    def _extract_section_id_from_title(self, title: str) -> str:
        """Extract section ID from title with comprehensive pattern matching."""
        if not title:
            return ""

        title_clean = title.strip()

        # Pattern 1: Section/Article with Roman or Arabic numerals
        # SECTION A, SEZIONE I, ARTICLE 1, ARTICOLO 2
        match = re.match(
            r'^(?:SECTION|SEZIONE|PARTE|PART|CAPITOLO|CHAPTER|ARTICLE|ARTICOLO|ART\.)\s+([A-Z\d]+(?:\s+[IV]+)?)',
            title_clean, re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # Pattern 2: Numbered sections (1.2.3, 2.1, etc.)
        match = re.match(r'^(\d+(?:\.\d+)*)', title_clean)
        if match:
            return match.group(1)

        # Pattern 3: Letter-number combinations (A1, B2, etc.)
        match = re.match(r'^([A-Z]\d*)', title_clean)
        if match:
            return match.group(1)

        # Pattern 4: Letter-dot-number (A.1, B.2, etc.)
        match = re.match(r'^([A-Z]\.\d+)', title_clean)
        if match:
            return match.group(1)

        # Pattern 5: Single letters with parentheses (a), b), etc.)
        match = re.match(r'^([a-z])\)', title_clean)
        if match:
            return match.group(1)

        # Pattern 6: Single letters with dots (A., B., etc.)
        match = re.match(r'^([A-Z])\.', title_clean)
        if match:
            return match.group(1)

        # Pattern 7: Italian specific patterns (C.1 - Oggetto dell'assicurazione)
        match = re.match(r'^([A-Z]\.?\d*)\s*[-–—:]*\s*', title_clean)
        if match:
            return match.group(1)

        # Pattern 8: Roman numerals
        match = re.match(r'^([IVX]+)', title_clean)
        if match:
            return match.group(1)

        # No identifiable section ID
        return ""

    def _create_enhanced_quality_chunk(self, chunk_id: int, title: str, content: str, page: int,
                                       section_path: List[str], tables_by_page: Dict[int, List[Dict]],
                                       method: str, semantic_type: str = "general",
                                       quality_indicators: Dict[str, Any] = None) -> DocumentChunk:
        """Create an enhanced quality document chunk with comprehensive metadata."""

        # Enhanced content cleaning and validation
        content = self.clean_text(content)

        # Calculate quality score
        quality_score = self._calculate_chunk_quality_score(title, content, semantic_type, quality_indicators or {})

        # Enhanced metadata
        enhanced_metadata = {
            'level': self._determine_level_from_title(title),
            'section_id': self._extract_section_id_from_title(title),
            'word_count': len(content.split()),
            'char_count': len(content),
            'sentence_count': len(self._enhanced_split_into_sentences(content)),
            'chunk_method': method,
            'parent_section': section_path[-2] if len(section_path) > 1 else None,
            'document_hierarchy': section_path.copy(),
            'language': self._detect_chunk_language(content),
            'content_type': self._classify_content_type(title, content),
            'semantic_type': semantic_type,
            'quality_score': quality_score,
            'has_questions': bool(re.search(r'\?', content)),
            'has_lists': bool(re.search(r'^\s*[•\-\*\d+\.\)]\s', content, re.MULTILINE)),
            'has_definitions': bool(
                re.search(r'(significa|significa che|definisce|definition|means|refers to)', content,
                          re.IGNORECASE)),
            'has_procedures': bool(
                re.search(r'(procedura|procedure|modalità|step|fase|point)', content, re.IGNORECASE)),
            'context_preserved': method in ['enhanced_structured', 'enhanced_intelligent_fallback'],
            'is_complete_concept': self._is_complete_concept(content, semantic_type),
            'readability_score': self._calculate_readability_score(content),
            'topic_keywords': self._extract_topic_keywords(content, semantic_type),
        }

        chunk = DocumentChunk(
            chunk_id=f"chunk_{chunk_id:04d}",
            chunk_type=self._determine_enhanced_chunk_type(title, content, semantic_type),
            title=title,
            content=content,
            page_number=page,
            section_path=section_path.copy(),
            metadata=enhanced_metadata,
            tables=tables_by_page.get(page, [])
        )

        return chunk

    def _calculate_chunk_quality_score(self, title: str, content: str, semantic_type: str,
                                       quality_indicators: Dict[str, Any]) -> float:
        """Calculate a comprehensive quality score for the chunk."""

        score = 0.0

        # Title quality (20% of score)
        if title and title.strip():
            score += 0.1
            if 10 <= len(title) <= 100:
                score += 0.05
            if title.endswith('?'):  # Questions are valuable
                score += 0.05

        # Content quality (40% of score)
        if content and content.strip():
            score += 0.1
            word_count = len(content.split())
            if self.config.min_chunk_size <= len(content) <= self.config.optimal_chunk_size:
                score += 0.1
            if word_count >= 20:  # Substantial content
                score += 0.05
            if re.search(r'[.!?]', content):  # Has proper punctuation
                score += 0.05
            if not re.search(r'\.\.\.|…', content):  # Not truncated
                score += 0.1

        # Semantic value (20% of score)
        if semantic_type != 'general':
            score += 0.1
            if semantic_type in ['coverage', 'exclusions', 'definitions', 'procedures']:
                score += 0.1  # High-value semantic types

        # Structure and completeness (20% of score)
        if self._is_complete_concept(content, semantic_type):
            score += 0.1
        if quality_indicators.get('has_section_id', False):
            score += 0.05
        if not quality_indicators.get('is_orphaned', False):
            score += 0.05

        return min(1.0, score)  # Cap at 1.0

    def _is_complete_concept(self, content: str, semantic_type: str) -> bool:
        """Check if the content represents a complete concept."""

        # Check for truncation indicators
        if content.endswith('...') or content.endswith('…'):
            return False

        # Check for incomplete sentences
        if not re.search(r'[.!?]\s*$', content.strip()):
            # Allow exceptions for lists and structured content
            if not re.search(r'^\s*[•\-\*\d+\.\)]\s', content, re.MULTILINE):
                return False

        # Semantic type specific checks
        if semantic_type == 'definitions':
            # Should have definition indicators
            if not re.search(r'(significa|significa che|definisce|definition|means|refers to)', content,
                             re.IGNORECASE):
                return False

        elif semantic_type == 'procedures':
            # Should have procedural indicators
            if not re.search(r'(procedura|procedure|modalità|step|fase|point|come|how)', content, re.IGNORECASE):
                return False

        return True

    def _calculate_readability_score(self, content: str) -> float:
        """Calculate a basic readability score for the content."""

        if not content:
            return 0.0

        words = content.split()
        sentences = self._enhanced_split_into_sentences(content)

        if not words or not sentences:
            return 0.0

        avg_words_per_sentence = len(words) / len(sentences)

        # Simple readability metric (inverse of average sentence length)
        # Shorter sentences are generally more readable
        if avg_words_per_sentence <= 15:
            readability = 1.0
        elif avg_words_per_sentence <= 25:
            readability = 0.8
        elif avg_words_per_sentence <= 35:
            readability = 0.6
        else:
            readability = 0.4

        # Bonus for proper punctuation and structure
        if re.search(r'[.!?]', content):
            readability += 0.1

        # Penalty for excessive jargon (very long words)
        long_words = [w for w in words if len(w) > 12]
        if len(long_words) / len(words) > 0.1:  # More than 10% long words
            readability -= 0.2

        return max(0.0, min(1.0, readability))

    def _extract_topic_keywords(self, content: str, semantic_type: str) -> List[str]:
        """Extract key topic keywords from content."""

        keywords = []
        content_lower = content.lower()

        # Use profile-specific semantic indicators if available
        if self.config.company_profile and hasattr(self.config.company_profile, 'semantic_indicators'):
            for topic, indicators in self.config.company_profile.semantic_indicators.items():
                for indicator in indicators:
                    if indicator in content_lower:
                        keywords.append(topic)
                        break

        # Add semantic type as keyword
        if semantic_type != 'general':
            keywords.append(semantic_type)

        # Extract important domain-specific terms
        domain_terms = {
            'italian': [
                'assicurato', 'polizza', 'garanzia', 'copertura', 'sinistro', 'premio',
                'rimborso', 'indennizzo', 'esclusioni', 'assistenza', 'viaggio', 'bagaglio'
            ],
            'english': [
                'insured', 'policy', 'coverage', 'claim', 'premium', 'reimbursement',
                'exclusions', 'assistance', 'travel', 'baggage', 'benefits'
            ]
        }

        language = self._detect_chunk_language(content)
        if language in domain_terms:
            for term in domain_terms[language]:
                if term in content_lower:
                    keywords.append(term)

        return list(set(keywords))  # Remove duplicates

    def _enhance_chunk_quality(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """Enhance chunk quality through post-processing validation and improvement."""

        if not chunks:
            return chunks

        enhanced_chunks = []

        for chunk in chunks:
            # Skip low-quality chunks if quality filtering is enabled
            quality_score = chunk.metadata.get('quality_score', 0.0)
            if quality_score < self.config.min_quality_score:
                self.logger.warning(f"Filtered out low-quality chunk: {chunk.title} (score: {quality_score:.2f})")
                continue

            # Enhance content readability if enabled
            if self.config.enhance_readability:
                chunk.content = self._enhance_content_readability(chunk.content)

            # Validate semantic completeness if required
            if self.config.preserve_semantic_units:
                if not self._is_complete_concept(chunk.content, chunk.metadata.get('semantic_type', 'general')):
                    self.logger.warning(f"Chunk may have incomplete semantic content: {chunk.title}")
                    # Try to merge with next chunk if possible
                    # (This would require more complex logic to implement properly)

            # Ensure complete sentences if required
            if self.config.require_complete_sentences:
                chunk.content = self._ensure_complete_sentences(chunk.content)

            enhanced_chunks.append(chunk)

        self.logger.info(f"Enhanced chunk quality: {len(chunks)} -> {len(enhanced_chunks)} chunks")
        return enhanced_chunks

    def _enhance_content_readability(self, content: str) -> str:
        """Enhance content readability through formatting improvements."""

        # Ensure proper spacing after punctuation
        content = re.sub(r'([.!?])(\w)', r'\1 \2', content)

        # Fix common formatting issues
        content = re.sub(r'\s+', ' ', content)  # Normalize whitespace
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)  # Normalize line breaks

        # Ensure proper punctuation spacing
        content = re.sub(r'\s+([.,;:!?])', r'\1', content)

        # Fix number formatting
        content = re.sub(r'(\d)\s+([.,])\s*(\d)', r'\1\2\3', content)

        return content.strip()

    def _ensure_complete_sentences(self, content: str) -> str:
        """Ensure content ends with complete sentences."""

        content = content.strip()

        # If content doesn't end with proper punctuation
        if not re.search(r'[.!?]\s*$', content):
            # Check if it's a list or structured content (these are OK without final punctuation)
            if re.search(r'^\s*[•\-\*\d+\.\)]\s', content, re.MULTILINE):
                return content  # Lists are OK as-is

            # For regular text, try to find the last complete sentence
            sentences = self._enhanced_split_into_sentences(content)
            if len(sentences) > 1:
                # Keep only complete sentences
                complete_sentences = []
                for sentence in sentences:
                    if re.search(r'[.!?]\s*$', sentence.strip()):
                        complete_sentences.append(sentence)
                    else:
                        break  # Stop at first incomplete sentence

                if complete_sentences:
                    content = ' '.join(complete_sentences).strip()

        return content

    def _validate_enhanced_chunk(self, chunk: DocumentChunk) -> bool:
        """Enhanced chunk validation with comprehensive quality checks."""

        if not chunk.content or not chunk.content.strip():
            return False

        content_length = len(chunk.content.strip())

        # Basic size validation
        if content_length < self.config.min_chunk_size:
            return False

        if content_length > self.config.max_chunk_size * 1.2:  # Allow some flexibility
            return False

        # Content quality validation
        if not re.search(r'\w+', chunk.content):
            return False

        # Language validation
        if chunk.metadata.get('language', 'unknown') == 'unknown':
            # Try to re-detect language
            chunk.metadata['language'] = self._detect_chunk_language(chunk.content)

        # Quality score validation
        quality_score = chunk.metadata.get('quality_score', 0.0)
        if quality_score < self.config.min_quality_score:
            return False

        # Semantic completeness validation (if enabled)
        if self.config.preserve_semantic_units:
            semantic_type = chunk.metadata.get('semantic_type', 'general')
            if not self._is_complete_concept(chunk.content, semantic_type):
                return False

        return True

    def _determine_enhanced_chunk_type(self, title: str, content: str, semantic_type: str) -> str:
        """Enhanced chunk type determination with semantic understanding."""

        title_lower = title.lower()
        content_lower = content.lower()

        # Question-based sections (highest priority)
        if title.endswith('?'):
            if re.search(r'che cosa.*assicurat', title_lower) or re.search(r'what.*insured', title_lower):
                return "coverage_section"
            elif re.search(r'che cosa non.*assicurat', title_lower) or re.search(r'what.*not.*insured',
                                                                                 title_lower):
                return "exclusions_section"
            elif re.search(r'limiti.*copertura', title_lower) or re.search(r'coverage.*limits', title_lower):
                return "limits_section"
            elif re.search(r'obblighi', title_lower) or re.search(r'obligations', title_lower):
                return "obligations_section"
            elif re.search(r'sinistro|claims?', title_lower):
                return "claims_section"
            elif re.search(r'pagare.*premio|payment', title_lower):
                return "payment_section"
            else:
                return "question_section"

        # Semantic type-based classification
        if semantic_type == 'coverage':
            return "coverage_section"
        elif semantic_type == 'exclusions':
            return "exclusions_section"
        elif semantic_type == 'claims':
            return "claims_section"
        elif semantic_type == 'premium':
            return "payment_section"
        elif semantic_type == 'assistance':
            return "assistance_section"
        elif semantic_type == 'definitions':
            return "definitions_section"
        elif semantic_type == 'procedures':
            return "procedures_section"

        # Traditional structural classification
        if re.search(r'section|sezione', title_lower):
            return "section"
        elif re.search(r'article|articolo', title_lower):
            return "article"
        elif re.search(r'^\d+\.\d+', title):
            return "subsection"
        elif re.search(r'^[A-Z]\.\d+', title):
            return "clause"
        else:
            return "content_block"

    def _classify_content_type(self, title: str, content: str) -> str:
        """Enhanced content type classification for RAG optimization."""

        content_lower = content.lower()
        title_lower = title.lower()

        # Use enhanced semantic detection
        semantic_type = self._detect_content_semantic_type(content)
        if semantic_type != 'general':
            return semantic_type

        # Enhanced pattern matching
        patterns = {
            'exclusions': [
                r'esclus[oi]', r'non\s+(?:coperto|assicurato)', r'limitazion[ei]', r'restrizion[ei]',
                r'exclud[ed]', r'not\s+(?:covered|insured)', r'limitation[s]?', r'restriction[s]?'
            ],
            'coverage': [
                r'copertur[ao]', r'assicurat[oi]', r'garanzi[ao]', r'protezion[ei]', r'benefici[oi]',
                r'cover(?:age|ed|s)', r'insur(?:ed|ance)', r'protect(?:ion|ed)', r'benefit[s]?'
            ],
            'limits': [
                r'limite[i]?', r'massimale[i]?', r'limit[s]?', r'maximum', r'cap'
            ],
            'premium': [
                r'premio', r'pagamento', r'costo', r'tariffa', r'importo',
                r'premium', r'payment', r'cost', r'fee', r'charge'
            ],
            'claims': [
                r'sinistro', r'danno', r'perdita', r'incidente', r'evento', r'denuncia',
                r'claim[s]?', r'damage', r'loss', r'incident', r'accident', r'report'
            ],
            'assistance': [
                r'assistenza', r'aiuto', r'supporto', r'servizio', r'soccorso',
                r'assistance', r'help', r'support', r'service', r'aid'
            ],
            'reimbursement': [
                r'rimborso', r'indennizzo', r'risarcimento',
                r'reimbursement', r'refund', r'compensation'
            ]
        }

        # Score each category
        scores = {}
        for category, category_patterns in patterns.items():
            score = 0
            for pattern in category_patterns:
                matches = len(re.findall(pattern, content_lower, re.IGNORECASE))
                score += matches * 2  # Weight content matches more

                # Also check title
                title_matches = len(re.findall(pattern, title_lower, re.IGNORECASE))
                score += title_matches * 3  # Weight title matches even more

            if score > 0:
                scores[category] = score

        # Return highest scoring category
        if scores:
            return max(scores, key=scores.get)

        return "general"

    def extract_tables_from_page(self, pdf_path: str, page_num: int) -> List[Dict[str, Any]]:
        """Extract tables with improved accuracy and better structure."""
        tables = []
        try:
            strategy = self.config.table_detection_strategy
            if strategy == "aggressive":
                methods = [("lattice", 60), ("stream", 60)]
            elif strategy == "conservative":
                methods = [("lattice", 90)]
            else:
                methods = [("lattice", self.config.table_accuracy_threshold),
                           ("stream", self.config.table_accuracy_threshold)]

            for method, threshold in methods:
                try:
                    if method == "lattice":
                        camelot_tables = camelot.read_pdf(
                            pdf_path, pages=str(page_num), flavor="lattice",
                            line_scale=40, table_areas=None
                        )
                    else:
                        camelot_tables = camelot.read_pdf(
                            pdf_path, pages=str(page_num), flavor="stream",
                            edge_tol=500
                        )

                    for i, table in enumerate(camelot_tables):
                        if table.accuracy >= threshold:
                            structured_table = self._structure_enhanced_table(table.df, i, method, table.accuracy,
                                                                              page_num)
                            tables.append(structured_table)
                            self.logger.debug(f"Extracted {method} table {i} with accuracy {table.accuracy:.1f}%")

                    if tables and strategy == "adaptive":
                        break

                except Exception as e:
                    self.logger.debug(f"Table extraction with {method} failed: {e}")
                    continue

        except Exception as e:
            self.logger.warning(f"Table extraction failed for page {page_num}: {e}")

        return tables

    def _structure_enhanced_table(self, df: pd.DataFrame, table_id: int, method: str, accuracy: float,
                                  page_num: int) -> Dict[str, Any]:
        """Structure table data with enhanced processing and validation."""

        # Clean empty rows and columns
        df = df.dropna(how='all').dropna(axis=1, how='all')

        if df.empty:
            return {
                "table_id": table_id, "page": page_num, "method": method,
                "accuracy": accuracy, "headers": [], "data": [],
                "raw_shape": (0, 0), "format": "structured_table",
                "metadata": {"header_row": 0, "data_rows": 0, "columns": 0}
            }

        # Enhanced header detection
        header_row = self._find_enhanced_header_row(df)

        if header_row < len(df):
            headers = df.iloc[header_row].fillna('').astype(str).tolist()
            data_rows = df.iloc[header_row + 1:] if header_row < len(df) - 1 else pd.DataFrame()
        else:
            headers = [f"Column_{i + 1}" for i in range(len(df.columns))]
            data_rows = df

        # Enhanced header cleaning
        headers = [self._clean_table_header(str(h), i) for i, h in enumerate(headers)]

        # Enhanced data structuring
        structured_data = []
        for _, row in data_rows.iterrows():
            row_data = {}
            for header, value in zip(headers, row):
                if pd.notna(value) and str(value).strip():
                    clean_value = self.clean_text(str(value))
                    if clean_value and clean_value != 'nan':
                        row_data[header] = clean_value
            if row_data:
                structured_data.append(row_data)

        return {
            "table_id": table_id, "page": page_num, "method": method,
            "accuracy": accuracy, "headers": headers, "data": structured_data,
            "raw_shape": df.shape, "format": "structured_table",
            "metadata": {
                "header_row": header_row, "data_rows": len(structured_data),
                "columns": len(headers), "quality_score": self._assess_table_quality(headers, structured_data)
            }
        }

    def _clean_table_header(self, header: str, index: int) -> str:
        """Clean and enhance table headers."""

        header = self.clean_text(header)

        if not header or header.lower() in ['nan', 'none', '']:
            return f"Column_{index + 1}"

        # Remove common artifacts
        header = re.sub(r'^\s*[-–—]+\s*', '', header)
        header = re.sub(r'\s*[-–—]+\s*$', '', header)

        # Limit length
        if len(header) > 50:
            header = header[:47] + "..."

        return header.strip() or f"Column_{index + 1}"

    def _find_enhanced_header_row(self, df: pd.DataFrame) -> int:
        """Find optimal header row with enhanced detection."""

        if len(df) <= 1:
            return 0

        scores = []
        for i in range(min(3, len(df))):
            row = df.iloc[i]
            score = 0

            # Basic filled ratio
            filled_ratio = row.notna().sum() / len(row)
            score += filled_ratio * 10

            # Check for header-like content
            for cell in row:
                if pd.notna(cell):
                    cell_str = str(cell).strip()

                    # Common table header indicators
                    header_indicators = [
                        'OPTION', 'AMOUNT', 'TYPE', 'NAME', 'COVERAGE', 'LIMIT', 'PREMIUM',
                        'COPERTURA', 'GARANZIA', 'LIMITE', 'PREMIO', 'TIPO', 'NOME', 'IMPORTO'
                    ]

                    if any(indicator in cell_str.upper() for indicator in header_indicators):
                        score += 5

                    # Reasonable header length
                    if 3 <= len(cell_str) <= 50:
                        score += 2

                    # Proper capitalization
                    if cell_str.istitle() or cell_str.isupper():
                        score += 1

                    # Not purely numeric (headers usually aren't)
                    if not cell_str.replace('.', '').replace(',', '').isdigit():
                        score += 1

            scores.append(score)

        return scores.index(max(scores)) if scores else 0

    def _assess_table_quality(self, headers: List[str], data: List[Dict[str, Any]]) -> float:
        """Assess table quality score."""

        score = 0.0

        # Header quality
        if headers:
            score += 0.3
            unique_headers = len(set(headers))
            if unique_headers == len(headers):  # All unique
                score += 0.2

        # Data quality
        if data:
            score += 0.3

            # Check data consistency
            if len(data) > 1:
                avg_fields = sum(len(row) for row in data) / len(data)
                if avg_fields >= len(headers) * 0.7:  # Most fields filled
                    score += 0.2

        return min(1.0, score)

    def _detect_chunk_language(self, content: str) -> str:
        """Enhanced language detection for individual chunks."""

        content_lower = content.lower()

        # Enhanced Italian indicators with weights
        italian_indicators = {
            'che': 3, 'della': 3, 'sono': 3, 'assicurato': 10, 'garanzia': 8, 'polizza': 10,
            'articolo': 5, 'società': 5, 'copertura': 8, 'sinistro': 8, 'rimborso': 5,
            'assistenza': 5, 'viaggio': 3, 'bagaglio': 5, 'spese': 3, 'mediche': 5
        }

        # Enhanced English indicators with weights
        english_indicators = {
            'the': 2, 'and': 2, 'of': 2, 'insured': 10, 'coverage': 8, 'policy': 10,
            'article': 5, 'company': 5, 'claim': 8, 'assistance': 5, 'travel': 3,
            'baggage': 5, 'medical': 5, 'expenses': 5, 'benefits': 5
        }

        italian_score = 0
        english_score = 0

        # Count weighted occurrences
        words = re.findall(r'\b\w+\b', content_lower)
        word_set = set(words)

        for word, weight in italian_indicators.items():
            if word in word_set:
                italian_score += weight

        for word, weight in english_indicators.items():
            if word in word_set:
                english_score += weight

        # Character-level indicators
        italian_chars = (content_lower.count('à') + content_lower.count('è') + content_lower.count('é') +
                         content_lower.count('ì') + content_lower.count('ò') + content_lower.count('ù'))
        italian_score += italian_chars * 5

        # Determine language
        if italian_score > english_score * 1.2:  # Require clear majority
            return 'italian'
        elif english_score > italian_score * 1.2:
            return 'english'
        else:
            return 'unknown'

    def extract_document_metadata(self, doc: fitz.Document, text_sample: str = "") -> Dict[str, Any]:
        """Extract comprehensive document metadata with enhanced analysis."""

        return {
            "page_count": doc.page_count,
            "title": doc.metadata.get('title', '') or self._extract_title_from_content(text_sample),
            "author": doc.metadata.get('author', ''),
            "subject": doc.metadata.get('subject', ''),
            "creator": doc.metadata.get('creator', ''),
            "producer": doc.metadata.get('producer', ''),
            "creation_date": doc.metadata.get('creationDate', ''),
            "mod_date": doc.metadata.get('modDate', ''),
            "is_pdf": doc.is_pdf,
            "is_encrypted": doc.is_encrypted,
            "detected_language": self._detect_document_language(text_sample),
            "company_profile": self.config.company_profile.name if self.config.company_profile else "unknown",
            "document_type": self.config.document_type.value,
            "processing_config": {
                "table_accuracy_threshold": self.config.table_accuracy_threshold,
                "max_chunk_size": self.config.max_chunk_size,
                "min_chunk_size": self.config.min_chunk_size,
                "optimal_chunk_size": self.config.optimal_chunk_size,
                "overlap_size": self.config.overlap_size,
                "preserve_semantic_units": self.config.preserve_semantic_units,
                "min_quality_score": self.config.min_quality_score
            },
            "quality_indicators": {
                "estimated_readability": self._estimate_document_readability(text_sample),
                "structure_complexity": self._estimate_structure_complexity(text_sample),
                "content_density": len(text_sample.split()) / max(1, text_sample.count('\n')),
            }
        }

    def _extract_title_from_content(self, text: str) -> str:
        """Enhanced title extraction from content."""

        lines = text.split('\n')[:15]  # Look at more lines

        for line in lines:
            line = line.strip()
            if 10 < len(line) < 200:
                # Enhanced title indicators
                title_indicators = [
                    'INSURANCE', 'ASSICURAZIONE', 'CONTRACT', 'CONTRATTO',
                    'POLICY', 'POLIZZA', 'CONDITIONS', 'CONDIZIONI',
                    'TRAVEL', 'VIAGGIO', 'PROTECTION', 'PROTEZIONE'
                ]

                if any(keyword in line.upper() for keyword in title_indicators):
                    return self.clean_text(line)

        return ""

    def _detect_document_language(self, text: str) -> str:
        """Enhanced document language detection with better accuracy."""

        text_lower = text.lower()

        # Enhanced Italian indicators with higher precision
        italian_indicators = {
            'che cosa': 20, 'assicurato': 15, 'polizza': 15, 'garanzia': 12, 'sezione': 8,
            'articolo': 8, 'copertura': 12, 'sinistro': 12, 'premio': 8, 'contratto': 8,
            'della': 5, 'sono': 5, 'società': 8, 'centrale operativa': 15, 'spese mediche': 15,
            'assistenza': 8, 'quando comincia': 12, 'come posso': 12, 'quali obblighi': 12,
            'cosa fare': 8, 'rimborso': 8, 'indennizzo': 8, 'esclusioni': 12
        }

        # Enhanced English indicators with higher precision
        english_indicators = {
            'what is': 20, 'insured': 15, 'policy': 15, 'coverage': 12, 'section': 8,
            'article': 8, 'insurance': 15, 'contract': 8, 'travel': 5, 'baggage': 8,
            'medical expenses': 15, 'assistance': 8, 'when does': 12, 'how can': 12,
            'what are my': 12, 'what to do': 8, 'reimbursement': 8, 'claims': 12,
            'exclusions': 12, 'company': 5, 'insured person': 15
        }

        italian_score = 0
        english_score = 0

        # Count weighted matches with context
        for indicator, weight in italian_indicators.items():
            count = text_lower.count(indicator)
            italian_score += count * weight

        for indicator, weight in english_indicators.items():
            count = text_lower.count(indicator)
            english_score += count * weight

        # Enhanced pattern bonuses
        if re.search(r'che cosa (?:è|sono)\s*assicurat[oi]\?', text_lower):
            italian_score += 30
        if re.search(r'what is (?:insured|covered)\?', text_lower):
            english_score += 30

        # Company-specific patterns with higher weights
        if any(pattern in text_lower for pattern in ['axa', 'inter partner assistance', 'centrale operativa']):
            italian_score += 25
        if any(pattern in text_lower for pattern in ['nobis', 'filo diretto']):
            english_score += 15

        # Enhanced character-level indicators
        italian_chars = (text_lower.count('à') + text_lower.count('è') + text_lower.count('é') +
                         text_lower.count('ì') + text_lower.count('ò') + text_lower.count('ù'))
        italian_score += italian_chars * 8

        # Document structure patterns
        if re.search(r'pag\.\s*\d+\s*di\s*\d+', text_lower):
            italian_score += 15
        if re.search(r'page \d+ of \d+', text_lower):
            english_score += 15

        self.logger.info(f"Enhanced language detection scores - Italian: {italian_score}, English: {english_score}")

        # More decisive thresholds
        if italian_score > english_score * 1.2:
            return 'italian'
        elif english_score > italian_score * 1.2:
            return 'english'
        else:
            # Enhanced tie-breaking with more specific indicators
            specific_italian = any(
                word in text_lower for word in ['assicurato', 'polizza', 'società', 'garanzia', 'che cosa'])
            specific_english = any(word in text_lower for word in ['insured', 'policy', 'company', 'what is'])

            if specific_italian and not specific_english:
                return 'italian'
            elif specific_english and not specific_italian:
                return 'english'
            else:
                return 'italian'  # Default to Italian for insurance docs

    def _estimate_document_readability(self, text: str) -> float:
        """Estimate document readability score."""

        if not text:
            return 0.0

        words = text.split()
        sentences = self._enhanced_split_into_sentences(text)

        if not words or not sentences:
            return 0.0

        avg_words_per_sentence = len(words) / len(sentences)

        # Readability based on sentence length and structure
        if avg_words_per_sentence <= 20:
            readability = 0.9
        elif avg_words_per_sentence <= 30:
            readability = 0.7
        elif avg_words_per_sentence <= 40:
            readability = 0.5
        else:
            readability = 0.3

        # Adjust for document structure
        if re.search(r'^\s*[•\-\*\d+\.\)]\s', text, re.MULTILINE):
            readability += 0.1  # Lists improve readability

        return min(1.0, readability)

    def _estimate_structure_complexity(self, text: str) -> str:
        """Estimate document structure complexity."""

        if not text:
            return "unknown"

        # Count structural elements
        sections = len(re.findall(r'^\s*(?:SECTION|SEZIONE|PARTE|CAPITOLO)', text, re.MULTILINE | re.IGNORECASE))
        articles = len(re.findall(r'^\s*(?:ARTICLE|ARTICOLO|ART\.)', text, re.MULTILINE | re.IGNORECASE))
        questions = len(re.findall(r'[^?]*\?', text))
        lists = len(re.findall(r'^\s*[•\-\*\d+\.\)]\s', text, re.MULTILINE))

        total_elements = sections + articles + questions + lists

        if total_elements >= 20:
            return "high"
        elif total_elements >= 10:
            return "medium"
        elif total_elements >= 5:
            return "low"
        else:
            return "minimal"

    def process_pdf(self, pdf_path: str, auto_detect_profile: bool = True) -> Dict[str, Any]:
        """Enhanced PDF processing with comprehensive quality control and separate output files."""

        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF file not found: {pdf_path}")

        self.logger.info(f"Starting enhanced PDF processing with quality optimization: {pdf_path}")

        doc = fitz.open(str(pdf_path))

        try:
            # Enhanced text extraction
            full_text = self.extract_with_pymupdf4llm(doc)
            self.logger.info(f"Extracted {len(full_text)} characters of text")

            if not full_text.strip():
                raise ValueError("No text content extracted from PDF")

            # Enhanced profile detection
            if auto_detect_profile and full_text:
                detected_profile = self.profile_manager.detect_best_profile(
                    full_text[:8000])  # Use more text for detection
                profile = self.profile_manager.get_profile(detected_profile)
                if profile:
                    self.config.company_profile = profile
                    self.logger.info(f"Applied enhanced profile: {detected_profile}")

            # Extract enhanced metadata
            text_sample = full_text[:5000] if full_text else ""
            metadata = self.extract_document_metadata(doc, text_sample)

            # Extract tables with enhanced processing
            tables_by_page = {}
            total_tables = 0
            for page_num in range(1, doc.page_count + 1):
                tables = self.extract_tables_from_page(str(pdf_path), page_num)
                if tables:
                    tables_by_page[page_num] = tables
                    total_tables += len(tables)

            self.logger.info(f"Extracted {total_tables} tables across {len(tables_by_page)} pages")

            # Create enhanced smart chunks with quality optimization
            document_chunks = self.create_enhanced_smart_chunks(full_text, tables_by_page)

            if not document_chunks:
                raise ValueError("Failed to create any chunks from document")

            # Final validation and quality enhancement
            document_chunks = [chunk for chunk in document_chunks if self._validate_enhanced_chunk(chunk)]

            if not document_chunks:
                raise ValueError("All chunks were filtered out during quality validation")

            # Build comprehensive processing stats
            processing_stats = {
                "total_pages": doc.page_count,
                "total_chunks": len(document_chunks),
                "total_tables": total_tables,
                "detected_language": metadata.get("detected_language", "unknown"),
                "company_profile": metadata.get("company_profile", "unknown"),
                "avg_chunk_size": sum(len(chunk.content) for chunk in document_chunks) // len(
                    document_chunks) if document_chunks else 0,
                "avg_quality_score": sum(chunk.metadata.get('quality_score', 0) for chunk in document_chunks) / len(
                    document_chunks) if document_chunks else 0,
                "high_quality_chunks": sum(
                    1 for chunk in document_chunks if chunk.metadata.get('quality_score', 0) >= 0.8),
                "section_distribution": self._analyze_enhanced_section_distribution(document_chunks),
                "content_type_distribution": self._analyze_content_type_distribution(document_chunks),
                "semantic_type_distribution": self._analyze_semantic_type_distribution(document_chunks),
                "language_distribution": self._analyze_language_distribution(document_chunks),
                "quality_distribution": self._analyze_quality_distribution(document_chunks),
                "tables_per_page": {page: len(tables) for page, tables in tables_by_page.items()},
                "processing_method": "enhanced_quality_optimized_chunking",
                "readability_metrics": {
                    "avg_readability": sum(
                        chunk.metadata.get('readability_score', 0) for chunk in document_chunks) / len(
                        document_chunks) if document_chunks else 0,
                    "chunks_with_complete_concepts": sum(
                        1 for chunk in document_chunks if chunk.metadata.get('is_complete_concept', False)),
                    "chunks_with_context": sum(
                        1 for chunk in document_chunks if chunk.metadata.get('context_preserved', False))
                }
            }

            result = {
                "source_file": str(pdf_path),
                "metadata": metadata,
                "chunks": [chunk.to_dict() for chunk in document_chunks],
                "rag_chunks": [chunk.to_rag_dict() for chunk in document_chunks],
                "tables_by_page": tables_by_page,
                "processing_stats": processing_stats
            }

            self.logger.info(f"✅ Successfully created {len(document_chunks)} enhanced quality-optimized chunks")
            self.logger.info(f"📊 Average quality score: {processing_stats['avg_quality_score']:.2f}")
            self.logger.info(f"🏆 High-quality chunks: {processing_stats['high_quality_chunks']}")

            return result

        finally:
            doc.close()

    def _analyze_enhanced_section_distribution(self, chunks: List[DocumentChunk]) -> Dict[str, int]:
        """Analyze enhanced chunk type distribution."""
        distribution = {}
        for chunk in chunks:
            chunk_type = chunk.chunk_type
            distribution[chunk_type] = distribution.get(chunk_type, 0) + 1
        return distribution

    def _analyze_content_type_distribution(self, chunks: List[DocumentChunk]) -> Dict[str, int]:
        """Analyze content type distribution."""
        distribution = {}
        for chunk in chunks:
            content_type = chunk.metadata.get('content_type', 'unknown')
            distribution[content_type] = distribution.get(content_type, 0) + 1
        return distribution

    def _analyze_semantic_type_distribution(self, chunks: List[DocumentChunk]) -> Dict[str, int]:
        """Analyze semantic type distribution."""
        distribution = {}
        for chunk in chunks:
            semantic_type = chunk.metadata.get('semantic_type', 'unknown')
            distribution[semantic_type] = distribution.get(semantic_type, 0) + 1
        return distribution

    def _analyze_language_distribution(self, chunks: List[DocumentChunk]) -> Dict[str, int]:
        """Analyze language distribution."""
        distribution = {}
        for chunk in chunks:
            language = chunk.metadata.get('language', 'unknown')
            distribution[language] = distribution.get(language, 0) + 1
        return distribution

    def _analyze_quality_distribution(self, chunks: List[DocumentChunk]) -> Dict[str, int]:
        """Analyze quality score distribution."""
        distribution = {
            "excellent (0.9-1.0)": 0,
            "good (0.8-0.9)": 0,
            "fair (0.6-0.8)": 0,
            "poor (0.4-0.6)": 0,
            "very_poor (0.0-0.4)": 0
        }

        for chunk in chunks:
            score = chunk.metadata.get('quality_score', 0.0)
            if score >= 0.9:
                distribution["excellent (0.9-1.0)"] += 1
            elif score >= 0.8:
                distribution["good (0.8-0.9)"] += 1
            elif score >= 0.6:
                distribution["fair (0.6-0.8)"] += 1
            elif score >= 0.4:
                distribution["poor (0.4-0.6)"] += 1
            else:
                distribution["very_poor (0.0-0.4)"] += 1

        return distribution

    def export_enhanced_separate_files(self, result: Dict[str, Any], base_filename: str) -> Dict[str, str]:
        """Export enhanced separate JSON files with comprehensive metadata."""

        # Enhanced RAG-optimized chunks file
        rag_chunks_file = f"{base_filename}_enhanced_rag_chunks.json"
        enhanced_rag_data = {
            "chunks": result["rag_chunks"],
            "processing_info": {
                "total_chunks": len(result["rag_chunks"]),
                "avg_quality_score": result["processing_stats"]["avg_quality_score"],
                "high_quality_chunks": result["processing_stats"]["high_quality_chunks"],
                "processing_method": result["processing_stats"]["processing_method"]
            }
        }
        with open(rag_chunks_file, "w", encoding="utf-8") as f:
            json.dump(enhanced_rag_data, f, indent=2, ensure_ascii=False)

        # Enhanced metadata file
        metadata_file = f"{base_filename}_enhanced_metadata.json"
        metadata_content = {
            "source_file": result["source_file"],
            "document_metadata": result["metadata"],
            "tables_by_page": result["tables_by_page"],
            "quality_metrics": {
                "readability_metrics": result["processing_stats"]["readability_metrics"],
                "quality_distribution": result["processing_stats"]["quality_distribution"]
            }
        }
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata_content, f, indent=2, ensure_ascii=False)

        # Comprehensive stats file
        stats_file = f"{base_filename}_enhanced_stats.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(result["processing_stats"], f, indent=2, ensure_ascii=False)

        # Quality analysis file
        quality_file = f"{base_filename}_quality_analysis.json"
        quality_analysis = {
            "overall_quality": {
                "avg_quality_score": result["processing_stats"]["avg_quality_score"],
                "high_quality_chunks": result["processing_stats"]["high_quality_chunks"],
                "total_chunks": result["processing_stats"]["total_chunks"]
            },
            "quality_distribution": result["processing_stats"]["quality_distribution"],
            "semantic_distribution": result["processing_stats"]["semantic_type_distribution"],
            "readability_metrics": result["processing_stats"]["readability_metrics"],
            "recommendations": self._generate_quality_recommendations(result["processing_stats"])
        }
        with open(quality_file, "w", encoding="utf-8") as f:
            json.dump(quality_analysis, f, indent=2, ensure_ascii=False)

        # Complete result file (legacy compatibility)
        complete_file = f"{base_filename}_complete_enhanced.json"
        with open(complete_file, "w", encoding="utf-8") as f:
            json.dump({
                "source_file": result["source_file"],
                "metadata": result["metadata"],
                "chunks": result["chunks"],
                "tables_by_page": result["tables_by_page"],
                "processing_stats": result["processing_stats"]
            }, f, indent=2, ensure_ascii=False)

        return {
            "rag_chunks": rag_chunks_file,
            "metadata": metadata_file,
            "stats": stats_file,
            "quality_analysis": quality_file,
            "complete": complete_file
        }

    def _generate_quality_recommendations(self, stats: Dict[str, Any]) -> List[str]:
        """Generate quality improvement recommendations based on processing stats."""

        recommendations = []

        avg_quality = stats.get("avg_quality_score", 0.0)
        total_chunks = stats.get("total_chunks", 0)
        high_quality_chunks = stats.get("high_quality_chunks", 0)

        if total_chunks == 0:
            recommendations.append("No chunks were created - check document content and processing parameters")
            return recommendations

        high_quality_ratio = high_quality_chunks / total_chunks

        # Overall quality assessment
        if avg_quality < 0.6:
            recommendations.append(
                "Document has low overall quality - consider adjusting chunk size or reviewing source document")
        elif avg_quality < 0.8:
            recommendations.append("Document has moderate quality - some optimization possible")
        else:
            recommendations.append("Document has good overall quality")

        # High-quality chunk ratio
        if high_quality_ratio < 0.3:
            recommendations.append(
                "Low percentage of high-quality chunks - consider increasing min_chunk_size or improving semantic detection")
        elif high_quality_ratio < 0.6:
            recommendations.append("Moderate percentage of high-quality chunks - fine-tuning recommended")
        else:
            recommendations.append("Good percentage of high-quality chunks")

        # Readability analysis
        readability_metrics = stats.get("readability_metrics", {})
        avg_readability = readability_metrics.get("avg_readability", 0.0)

        if avg_readability < 0.5:
            recommendations.append(
                "Low readability detected - consider content simplification or better sentence boundary detection")
        elif avg_readability < 0.7:
            recommendations.append("Moderate readability - acceptable for technical documents")
        else:
            recommendations.append("Good readability scores")

        # Context preservation
        chunks_with_context = readability_metrics.get("chunks_with_context", 0)
        context_ratio = chunks_with_context / total_chunks if total_chunks > 0 else 0

        if context_ratio < 0.5:
            recommendations.append(
                "Low context preservation - consider enabling semantic overlap or adjusting chunking strategy")
        else:
            recommendations.append("Good context preservation")

        # Semantic distribution
        semantic_dist = stats.get("semantic_type_distribution", {})
        general_chunks = semantic_dist.get("general", 0)
        general_ratio = general_chunks / total_chunks if total_chunks > 0 else 0

        if general_ratio > 0.7:
            recommendations.append(
                "High percentage of 'general' semantic types - consider improving semantic classification patterns")
        elif general_ratio > 0.5:
            recommendations.append("Moderate semantic classification - some improvement possible")
        else:
            recommendations.append("Good semantic classification")

        return recommendations

def main():
    """Enhanced main function with comprehensive quality reporting and separate file outputs."""
    print("=== Enhanced Production-Ready Multi-Language RAG PDF Processor ===")
    print("🚀 Now with Advanced Quality Optimization and Semantic Understanding!")

    # Enhanced production configuration
    config = ProcessingConfig(
        table_accuracy_threshold=75.0,
        preserve_structure=True,
        extract_metadata=True,
        max_chunk_size=1500,  # Increased for better context
        min_chunk_size=150,  # Increased for better quality
        optimal_chunk_size=1000,  # Sweet spot for RAG
        overlap_size=200,  # Enhanced context preservation
        preserve_semantic_units=True,
        maintain_context=True,
        min_quality_score=0.6,  # Quality threshold
        require_complete_sentences=True,
        preserve_lists=True,
        preserve_qa_pairs=True,
        debug_mode=True,
        auto_detect_language=True,
        enhance_readability=True
    )

    preprocessor = EnhancedAdvancedPDFPreprocessor(config)

    # Process your documents
    name = "18_Nobis - Baggage loss EN"  # Update with your filename
    pdf_path = os.path.join(DOCUMENT_DIR, f"{name}.pdf")

    try:
        print(f"\n🔍 Processing: {pdf_path}")
        print("⚙️ Enhanced Quality Optimization Enabled")

        result = preprocessor.process_pdf(pdf_path, auto_detect_profile=True)

        # Export enhanced separate files
        output_files = preprocessor.export_enhanced_separate_files(result, name)

        # Comprehensive processing summary
        stats = result["processing_stats"]

        print(f"\n📊 ENHANCED PROCESSING RESULTS:")
        print(f"  📄 Document: {result['source_file']}")
        print(f"  🏢 Profile: {stats['company_profile']}")
        print(f"  🌍 Language: {stats['detected_language']}")
        print(f"  📑 Pages: {stats['total_pages']}")
        print(f"  🧩 Total Chunks: {stats['total_chunks']}")
        print(f"  📊 Tables: {stats['total_tables']}")
        print(f"  📏 Avg Chunk Size: {stats['avg_chunk_size']} chars")
        print(f"  ⭐ Avg Quality Score: {stats['avg_quality_score']:.2f}")
        print(f"  🏆 High-Quality Chunks: {stats['high_quality_chunks']}/{stats['total_chunks']}")

        print(f"\n📋 CHUNK TYPE DISTRIBUTION:")
        for chunk_type, count in stats['section_distribution'].items():
            print(f"  - {chunk_type}: {count} chunks")

        print(f"\n🏷️ CONTENT TYPE DISTRIBUTION:")
        for content_type, count in stats.get('content_type_distribution', {}).items():
            print(f"  - {content_type}: {count} chunks")

        print(f"\n🧠 SEMANTIC TYPE DISTRIBUTION:")
        for semantic_type, count in stats.get('semantic_type_distribution', {}).items():
            print(f"  - {semantic_type}: {count} chunks")

        print(f"\n🌍 LANGUAGE DISTRIBUTION:")
        for language, count in stats.get('language_distribution', {}).items():
            print(f"  - {language}: {count} chunks")

        print(f"\n📈 QUALITY DISTRIBUTION:")
        for quality_range, count in stats.get('quality_distribution', {}).items():
            print(f"  - {quality_range}: {count} chunks")

        print(f"\n📚 READABILITY METRICS:")
        readability = stats.get('readability_metrics', {})
        print(f"  - Average Readability: {readability.get('avg_readability', 0):.2f}")
        print(f"  - Complete Concepts: {readability.get('chunks_with_complete_concepts', 0)} chunks")
        print(f"  - Context Preserved: {readability.get('chunks_with_context', 0)} chunks")

        print(f"\n📖 SAMPLE ENHANCED QUALITY CHUNKS:")
        # Sort chunks by quality score and show the best ones
        sorted_chunks = sorted(result['rag_chunks'],
                               key=lambda x: x.get('quality_score', 0),
                               reverse=True)

        for i, chunk in enumerate(sorted_chunks[:5]):
            print(f"  {i + 1}. [{chunk['chunk_type'].upper()}] {chunk['title'][:60]}...")
            print(f"     📍 Page {chunk['page_number']} | {chunk['word_count']} words")
            print(f"     ⭐ Quality: {chunk.get('quality_score', 0):.2f}")
            print(f"     🌍 Language: {chunk.get('language', 'unknown')}")
            print(f"     🏷️ Content: {chunk.get('content_type', 'unknown')}")
            print(f"     🧠 Semantic: {chunk.get('semantic_type', 'unknown')}")
            print(f"     🗂️ Path: {' > '.join(chunk['section_path'])}")
            print(f"     📝 Preview: {chunk['content'][:80]}...")
            print()

        # Quality recommendations
        quality_analysis_file = output_files.get('quality_analysis', '')
        if quality_analysis_file and os.path.exists(quality_analysis_file):
            with open(quality_analysis_file, 'r', encoding='utf-8') as f:
                quality_data = json.load(f)
                recommendations = quality_data.get('recommendations', [])

                if recommendations:
                    print(f"💡 QUALITY RECOMMENDATIONS:")
                    for i, rec in enumerate(recommendations, 1):
                        print(f"  {i}. {rec}")

        print(f"\n✅ ENHANCED PROCESSING COMPLETED SUCCESSFULLY!")
        print(
            f"🎯 Quality Optimization: {stats['high_quality_chunks']}/{stats['total_chunks']} high-quality chunks")
        print(f"\n📁 OUTPUT FILES:")
        for file_type, filename in output_files.items():
            print(f"  - {file_type.upper().replace('_', ' ')}: {filename}")

        print(f"\n🚀 Ready for Advanced RAG Applications!")
        print("   - Enhanced semantic understanding")
        print("   - Improved context preservation")
        print("   - Quality-optimized chunking")
        print("   - Multi-language support")

    except Exception as e:
        print(f"❌ Processing failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

# End of preprocessing_pdf.py
# ================================================================================

# File 29/41: prompts/__init__.py
# --------------------------------------------------------------------------------

# src/prompts/__init__.py

"""
Prompts module for insurance policy analysis.
"""

from .insurance_prompts import InsurancePrompts

__all__ = ['InsurancePrompts']


# End of prompts/__init__.py
# ================================================================================

# File 30/41: prompts/insurance_prompts.py
# --------------------------------------------------------------------------------

# src/prompts/insurance_prompts.py

"""
Collection of prompts for insurance policy analysis.
This module provides a structured way to manage different prompt templates
for various insurance analysis tasks.
"""


class InsurancePrompts:
    """
    A collection of prompt templates for insurance policy analysis.

    This class provides various system prompts for different insurance analysis scenarios,
    making it easy to select and modify prompts as needed.
    """

    @classmethod
    def standard_coverage(cls) -> str:
        """
        Standard prompt for determining coverage eligibility and amounts.

        Returns:
            str: The prompt template.
        """
        return """
            You are an expert assistant helping users understand their insurance coverage.
            Given a question and access to a policy document, follow these instructions:

            1. Determine if the case is covered:
               - Answer with one of the following:
                 - "Yes"
                 - "No - Unrelated event"
                 _ "No - condition(s) not met"
            2. If the answer is "Yes" then always:
               - Quote the exact sentence(s) from the policy that support your decision.
               - Quote the exact sentence from the policy that specifies this amount.
            4. Is the answer is "No - Unrelated event":
               - Then it is not necessary to quote anything.
            5. If the answer is "No - condition(s) not met":
               - Then, quote the exact sentence(s) from the policy that justify your decision, 
               don't add extrac content, others characters or punctuations mark that are not in the original policy

            Return the answer in JSON format with the following fields:
            {
              "answer": {
                "eligibility": "Yes | No - Unrelated event | No - condition(s) not met",
                "eligibility_policy": "Quoted text from policy",
                "amount_policy": "Amount like '1000 CHF' or null",
              }
            }
        """

    @classmethod
    def detailed_coverage(cls) -> str:
        """
        Detailed prompt that includes late reporting and multiple insured parties.

        Returns:
            str: The prompt template.
        """
        return """
            You are an expert assistant helping users understand their insurance coverage.
            Given a question, that you need to interpret correctly, and access to a policy document, 
            follow these instructions:
            
            1. First, identify the situation in which the event occurred, determine the individuals affected, 
            and verify whether they are covered under the policy.
            2. Determine if the case is covered:
               - Answer with one of the following:
                 - "Yes"
                 - "No - Unrelated event"
                 _ "No - condition(s) not met"
            3. If the answer is "Yes" then always:
               - Quote the exact sentence(s) from the policy that support your decision.
               - Quote the exact sentence from the policy that specifies this amount.
            4. Is the answer is "No - Unrelated event":
            5. If the answer is "No - condition(s) not met":
                - Then, quote the exact sentence(s) from the policy that justify your decision.

            Return the answer in JSON format with the following fields:
            {
              "answer": {
                "eligibility": "Yes | No - Unrelated event | No - condition(s) not met | Maybe",
                "eligibility_policy": "Quoted text from policy",
                "amount_policy_line": "Quoted policy text or null"
              }
            }
        """

    @classmethod
    def precise_coverage(cls) -> str:
        """
        Improved and more precise prompt for determining coverage eligibility and payout amounts.
        """
        return """
            You are an expert assistant that explains insurance coverage.
            
            ==========  TASKS  ==========
            1. FIND the single policy chapter, section, paragraph, or sentence that matches the user’s event.
            2. DECIDE eligibility:
               • "Yes"
               • "No - Unrelated event"
               • "No - condition(s) not met"
            3. QUOTE policy:
               • If "Yes":   – sentence(s) that grant coverage
                             – sentence(s) that state the amount **(if an amount sentence exists)**
               • If "No - condition(s) not met": quote only the sentence(s) that show the missing condition
               • If "No - Unrelated event": no quote
            4. SANITY CHECK  
               – If you found both a coverage sentence *and* an amount sentence → eligibility must be "Yes".  
               – If you found a coverage sentence but no amount sentence anywhere in the policy → eligibility is still 
                 "Yes" and "amount_policy" must be null.
            5. OUTPUT exactly in the JSON schema below.
            
            ==========  OUTPUT SCHEMA  ==========
            {
              "answer": {
                "eligibility": "...",
                "eligibility_policy": "...",
                "amount_policy": "..."
              }
            }
            
            ==========  EXAMPLE  (follow this layout)  ==========
            User event: “My checked bag never arrived – can I claim?”
            Policy snippet: «In the event that the air carrier fails to deliver ... Option 1 € 150,00 Option 2 € 350,00 ...»
            Expected answer:
            {
              "answer": {
                "eligibility": "Yes",
                "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                "amount_policy": "Option 1 € 150,00 Option 2 € 350,00 Option 3 € 500,00"
              }
            }
            (Do NOT output this example again.)
            
            ==========  REMEMBER  ==========
            • Return *only* valid JSON – no markdown, no explanations.
            • Do NOT invent keys or punctuation not present in the policy.
            • Keep quotes verbatim (no “[…]” ellipses).
        """

    @classmethod
    def precise_coverage_v2(cls) -> str:
        """
        Improved and more precise prompt for determining coverage eligibility and payout amounts.
        """
        return """
                You are an expert assistant that explains insurance coverage.

                ==========  TASKS  ==========
                1. FIND the single policy chapter, section, paragraph, or sentence that matches the user’s event.
                2. DECIDE eligibility:
                   • "Yes"
                   • "No - Unrelated event"
                   • "No - condition(s) not met"
                3. QUOTE policy:
                   • If "Yes":   – sentence(s) that grant coverage
                                 – sentence(s) that state the amount **(if an amount sentence exists)**
                   • If "No - condition(s) not met": quote only the sentence(s) that show the missing condition
                   • If "No - Unrelated event": no quote
                4. SANITY CHECK  
                   – If you found both a coverage sentence *and* an amount sentence → eligibility must be "Yes".  
                   – If you found a coverage sentence but no amount sentence anywhere in the policy → eligibility is still 
                     "Yes" and "amount_policy" must be null.
                5. OUTPUT exactly in the JSON schema below.

                ==========  WHEN DECIDING “condition(s) not met” VS. “Yes”  ==========

                • If the user’s event matches the loss description in a coverage clause:
                    – Check the *same* clause (and any cross-referenced article) for
                      explicit prerequisites, exclusions, or timing limits.
                    – If at least one of those conditions is clearly **not satisfied
                      in the user’s story**, choose "No - condition(s) not met".
                    – Otherwise choose "Yes".

                • Treat a prerequisite as **satisfied by default** when it is
                  *logically inherent* in the event:
                    (e.g. a derouted/lost/late bag was checked in; a cancellation
                    request implies the trip hasn’t started yet; a hospitalised
                    person hasn’t travelled).

                • DO NOT require procedural steps (PIR, police report, 24-h notice, etc.)
                  to be mentioned; assume they can still be provided later unless user
                  admits they didn’t do them.


                ==========  OUTPUT SCHEMA  ==========
                {
                  "answer": {
                    "eligibility": "...",
                    "eligibility_policy": "...",
                    "amount_policy": "..."
                  }
                }

                ==========  EXAMPLE  (follow this layout)  ==========
                User event: “My checked bag never arrived – can I claim?”
                Policy snippet: «In the event that the air carrier fails to deliver ... Indemnity amount for baggage 
                                loss option 1 € 150,00 Option 2 € 350,00 ...»
                Expected answer:
                {
                  "answer": {
                    "eligibility": "Yes",
                    "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                    "amount_policy": "The Insured Person may choose... The Indemnification option selected and 
                        operative will be only the one resulting in the policy certificate according to the following: 
                        Indemnity amount for baggage loss option 1 € 150,00 Option 2 € 350,00 Option 3 € 500,00"
                  }
                }
                (Do NOT output this example again.)

                ==========  REMEMBER  ==========
                • Return *only* valid JSON – no markdown, no explanations.
                • Do NOT invent keys or punctuation not present in the policy.
                • Keep quotes verbatim (no “[…]” ellipses).
            """

    @classmethod
    def precise_coverage_v2_1(cls) -> str:
        """
        Improved and more precise prompt for determining coverage eligibility and payout amounts.
        """
        return """
                You are an expert assistant that explains insurance coverage.

                ==========  TASKS  ==========
                1. FIND the single policy chapter, section, paragraph, or sentence that matches the user’s event.
                2. DECIDE eligibility:
                   • "Yes"
                   • "No - Unrelated event"
                   • "No - condition(s) not met"
                3. QUOTE policy:
                   • If "Yes":   – sentence(s) that grant coverage
                                 – sentence(s) that state the amount **(if an amount sentence exists)**
                   • If "No - condition(s) not met": quote only the sentence(s) that show the missing condition
                   • If "No - Unrelated event": no quote
                4. AMOUNT EXTRACTION RULES:
                   – Look for ANY text containing "€" followed by numbers (e.g., "€ 350", "Fino a € 2.000")
                   – Quote the EXACT text that contains the euro amount
                   – If multiple amounts exist, quote the one most relevant to the user's event
                   – If NO euro amounts exist anywhere, use null
                5. SANITY CHECK  
                   – If you found both a coverage sentence *and* an amount sentence → eligibility must be "Yes".  
                   – If you found a coverage sentence but no amount sentence anywhere in the policy → eligibility is still 
                     "Yes" and "amount_policy" must be null.
                5. OUTPUT exactly in the JSON schema below.
                
                ==========  WHEN DECIDING “condition(s) not met” VS. “Yes”  ==========

                • If the user’s event matches the loss description in a coverage clause:
                    – Check the *same* clause (and any cross-referenced article) for
                      explicit prerequisites, exclusions, or timing limits.
                    – If at least one of those conditions is clearly **not satisfied
                      in the user’s story**, choose "No - condition(s) not met".
                    – Otherwise choose "Yes".
                
                • Treat a prerequisite as **satisfied by default** when it is
                  *logically inherent* in the event:
                    (e.g. a derouted/lost/late bag was checked in; a cancellation
                    request implies the trip hasn’t started yet; a hospitalised
                    person hasn’t travelled).
                
                • DO NOT require procedural steps (PIR, police report, 24-h notice, etc.)
                  to be mentioned; assume they can still be provided later unless user
                  admits they didn’t do them.


                ==========  OUTPUT SCHEMA  ==========
                {
                  "answer": {
                    "eligibility": "...",
                    "eligibility_policy": "...",
                    "amount_policy": "..."
                  }
                }

                ==========  EXAMPLE  (follow this layout)  ==========
                User event: “My checked bag never arrived – can I claim?”
                Policy snippet: «In the event that the air carrier fails to deliver ... Indemnity amount for baggage 
                                loss option 1 € 150,00 Option 2 € 350,00 ...»
                Expected answer:
                {
                  "answer": {
                    "eligibility": "Yes",
                    "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                    "amount_policy": "The Insured Person may choose... The Indemnification option selected and 
                        operative will be only the one resulting in the policy certificate according to the following: 
                        Indemnity amount for baggage loss option 1 € 150,00 Option 2 € 350,00 Option 3 € 500,00"
                  }
                }
                (Do NOT output this example again.)

                ==========  REMEMBER  ==========
                • Return *only* valid JSON – no markdown, no explanations.
                • Do NOT invent keys or punctuation not present in the policy.
                • Keep quotes verbatim (no “[…]” ellipses).
            """

    @classmethod
    def precise_coverage_qwen_v2(cls) -> str:
        """
        Strict compliance prompt for Qwen3-14B that:
        - DEMANDS verbatim policy text copying
        - PROHIBITS any interpretation or paraphrasing
        - ENFORCES machine-readable JSON output
        - RETURNS COMPLETE POLICY SEGMENTS (no truncation)
        """
        return (
            "ROLE: Insurance Policy Compliance Engine\n\n"
            "OPERATING PRINCIPLES:\n"
            "1. You are a policy matching system, NOT an interpreter\n"
            "2. Your output must be legally defensible as direct policy citation\n\n"
            "INPUT PROCESSING:\n"
            "1. Read the claim description\n"
            "2. Scan policy excerpts for LITERAL matches\n"
            "3. Identify the DECISIVE POLICY SEGMENT that conclusively determines coverage\n\n"
            "DECISION REQUIREMENTS:\n"
            "1. Categorize using ONLY these exact phrases:\n"
            "   - \"Yes\" (policy explicitly approves)\n"
            "   - \"No - Unrelated event\" (policy never mentions this event type)\n"
            "   - \"No - condition(s) not met\" (policy mentions but excludes this case)\n"
            "2. Copy the COMPLETE DECISIVE POLICY SEGMENT verbatim (no truncation, no ellipsis)\n"
            "3. Extract monetary amounts EXACTLY as written (\"€500\" not \"500 EUR\")\n\n"
            "OUTPUT SPECIFICATION:\n"
            "{\n"
            "  \"answer\": {\n"
            "    \"eligibility\": \"[exact_phrase]\",\n"
            "    \"eligibility_policy\": \"[complete_verbatim_text_from_policy]\",\n"
            "    \"amount_policy\": [\"exact_amount\"|null]\n"
            "  }\n"
            "}\n\n"
            "COMPLIANCE RULES:\n"
            "1. POLICY TEXT MUST:\n"
            "   - Be copied character-for-character\n"
            "   - Come from the provided excerpts ONLY\n"
            "   - Be enclosed in double quotes\n"
            "   - Include the COMPLETE relevant sentence or clause\n"
            "   - NEVER use [...] or ellipsis or truncation\n"
            "\n"
            "2. AMOUNTS MUST:\n"
            "   - Use original formatting (\"€1,000\" not \"1000\")\n"
            "   - Be null (unquoted) if unspecified\n"
            "\n"
            "3. STRICT PROHIBITIONS:\n"
            "   - NO combining multiple policy segments\n"
            "   - NO explanatory phrases like \"because\" or \"as stated\"\n"
            "   - NO markdown, headings, or whitespace deviations\n"
            "   - NO truncation with [...] or ellipsis\n"
            "   - NO abbreviating or shortening policy text\n\n"
            "VALID OUTPUT EXAMPLES:\n"
            "{\"answer\": {\"eligibility\": \"Yes\", \"eligibility_policy\": \"Baggage loss covered up to the insured amount\", \"amount_policy\": \"€1,200\"}}\n"
            "{\"answer\": {\"eligibility\": \"No - condition(s) not met\", \"eligibility_policy\": \"Excludes pre-existing medical conditions not declared at time of purchase\", \"amount_policy\": null}}\n"
            "{\"answer\": {\"eligibility\": \"No - condition(s) not met\", \"eligibility_policy\": \"which lasts more than 4 hours with respect to the arrival time stipulated in the flight plan\", \"amount_policy\": null}}\n\n"
            "INVALID OUTPUT EXAMPLES:\n"
            "{\"answer\": {\"eligibility\": \"No\", \"eligibility_policy\": \"This isn't covered\"}}  # Paraphrased\n"
            "{\"answer\": {\"eligibility\": \"Yes\", \"eligibility_policy\": \"Covered\"}}  # Too vague\n"
            "{\"answer\": {\"eligibility\": \"Yes\", \"eligibility_policy\": \"Pages 12-14 describe coverage\"}}  # Reference not text\n"
            "{\"answer\": {\"eligibility\": \"No - condition(s) not met\", \"eligibility_policy\": \"delay [...] more than 4 hours\"}}  # INVALID: Contains [...]\n"
        )

    @classmethod
    def precise_coverage_v2_2(cls) -> str:
        """
        Improved and more precise prompt for determining coverage eligibility and payout amounts.
        """
        return """
                    You are an expert assistant that explains insurance coverage.

                    ==========  TASKS  ==========
                    1. FIND the single policy chapter, section, paragraph, or sentence that matches the user’s event.  
                    2. DECIDE eligibility:  
                       • "Yes"  
                       • "No - condition(s) not met"  
                    3. QUOTE policy:  
                       • If "Yes":   – sentence(s) that grant coverage  
                                     – sentence(s) that state the amount **(if an amount sentence exists)**  
                       • If "No - condition(s) not met": quote only the sentence(s) that show the missing condition  
                    4. SANITY CHECK  
                       – If you found both a coverage sentence *and* an amount sentence → eligibility must be **"Yes"**.  
                       – If you found a coverage sentence but no amount sentence anywhere in the policy → eligibility is still **"Yes"** and `"amount_policy"` must be null.
                    
                    ==========  WHEN DECIDING “condition(s) not met” VS. “Yes”  ==========
                    
                    • If the user’s event matches the loss description in a coverage clause:  
                        – Check the *same* clause (and any cross-referenced article) for explicit prerequisites, exclusions, or timing limits.  
                        – If at least one of those conditions is clearly **not satisfied in the user’s story**, choose **"No - condition(s) not met"**.  
                        – Otherwise choose **"Yes"**.
                    
                    • Treat a prerequisite as **satisfied by default** when it is *logically inherent* in the event:  
                      (e.g. a derouted/lost/late bag was checked in; a cancellation request implies the trip hasn’t started yet; a hospitalised person hasn’t travelled).
                    
                    • DO NOT require procedural steps (PIR, police report, 24-h notice, etc.) to be mentioned; assume they can still be provided later unless user admits they didn’t do them.
                    
                    ==========  OUTPUT SCHEMA  ==========
                    {
                      "answer": {
                        "eligibility": "...",
                        "eligibility_policy": "...",
                        "amount_policy": "..."
                      }
                    }
                    
                    ==========  EXAMPLE  (follow this layout)  ==========
                    User event: “My checked bag never arrived – can I claim?”
                    Policy snippet: «In the event that the air carrier fails to deliver ... Indemnity amount for baggage 
                                    loss option 1 € 150,00 Option 2 € 350,00 ...»
                    Expected answer:
                    {
                      "answer": {
                        "eligibility": "Yes",
                        "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                        "amount_policy": "The Insured Person may choose... The Indemnification option selected and 
                            operative will be only the one resulting in the policy certificate according to the following: 
                            Indemnity amount for baggage loss option 1 € 150,00 Option 2 € 350,00 Option 3 € 500,00"
                      }
                    }
                    (Do NOT output this example again.)
                    
                    ==========  REMEMBER  ==========
                    • Return *only* valid JSON – no markdown, no explanations.  
                    • Do NOT invent keys or punctuation not present in the policy.  
                    • Keep quotes verbatim (no “[…]” ellipses).

                """

    @classmethod
    def precise_coverage_v3(cls) -> str:
        """
        Same logic as v2, but explicitly instructs the model to reason
        step-by-step internally (chain-of-thought) while keeping that
        reasoning hidden from the user. Output format is unchanged.
        """
        return r"""
            You are an expert assistant that explains insurance coverage.
        
            ==========  THINK (INTERNAL)  ==========
            First, reason step-by-step through all tasks below.  
            Keep this chain-of-thought strictly internal and **never** reveal it.  
            After finishing your reasoning, produce only the final JSON answer.
        
            ==========  TASKS  ==========
            1. FIND the single policy chapter, section, paragraph, or sentence that matches the user’s event.  
               • **If no clause describes the user’s loss, SKIP to step 5 and set eligibility to "No - Unrelated event".**
            2. DECIDE eligibility:  
               • "Yes"  
               • "No - Unrelated event"  
               • "No - condition(s) not met"
            3. QUOTE policy:  
               • If "Yes":   – sentence(s) that grant coverage  
                             – sentence(s) that state the amount **(if an amount sentence exists)**  
               • If "No - Unrelated event": quote nothing **and leave "amount_policy" null**.  
               • If "No - condition(s) not met": quote only the sentence(s) that show the missing condition
            4. SANITY CHECK  
               – If you found both a coverage sentence *and* an amount sentence → eligibility must be **"Yes"**.  
               – If you found a coverage sentence but no amount sentence anywhere in the policy → eligibility is still **"Yes"** and "amount_policy" must be **null**.  
               – **If you quoted any sentence, eligibility cannot be "No - Unrelated event".**
            5. OUTPUT exactly in the JSON schema below.
        
            ==========  WHEN DECIDING “condition(s) not met” VS. “Yes”  ==========
        
            • If the user’s event matches the loss description in a coverage clause:  
                – Check the *same* clause (and any cross-referenced article) for explicit prerequisites, exclusions, or timing limits.  
                – If at least one of those conditions is clearly **not satisfied in the user’s story**, choose **"No - condition(s) not met"**.  
                – Otherwise choose **"Yes"**.  
        
            • Treat a prerequisite as **satisfied by default** when it is *logically inherent* in the event  
              (e.g. a derouted/lost/late bag was checked in; a cancellation request implies the trip hasn’t started yet; a hospitalised person hasn’t travelled).
        
            • DO NOT require procedural steps (PIR, police report, 24-h notice, etc.) to be mentioned; assume they can still be provided later unless the user admits they didn’t do them.
        
            ==========  OUTPUT SCHEMA  ==========
            {
              "answer": {
                "eligibility": "...",
                "eligibility_policy": "...",
                "amount_policy": "..."
              }
            }
        
            ==========  EXAMPLE  (follow this layout)  ==========
            User event: “My checked bag never arrived – can I claim?”  
            Policy snippet: «In the event that the air carrier fails to deliver ... Indemnity amount for baggage  
                            loss option 1 € 150,00 Option 2 € 350,00 ...»  
            Expected answer:
            {
              "answer": {
                "eligibility": "Yes",
                "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                "amount_policy": "The Insured Person may choose... The Indemnification option selected and 
                    operative will be only the one resulting in the policy certificate according to the following: 
                    Indemnity amount for baggage loss option 1 € 150,00 Option 2 € 350,00 Option 3 € 500,00"
              }
            }
            (Do NOT output this example again.)
        
            ==========  REMEMBER  ==========
            • Return *only* valid JSON – no markdown, no explanations.  
            • Do NOT invent keys or punctuation not present in the policy.  
            • Keep quotes verbatim (no “[…]” ellipses).
    """

    @classmethod
    def precise_coverage_v4(cls) -> str:
        """
        Prompt for deterministic coverage decisions and payout amounts.
        """
        return """
                You are an expert assistant that explains insurance coverage.

                ==========  TASKS  ==========
                1. FIND the single policy chapter, section, paragraph, or sentence that matches the user’s event.
                1.b CHECK that a guarantee actually exists for the user’s risk
                    • If no clause grants this type of benefit → choose "No - Unrelated event".
                1.c CONFIRM the guarantee is active for timing / territory / object
                    • If the guarantee is outside its validity window → "No - Unrelated event".

                2. DECIDE eligibility:            # (use exactly one of)
                   • "Yes"
                   • "No - Unrelated event"
                   • "No - condition(s) not met"

                3. QUOTE policy:
                   • If "Yes":   – sentence(s) that grant coverage
                                 – sentence(s) that state the amount **(if an amount sentence exists)**
                   • If "No - condition(s) not met": quote only the sentence(s) that show the missing condition
                   • If "No - Unrelated event": no quote

                4. SANITY CHECK
                   – If you found both a coverage sentence *and* an amount sentence → eligibility must be "Yes".
                   – If you found a coverage sentence but no amount sentence anywhere in the policy → eligibility is still 
                     "Yes" and "amount_policy" must be null.

                5. OUTPUT exactly in the JSON schema below.

                ==========  WHEN DECIDING “condition(s) not met” VS. “Yes”  ==========

                • If the user’s event matches the loss description in a coverage clause:
                    – Check the *same* clause (and any cross-referenced article) for
                      explicit prerequisites, exclusions, timing limits, territorial limits, sub-limits, or person definitions.
                    – If at least one of those conditions is clearly **not satisfied
                      in the user’s story**, choose "No - condition(s) not met".
                    – Otherwise choose "Yes".

                • Treat a prerequisite as **satisfied by default** when it is
                  *logically inherent* in the event
                  (e.g. a derouted/lost/late bag was checked in; a cancellation
                  request implies the trip hasn’t started yet; a hospitalised
                  person hasn’t travelled).

                • DO NOT require procedural steps (PIR, police report, 24-h notice, etc.)
                  to be mentioned; assume they can still be provided later unless the user
                  admits they didn’t do them.

                ==========  OUTPUT SCHEMA  ==========
                {
                  "answer": {
                    "eligibility": "...",
                    "eligibility_policy": "...",
                    "amount_policy": "..."
                  }
                }

                ==========  EXAMPLE  (positive)  ==========
                User event: “My checked bag never arrived – can I claim?”
                Policy snippet: «In the event that the air carrier fails to deliver ... Indemnity amount for baggage 
                                loss option 1 € 150,00 Option 2 € 350,00 ...»
                Expected answer:
                {
                  "answer": {
                    "eligibility": "Yes",
                    "eligibility_policy": "In the event that the air carrier fails to deliver the Insured's Baggage ...",
                    "amount_policy": "The Insured Person may choose... Indemnity amount for baggage loss option 1 € 150,00 ..."
                  }
                }

                ==========  EXAMPLE  (negative – guarantee expired)  ==========
                User event: “I am already abroad and my colleague at home was hospitalised; can I claim the unused nights?”
                Policy snippet: «The 'Trip Cancellation' guarantee ... starts at booking and ends when the Insured begins to use the first service ...»
                Expected answer:
                {
                  "answer": {
                    "eligibility": "No - Unrelated event",
                    "eligibility_policy": "",
                    "amount_policy": null
                  }
                }

                (Do NOT output these examples again.)

                ==========  REMEMBER  ==========
                • Return *only* valid JSON – no markdown, no explanations.
                • Do NOT invent keys or punctuation not present in the policy.
                • Keep quotes verbatim (no “[…]” ellipses).
            """

    @classmethod
    def relevance_filter_v1(cls) -> str:
        """
        Prompt optimised for Microsoft Phi-4.
        Determines whether a user question is completely unrelated to the policy.
        """
        return """
                You are an INSURANCE-POLICY RELEVANCE FILTER.

                ==========  TASKS  ==========
                1. DECIDE whether the user’s question is **COMPLETELY UNRELATED** to the coverage topics in the policy text.
                   • Focus only on the high-level type of loss/event (baggage, trip cancellation, medical, rental car, etc.).
                   • Ignore exclusions, sub-limits, conditions, dates, wording quirks.
                   • If the question mentions any loss/event that the policy covers → it is RELATED.
                2. GIVE a brief one-sentence reason.

                ==========  OUTPUT  ==========
                Return exactly this JSON:
                {
                  "is_relevant": true/false,
                  "reason": "Brief explanation (≤ 25 words)"
                }

                ==========  EXAMPLES  ==========
                • Policy covers baggage loss  
                  Question: “Will you pay for overseas hospital bills?”  
                  ⇒ { "is_relevant": false, "reason": "Hospital bills are medical expenses; policy covers only baggage loss." }

                • Policy covers trip cancellation  
                  Question: “My rental car got scratched—am I covered?”  
                  ⇒ { "is_relevant": false, "reason": "Rental-car damage is unrelated to trip cancellation coverage." }

                • Policy covers baggage loss  
                  Question: “My suitcase was stolen from the taxi—can I claim?”  
                  ⇒ { "is_relevant": true, "reason": "Stolen baggage is a form of baggage loss covered by the policy." }

                (Do NOT output these examples again.)

                ==========  REMEMBER  ==========
                • Output **only** the JSON—no markdown, no extra commentary.
                • Use lowercase true/false.
                • Keep the reason short and specific.
            """

    @classmethod
    def relevance_filter_v2(cls) -> str:
        """
        Prompt optimised for Microsoft Phi-4.
        Determines whether a user question is completely unrelated to the policy.
        """
        return """
                    You are an **INSURANCE-POLICY RELEVANCE FILTER**.
                    
                    ====================  TASK  ====================
                    Decide if the user’s question is ABOUT a loss/event type that the policy covers.
                    
                    • Work ONLY at the **high-level category**: baggage loss, trip cancellation, medical costs, rental-car damage, personal liability, etc.  
                    • DO NOT worry about:
                      – how the loss happened (airport vs. taxi vs. hotel)  
                      – exclusions, sub-limits, dates, conditions, documents, deductibles  
                      – whether the policy will actually pay.  
                    • If the question involves ANY loss/event type that appears in the policy text → it is **RELATED**.
                    
                    ====================  OUTPUT  ==================
                    Return exactly this JSON (no markdown, no extra words):
                    
                    {
                      "is_relevant": true/false,
                      "reason": "Brief ≤ 25 words explaining the category match or mismatch"
                    }
                    
                    ====================  EXAMPLES  ================
                    • Policy section “Baggage loss or delay”  
                      Q: “My suitcase was stolen from a taxi—can I claim?”  
                      → { "is_relevant": true, "reason": "Baggage theft is a type of baggage loss mentioned in the policy." }
                    
                    • Policy section “Trip cancellation”  
                      Q: “I broke my leg abroad; will you cover hospital bills?”  
                      → { "is_relevant": false, "reason": "Hospital bills are medical expenses, not trip cancellation." }
                    
                    • Policy section “Medical expenses abroad”  
                      Q: “Airline lost my snowboard—am I covered?”  
                      → { "is_relevant": false, "reason": "Baggage loss is not a medical-expense event." }
                    
                    (Do NOT repeat these examples in your answer.)
                                    """

    @classmethod
    def get_prompt(cls, prompt_name: str) -> str:
        """
        Get a specific prompt by name.

        Args:
            prompt_name: Name of the prompt to retrieve

        Returns:
            The prompt template string

        Raises:
            ValueError: If the prompt name is not found
        """
        prompt_map = {
            "standard": cls.standard_coverage(),
            "detailed": cls.detailed_coverage(),
            "precise": cls.precise_coverage(),
            "precise_v2": cls.precise_coverage_v2(),
            "precise_v2_1": cls.precise_coverage_v2_1(),
            "precise_v2_2": cls.precise_coverage_v2_2(),
            "precise_v3": cls.precise_coverage_v3(),
            "precise_v4": cls.precise_coverage_v4(),
            "relevance_filter_v1": cls.relevance_filter_v1(),
            "relevance_filter_v2": cls.relevance_filter_v2(),
            "precise_v2_qwen": cls.precise_coverage_qwen_v2()
        }

        if prompt_name not in prompt_map:
            raise ValueError(f"Prompt '{prompt_name}' not found. Available prompts: {', '.join(prompt_map.keys())}")

        return prompt_map[prompt_name]



# End of prompts/insurance_prompts.py
# ================================================================================

# File 31/41: rag_runner.py
# --------------------------------------------------------------------------------

# src/rag_runner.py

import logging
from typing import Optional, Tuple, List

from config import *
from models.factory import get_model_client, get_shared_relevance_client
from utils import read_questions, list_policy_paths
from models.vector_store import LocalVectorStore
from output_formatter import extract_policy_id, format_results_as_json, save_policy_json, create_model_specific_output_dir
from prompts.insurance_prompts import InsurancePrompts

logger = logging.getLogger(__name__)


def get_vector_store(strategy: str, model_name: str):
    """Factory function to get appropriate vector store based on strategy."""

    if strategy == "simple":
        return LocalVectorStore(model_name=model_name)

    # elif strategy == "enhanced":
    #     from models.vector_store import EnhancedLocalVectorStore
    #     return EnhancedLocalVectorStore(
    #         model_name=model_name,
    #         chunking_strategy="structural",
    #         chunking_config={"max_length": 1000, "preserve_sections": True}
    #     )
    #
    # elif strategy == "hybrid":
    #     # Could combine multiple approaches
    #     return HybridVectorStore(model_name=model_name)

    else:
        raise ValueError(f"Unknown RAG strategy: {strategy}")

def check_query_relevance(
        question: str,
        context_chunks: List[str],
        relevance_client
) -> Tuple[bool, str]:
    """
    Check if a query is relevant to the policy content.

    Args:
        question: The user query
        context_chunks: Retrieved context chunks from the policy
        relevance_client: Model client initialized with the relevance filter prompt

    Returns:
        Tuple of (is_relevant, reason)
    """
    try:
        # Use the relevance client to query
        response = relevance_client.query(question, context_files=context_chunks)

        # Extract the relevance information from the response
        if isinstance(response, dict):
            is_relevant = response.get("is_relevant", True)  # Default to True if key missing
            reason = response.get("reason", "No reason provided")
            return bool(is_relevant), str(reason)  # Ensure proper types
        else:
            logger.warning(f"Unexpected response format from relevance check: {response}")
            return True, "Unexpected response format, assuming relevant"

    except Exception as e:
        logger.warning(f"Error in relevance check: {e}, assuming relevant")
        return True, "Error in relevance check, assuming relevant"

def run_rag(
        model_provider: str = "openai",
        model_name: str = "gpt-4o",
        max_questions: Optional[int] = None,
        output_dir: Optional[str] = None,
        prompt_name: str = "standard",
        use_persona: bool = False,
        question_ids: Optional[list] = None,
        policy_id: Optional[str] = None,
        k: int = 3,
        filter_irrelevant: bool = False,
        relevance_prompt_name: str = "relevance_filter_v1",
        rag_strategy: str = "simple",
) -> None:
    """
    Executes the RAG pipeline using a modular model client, either OpenAI or HuggingFace.
    Now generates a JSON file for each policy with question results.

    Args:
        model_provider (str): One of "openai" or "hf" (Hugging Face).
        model_name (str): Model name (e.g., "gpt-4o" or "microsoft/phi-4").
        max_questions (Optional[int]): Maximum number of questions to process (None = all questions).
        output_dir (Optional[str]): Directory to save JSON output files.
        prompt_name (str): Name of the prompt template to use.
        use_persona (bool): Whether to use persona extraction for the queries (default: False).
        question_ids (Optional[list]): List of question IDs to process (None = all questions).
        policy_id (Optional[str]): Filter to only process a specific policy ID (None = all policies).
        k (int): Number of context chunks to retrieve for each question (default: 3).
        filter_irrelevant (bool): Whether to filter out irrelevant context chunks (default: False).
        relevance_prompt_name (str): Name of the prompt template to use for relevance filtering.
        rag_strategy (str): Name of the approach strategy
    """
    # Select the prompt template
    try:
        # Get the prompt by name from our InsurancePrompts class
        sys_prompt = InsurancePrompts.get_prompt(prompt_name)
        logger.info(f"Using prompt template: {prompt_name}")
    except ValueError as e:
        # If prompt not found, fall back to standard prompt
        logger.warning(f"Prompt selection error: {str(e)}. Falling back to standard prompt.")
        sys_prompt = InsurancePrompts.standard_coverage()

    # Create output directory for JSON files
    if output_dir is None:
        output_dir = os.path.join(base_dir, JSON_PATH)

    model_output_dir = create_model_specific_output_dir(output_dir, model_name)
    logger.info(f"JSON output will be saved to: {model_output_dir}")

    # List all policy PDFs
    pdf_paths = list_policy_paths(DOCUMENT_DIR)
    if not pdf_paths:
        logger.error("No PDF policies found in directory")
        return
    # Filter policies by ID if specified
    if policy_id:
        filtered_paths = []
        for path in pdf_paths:
            if extract_policy_id(path) == policy_id:
                filtered_paths.append(path)
                logger.info(f"Found policy with ID {policy_id}: {os.path.basename(path)}")

        if not filtered_paths:
            logger.warning(f"No policy found with ID {policy_id}. Check if the policy exists and ID is correct.")
            return

        pdf_paths = filtered_paths
        logger.info(f"Filtered to {len(pdf_paths)} policies matching ID {policy_id}")

    # Read questions (use max_questions if provided, otherwise use all questions)
    questions_df = read_questions(DATASET_PATH)

    # First filter by question_ids if provided
    if question_ids:
        questions_df = questions_df[questions_df["Id"].astype(str).isin(question_ids)]
        logger.info(f"Filtered to {len(questions_df)} questions by ID: {', '.join(question_ids)}")

    # Then apply max_questions limit
    if max_questions is not None:
        # Limit to specified number of questions
        questions = questions_df[["Id", "Questions"]].to_numpy()[:max_questions]
        logger.info(f"Processing {len(questions)} out of {len(questions_df)} questions (limited by --num-questions)")
    else:
        # Use all questions (or all filtered by ID)
        questions = questions_df[["Id", "Questions"]].to_numpy()
        logger.info(f"Processing all {len(questions)} questions")

    # Process each policy
    for pdf_path in pdf_paths:
        policy_id = extract_policy_id(pdf_path)
        logger.info(f"Processing policy ID: {policy_id} from file: {os.path.basename(pdf_path)}")

        # Initialize the model client
        model_client = get_model_client(model_provider, model_name, sys_prompt)

        # Initialize relevance checking client if filtering is enabled
        relevance_client = None
        if filter_irrelevant:
            try:
                relevance_prompt = InsurancePrompts.get_prompt(relevance_prompt_name)
                # Use shared model client to avoid loading a second model instance
                relevance_client = get_shared_relevance_client(model_client, relevance_prompt)
                logger.info(f"Relevance filtering enabled - using shared model with prompt: {relevance_prompt_name}")
            except ValueError as e:
                logger.warning(f"Relevance prompt selection error: {str(e)}. Falling back to relevance_filter_v1.")
                relevance_prompt = InsurancePrompts.relevance_filter_v1()
                relevance_client = get_shared_relevance_client(model_client, relevance_prompt)

        # Initialize vector store with just this policy file
        try:
            logger.info(f"Initializing vector store for policy: {policy_id}")
            context_provider = get_vector_store(rag_strategy, EMBEDDING_MODEL_PATH)
            context_provider.index_documents([pdf_path])
            logger.info(f"Vector store initialized successfully for policy: {policy_id}")
        except Exception as e:
            logger.error(f"Error initializing vector store for policy {policy_id}: {e}")
            logger.info("Continuing without vector store")
            context_provider = None

        # Process all questions for this policy
        policy_results = []

        for q_id, question in questions:
            try:
                logger.info(f"→ Querying policy {policy_id} with question {q_id}: {question}")

                # Get relevant context if vector store is available
                context_texts = []
                if context_provider:
                    try:
                        context_texts = context_provider.retrieve(question, k=k)
                        logger.info(f"Retrieved {len(context_texts)} context chunks")
                    except Exception as e:
                        logger.error(f"Error retrieving context: {e}")

                # Only check relevance if filtering is enabled and we have context chunks
                if filter_irrelevant and relevance_client and context_texts:
                    # First assess query relevance using the lightweight filter
                    is_relevant, reason = check_query_relevance(question, context_texts, relevance_client)

                    if not is_relevant:
                        logger.info(f"✓ Question {q_id} marked as IRRELEVANT: {reason}")
                        # Create standardized "irrelevant" response and skip main processing
                        result_row = [
                            model_name,
                            str(q_id),
                            question,
                            "No - Unrelated event",
                            "",
                            None,
                        ]
                        policy_results.append(result_row)
                        continue  # Skip to next question
                    else:
                        logger.info(
                            f"✓ Question {q_id} IS RELEVANT: {reason} - proceeding with detailed analysis")

                # Query the model with the question and context
                response = model_client.query(question, context_files=context_texts, use_persona=use_persona)

                result_row = [
                    model_name,
                    str(q_id),
                    question,
                    response.get("answer", {}).get("eligibility", ""),
                    response.get("answer", {}).get("eligibility_policy", ""),
                    response.get("answer", {}).get("amount_policy", ""),
                ]
                policy_results.append(result_row)
                logger.info(f"✓ Processed question {q_id} for policy {policy_id}")
            except Exception as e:
                logger.error(f"✗ Error processing question {q_id} for policy {policy_id}: {e}")
                policy_results.append([model_name, str(q_id), question, "Error", str(e), "", ""])

        # Format and save policy JSON
        policy_json = format_results_as_json(pdf_path, policy_results)
        save_policy_json(policy_json, output_dir)

    logger.info("✅ RAG run completed successfully.")


def run_batch_rag(
        model_provider: str = "openai",
        model_name: str = "gpt-4o",
        max_questions: Optional[int] = None,
        output_dir: Optional[str] = None,
        prompt_name: str = "standard",
        use_persona: bool = False,
        question_ids: Optional[list] = None,
        policy_id: Optional[str] = None,
        k: int = 3,
        filter_irrelevant: bool = False,
        relevance_prompt_name: str = "relevance_filter_v1",
) -> None:
    """
    Alternative implementation that processes all policies together and then
    organizes results into separate JSON files.

    Args:
        model_provider (str): One of "openai" or "hf" (Hugging Face).
        model_name (str): Model name (e.g., "gpt-4o" or "microsoft/phi-4").
        max_questions (Optional[int]): Maximum number of questions to process (None = all questions).
        output_dir (Optional[str]): Directory to save JSON output files.
        prompt_name (str): Name of the prompt template to use.
        use_persona (bool): Whether to use persona extraction for the queries (default: False).
        question_ids (Optional[list]): List of question IDs to process (None = all questions).
        policy_id (Optional[str]): Filter to only process a specific policy ID (None = all policies).
        k (int): Number of context chunks to retrieve for each question (default: 3).
        filter_irrelevant (bool): Whether to filter out irrelevant context chunks (default: False).
        relevance_prompt_name (str): Name of the prompt template to use for relevance filtering.
    """
    # Select the prompt template
    try:
        # Get the prompt by name from our InsurancePrompts class
        sys_prompt = InsurancePrompts.get_prompt(prompt_name)
        logger.info(f"Using prompt template: {prompt_name}")
    except ValueError as e:
        # If prompt not found, fall back to standard prompt
        logger.warning(f"Prompt selection error: {str(e)}. Falling back to standard prompt.")
        sys_prompt = InsurancePrompts.standard_coverage()

    # Initialize the model client
    model_client = get_model_client(model_provider, model_name, sys_prompt)

    # Initialize relevance checking client if filtering is enabled
    relevance_client = None
    if filter_irrelevant:
        try:
            relevance_prompt = InsurancePrompts.get_prompt(relevance_prompt_name)
            # Use shared model client to avoid loading a second model instance
            relevance_client = get_shared_relevance_client(model_client, relevance_prompt)
            logger.info(f"Relevance filtering enabled - using shared model with prompt: {relevance_prompt_name}")
        except ValueError as e:
            logger.warning(f"Relevance prompt selection error: {str(e)}. Falling back to relevance_filter_v1.")
            relevance_prompt = InsurancePrompts.relevance_filter_v1()
            relevance_client = get_shared_relevance_client(model_client, relevance_prompt)

    # List all policy PDFs
    pdf_paths = list_policy_paths(DOCUMENT_DIR)

    # Filter policies by ID if specified
    if policy_id:
        filtered_paths = []
        for path in pdf_paths:
            if extract_policy_id(path) == policy_id:
                filtered_paths.append(path)
                logger.info(f"Found policy with ID {policy_id}: {os.path.basename(path)}")

        if not filtered_paths:
            logger.warning(f"No policy found with ID {policy_id}. Check if the policy exists and ID is correct.")
            return

        pdf_paths = filtered_paths
        logger.info(f"Filtered to {len(pdf_paths)} policies matching ID {policy_id}")

    # Structure to store results by policy
    policy_results = {}

    # Create output directory for JSON files
    if output_dir is None:
        output_dir = os.path.join(base_dir, "resources/results/json_output")

    model_output_dir = create_model_specific_output_dir(output_dir, model_name)
    logger.info(f"JSON output will be saved to: {model_output_dir}")

    # Initialize and populate the vector store with all PDFs
    context_provider = None
    if pdf_paths:
        logger.info(f"Initializing vector store with {len(pdf_paths)} PDF documents")
        try:
            context_provider = LocalVectorStore(model_name=EMBEDDING_MODEL_PATH)
            context_provider.index_documents(pdf_paths)
            logger.info(f"Vector store initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing vector store: {e}")
            logger.info("Continuing without vector store")

    questions_df = read_questions(DATASET_PATH)
    # First filter by question_ids if provided
    if question_ids:
        questions_df = questions_df[questions_df["Id"].astype(str).isin(question_ids)]
        logger.info(f"Filtered to {len(questions_df)} questions by ID: {', '.join(question_ids)}")

    # Then apply max_questions limit
    if max_questions is not None:
        # Limit to specified number of questions
        questions = questions_df[["Id", "Questions"]].to_numpy()[:max_questions]
        logger.info(f"Processing {len(questions)} out of {len(questions_df)} questions (limited by --num-questions)")
    else:
        # Use all questions (or all filtered by ID)
        questions = questions_df[["Id", "Questions"]].to_numpy()
        logger.info(f"Processing all {len(questions)} questions")

    # Initialize policy result dictionaries
    for pdf_path in pdf_paths:
        policy_id = extract_policy_id(pdf_path)
        policy_results[policy_id] = {
            "policy_path": pdf_path,
            "results": []
        }

    # Process all questions for all policies
    for q_id, question in questions:
        try:
            logger.info(f"→ Querying: {question}")

            # Process each policy separately for this question
            for pdf_path in pdf_paths:
                policy_id = extract_policy_id(pdf_path)
                logger.info(f"  Processing policy {policy_id}")

                # Get relevant context for this policy
                context_texts = []
                if context_provider:
                    try:
                        # Try to retrieve context specifically for this policy
                        context_texts = context_provider.retrieve(query=question, k=k, policy_id=policy_id)
                        logger.info(f"Retrieved {len(context_texts)} context chunks for policy {policy_id}")
                    except Exception as e:
                        logger.error(f"Error retrieving context: {e}")

                # Only check relevance if filtering is enabled and we have context chunks
                if filter_irrelevant and relevance_client and context_texts:
                    # First assess query relevance using the lightweight filter
                    is_relevant, reason = check_query_relevance(question, context_texts, relevance_client)

                    if not is_relevant:
                        logger.info(f"✓ Question {q_id} marked as IRRELEVANT for policy {policy_id}: {reason}")
                        # Create standardized "irrelevant" response and skip main processing
                        result_row = [
                            model_name,
                            str(q_id),
                            question,
                            "No - Unrelated event",
                            "",
                            None,
                        ]
                        policy_results[policy_id]["results"].append(result_row)
                        continue  # Skip to next policy for this question
                    else:
                        logger.info(
                            f"✓ Question {q_id} IS RELEVANT for policy {policy_id}: {reason} - proceeding with detailed analysis")

                # Query the model with the question and context
                response = model_client.query(question, context_files=context_texts, use_persona=use_persona)
                result_row = [
                    model_name,
                    str(q_id),
                    question,
                    response.get("answer", {}).get("eligibility", ""),
                    response.get("answer", {}).get("eligibility_policy", ""),
                    response.get("answer", {}).get("amount_policy", "")
                ]
                policy_results[policy_id]["results"].append(result_row)
                logger.info(f"✓ Processed question {q_id} for policy {policy_id}")
        except Exception as e:
            logger.error(f"✗ Error processing question {q_id}: {e}")
            # Add error result to all policies
            for policy_id in policy_results:
                policy_results[policy_id]["results"].append([model_name, str(q_id), question, "Error", str(e), "", ""])

    # Format and save results for each policy
    for policy_id, data in policy_results.items():
        policy_json = format_results_as_json(data["policy_path"], data["results"])
        save_policy_json(policy_json, output_dir, model_name)

    logger.info("✅ RAG run completed successfully.")


# End of rag_runner.py
# ================================================================================

# File 32/41: scripts/__init__.py
# --------------------------------------------------------------------------------



# End of scripts/__init__.py
# ================================================================================

# File 33/41: scripts/clean_outcome_justification.py
# --------------------------------------------------------------------------------

import fitz
import json
import os
import pandas as pd
import re

from config import DOCUMENT_DIR, JSON_PATH

NB_CONSECUTIVE_TOKENS = 5
AITIS_DELIMITER = '[AITIS-DELIMITER]'


def extract_policy_text(pdf_path):
    text = ""
    document = fitz.open(pdf_path)

    for page in document:
        page_text = page.get_text('text')
        if page_text:  # Avoid appending None if extraction fails
            text += page_text

    return text


def sanitize_text(text: str, is_llm_generated: bool = False):
    sanitized_text = text.replace("\n", " ")              # Replace the end of line by a space
    sanitized_text = re.sub(r'\s+', ' ', sanitized_text)    # Remove unnecessary white spaces

    if(is_llm_generated):
        sanitized_text = sanitized_text.replace('"*', '')
        sanitized_text = sanitized_text.replace('*"', '')
        sanitized_text = sanitized_text.replace('(**', '')
        sanitized_text = sanitized_text.replace('**)', '')
        # sanitized_text = sanitized_text.replace('> **', '')
        # sanitized_text = sanitized_text.replace('> -', '-')
        # sanitized_text = sanitized_text.replace('"', '')
        sanitized_text = sanitized_text.replace('**', '')
        # sanitized_text = sanitized_text.replace('*', ' ')
        # sanitized_text = sanitized_text.replace('"', ' ')
        # sanitized_text = sanitized_text.replace('(', ' ')
        # sanitized_text = sanitized_text.replace(')', ' ')
        # sanitized_text = sanitized_text.replace('>', ' ')
        sanitized_text = re.sub(r'\s+', ' ', sanitized_text)    # Remove unnecessary white spaces

    return sanitized_text


def tokenize_text(text: str):
    return text.split()


def escape_postgres_text(value):
    if isinstance(value, str):
        value = value.replace("'", "''")  # escape single quotes
        return f"E'{value}'"  # use PostgreSQL's E'' notation for special characters
    return value


def extract_ground_truth_text(reference_text: str, mixed_text: str):
    reference_text_sanitized  = sanitize_text(reference_text)
    mixed_text_sanitized      = sanitize_text(mixed_text, is_llm_generated=True)

    reference_tokens          = tokenize_text(reference_text_sanitized)
    mixed_tokens              = tokenize_text(mixed_text_sanitized)

    truth_text                = ' '.join(reference_tokens)

    column_names              = []
    for i in range(NB_CONSECUTIVE_TOKENS):
        column_names.append('Token_' + str(i))

    sliding_window            = []
    for i in range(len(mixed_tokens)):
        if i >= len(mixed_tokens) - NB_CONSECUTIVE_TOKENS + 1:
            break

        else:
            sliding_window.append(mixed_tokens[i:i+NB_CONSECUTIVE_TOKENS])

    mixed_tokens_df = pd.DataFrame(sliding_window, columns=[column_names])
    mixed_tokens_df['Consecutive_Tokens_Found'] = False

    for index, row in mixed_tokens_df.iterrows():
        searched_text = ''

        for i in range(NB_CONSECUTIVE_TOKENS):
            searched_text = ' '.join([searched_text, row['Token_' + str(i)]])

        matches = truth_text.count(searched_text)
        if (matches):
            mixed_tokens_df.at[index, 'Consecutive_Tokens_Found'] = True

    # Detect consecutive sequences in the mixed_text
    sequences = []
    in_sequence = False
    current_tokens = []

    for i, row in mixed_tokens_df.iterrows():
        if row["Consecutive_Tokens_Found"]:
            if not in_sequence:
                # Start of a new valid sequence
                in_sequence = True
                current_tokens = list(row[f'Token_{j}'] for j in range(NB_CONSECUTIVE_TOKENS))
            else:
                # Continue sequence: add only the last token to avoid overlap
                current_tokens.append(row[f'Token_{NB_CONSECUTIVE_TOKENS - 1}'])
        else:
            if in_sequence:
                # End of current sequence
                sequences.append(' '.join(current_tokens))
                current_tokens = []
                in_sequence = False

    # Catch final sequence if it ends at the last row
    if in_sequence:
        sequences.append(' '.join(current_tokens))

    # Return found sequences separated with the delimiter
    corrected_text = AITIS_DELIMITER.join(sequences)
    return corrected_text


def main():
    # Resolve the path relative to this script file
    POLICY_ID =                 18
    POLICY_PDF_DIRECTORY_PATH = DOCUMENT_DIR
    POLICY_PDF_FILENAME =       '18_Nobis - Baggage loss EN.pdf'
    llm_responses_file_path =   os.path.join(JSON_PATH,f'policy_id-{POLICY_ID}__21-05-2025_15-03-07.json')
    output_path =               os.path.join(JSON_PATH, f'policy_{POLICY_ID}_cleanup_21-05-2025_15-03-07.json')
    policy_pdf_path =           os.path.join(POLICY_PDF_DIRECTORY_PATH, POLICY_PDF_FILENAME)

    # Load the JSON file
    with open(llm_responses_file_path, 'r', encoding='utf-8') as json_file:
        data = json.load(json_file)

    new_data = {
        'policy_id': data['policy_id'],
        'questions': []
    }

    # Extract the text from the pdf file associated to the specified policy
    policy_text = extract_policy_text(policy_pdf_path)

    for question in data['questions']:
        new_question = question.copy()
        outcome_justification = question.get('outcome_justification')
        payment_justification = question.get('payment_justification')
        request_id = question.get('request_id')
        if outcome_justification:
            new_question['outcome_justification'] = extract_ground_truth_text(policy_text, outcome_justification)
        if payment_justification:
            new_question['payment_justification'] = extract_ground_truth_text(policy_text, payment_justification)
        new_data['questions'].append(new_question)

    # Write to new JSON file with the cleanup justifications
    with open(output_path, 'w', encoding='utf-8') as outfile:
        json.dump(new_data, outfile, indent=2, ensure_ascii=False)

    # with open(f'policy_{POLICY_ID}_pdf_text.txt', 'w', encoding='utf-8') as file:
    #     file.write(sanitize_text(policy_text))


if __name__ == '__main__':
    main()


# End of scripts/clean_outcome_justification.py
# ================================================================================

# File 34/41: scripts/evaluate_results.py
# --------------------------------------------------------------------------------

# scripts/evaluate_results.py

import os
import json
import re

import editdistance
import pandas as pd
import numpy as np
import argparse
from datetime import datetime
from typing import Dict, List, Tuple
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import JSON_PATH, GT_PATH, EVALUATION_RESULTS_FILES_PATH


def normalize_field(value):
    """Normalize a field value to handle None, empty strings, and whitespace consistently."""
    if value is None:
        return None
    if isinstance(value, str):
        value = value.strip()
        if value == "" or value.lower() in ["null", "none"]:
            return None
    return value


def calculate_similarity_score(str1, str2):
    """Calculate similarity score between two strings (1 = identical, 0 = completely different)."""
    str1 = normalize_field(str1)
    str2 = normalize_field(str2)

    if str1 is None and str2 is None:
        return 1.0
    if str1 is None:
        str1 = ""
    if str2 is None:
        str2 = ""

    str1 = str(str1)
    str2 = str(str2)

    # Calculate edit distance
    distance = editdistance.eval(str1, str2)

    max_len = max(len(str1), len(str2))
    if max_len == 0:
        return 1.0

    return 1.0 - (distance / max_len)


def calculate_text_iou(text1, text2):
    """Calculate Intersection over Union (IoU) for text strings based on word sets."""
    text1 = normalize_field(text1)
    text2 = normalize_field(text2)

    if text1 is None:
        text1 = ""
    if text2 is None:
        text2 = ""

    if text1 == "" and text2 == "":
        return 1.0

    if (text1 == "" and text2 != "") or (text1 != "" and text2 == ""):
        return 0.0

    def tokenize(text):
        tokens = re.findall(r'\b\w+\b', text.lower())
        return set(tokens)

    tokens1 = tokenize(str(text1))
    tokens2 = tokenize(str(text2))

    intersection = tokens1.intersection(tokens2)
    union = tokens1.union(tokens2)

    if len(union) == 0:
        return 1.0

    return len(intersection) / len(union)


def numpy_to_python(obj):
    """Convert numpy types to native Python types for JSON serialization."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: numpy_to_python(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [numpy_to_python(item) for item in obj]
    return obj


def get_model_directories(json_path: str) -> List[str]:
    """Get all model directories from the JSON output path."""
    if not os.path.exists(json_path):
        return []

    model_dirs = []
    for item in os.listdir(json_path):
        item_path = os.path.join(json_path, item)
        if os.path.isdir(item_path):
            # Check if directory contains policy output files
            files = os.listdir(item_path)
            if any(f.startswith("policy_id-") and f.endswith(".json") for f in files):
                model_dirs.append(item)

    return sorted(model_dirs)


def get_latest_output_files(model_dir_path: str) -> List[str]:
    """Find the most recent output file for each policy ID in a model directory."""
    if not os.path.exists(model_dir_path):
        return []

    all_files = [f for f in os.listdir(model_dir_path)
                 if f.startswith("policy_id-") and f.endswith(".json")]

    # Group files by policy ID
    policy_files = {}
    for file in all_files:
        # Extract policy ID from filename: policy_id-{id}__{model}__{timestamp}.json
        parts = file.split("__")
        if len(parts) >= 2:
            policy_id_part = parts[0]
            policy_id = policy_id_part.replace("policy_id-", "")

            if policy_id not in policy_files:
                policy_files[policy_id] = []
            policy_files[policy_id].append(file)

    # For each policy ID, find the most recent file
    latest_files = []
    for policy_id, files in policy_files.items():
        # Sort files by timestamp (after the double underscore)
        sorted_files = sorted(files, key=lambda x: x.split("__")[-1].split(".")[0], reverse=True)
        if sorted_files:
            latest_files.append(sorted_files[0])

    return latest_files


def evaluate_model_outputs(model_name: str, model_dir_path: str, gt_path: str) -> Tuple[pd.DataFrame, Dict]:
    """Evaluate outputs for a single model against ground truth files."""
    print(f"\n=== Evaluating Model: {model_name} ===")

    # Lists to store evaluation results
    all_results = []
    y_true = []
    y_pred = []

    # Valid outcome categories
    valid_outcomes = [
        "Yes",
        "No - Unrelated event",
        "No - condition(s) not met"
    ]

    # Get the latest output file for each policy ID
    output_files = get_latest_output_files(model_dir_path)

    # Get all ground truth files
    gt_files = [f for f in os.listdir(gt_path) if f.startswith("GT_policy_") and f.endswith(".json")]

    if not output_files:
        print(f"Error: No output files found in {model_dir_path}")
        return None, None

    if not gt_files:
        print(f"Error: No ground truth files found in {gt_path}")
        return None, None

    # Map policy IDs to GT files for easier lookup
    gt_file_map = {f.split("_")[2].split(".")[0]: f for f in gt_files}

    print(f"Found {len(output_files)} output files and {len(gt_files)} ground truth files")

    total_output_questions = 0
    total_evaluated_questions = 0

    for output_file in output_files:
        # Extract policy ID from filename
        policy_id = output_file.split("__")[0].replace("policy_id-", "")

        # Find corresponding ground truth file
        if policy_id not in gt_file_map:
            print(f"Warning: No ground truth file found for policy {policy_id}")
            continue

        gt_file_name = gt_file_map[policy_id]
        gt_file_path = os.path.join(gt_path, gt_file_name)

        # Load files
        try:
            with open(os.path.join(model_dir_path, output_file), 'r', encoding='utf-8') as f:
                output_data = json.load(f)

            with open(gt_file_path, 'r', encoding='utf-8') as f:
                gt_data = json.load(f)
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            print(f"Error loading files for policy {policy_id}: {e}")
            continue

        # Create lookup dictionaries for faster access
        output_questions = output_data.get("questions", [])
        gt_questions_dict = {q["request_id"]: q for q in gt_data.get("questions", [])}

        questions_in_file = len(output_questions)
        total_output_questions += questions_in_file

        print(f"Policy {policy_id}: Found {questions_in_file} questions in output")

        # Compare each output question with ground truth
        for output_question in output_questions:
            request_id = output_question.get("request_id")

            if request_id not in gt_questions_dict:
                print(f"Warning: Policy {policy_id}, Request {request_id} not found in ground truth")
                continue

            gt_question = gt_questions_dict[request_id]
            total_evaluated_questions += 1

            # Get the outcome values and normalize them
            output_outcome = normalize_field(output_question.get("outcome", ""))
            gt_outcome = normalize_field(gt_question.get("outcome", ""))

            # Convert None to empty string for outcome comparison
            output_outcome = output_outcome if output_outcome is not None else ""
            gt_outcome = gt_outcome if gt_outcome is not None else ""

            # Add to lists for classification metrics
            y_true.append(gt_outcome)
            y_pred.append(output_outcome)

            # Calculate exact match for outcome
            outcome_match = 0 if output_outcome == gt_outcome else 1

            # Get and normalize justification fields
            output_justification = normalize_field(output_question.get("outcome_justification"))
            gt_justification = normalize_field(gt_question.get("outcome_justification"))

            # Calculate similarity scores
            justification_similarity = calculate_similarity_score(output_justification, gt_justification)
            justification_iou = calculate_text_iou(output_justification, gt_justification)

            # Get payment justification fields
            output_payment = normalize_field(output_question.get("payment_justification"))
            gt_payment = normalize_field(gt_question.get("payment_justification"))
            payment_similarity = calculate_similarity_score(output_payment, gt_payment)

            # Add to results
            all_results.append({
                "model_name": model_name,
                "policy_id": policy_id,
                "request_id": request_id,
                "question": gt_question.get("question", ""),
                "output_outcome": output_outcome,
                "gt_outcome": gt_outcome,
                "outcome_match": outcome_match,
                "output_justification": output_justification if output_justification is not None else "",
                "gt_justification": gt_justification if gt_justification is not None else "",
                "output_payment": output_payment if output_payment is not None else "",
                "gt_payment": gt_payment if gt_payment is not None else "",
                "justification_similarity": justification_similarity,
                "justification_iou": justification_iou,
                "payment_similarity": payment_similarity,
                "avg_justification_similarity": (justification_similarity + payment_similarity) / 2
                if justification_similarity is not None and payment_similarity is not None else None
            })

    if not all_results:
        print(f"No matching questions found for evaluation of model {model_name}")
        return None, None

    # Create DataFrame for results
    results_df = pd.DataFrame(all_results)

    # Calculate classification metrics
    outcome_accuracy = accuracy_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred, labels=valid_outcomes)
    class_report = classification_report(y_true, y_pred, labels=valid_outcomes, output_dict=True, zero_division=0)

    # Calculate per-category accuracy
    category_metrics = {}
    for category in valid_outcomes:
        if category in class_report:
            category_metrics[category] = {
                "precision": class_report[category]["precision"],
                "recall": class_report[category]["recall"],
                "f1-score": class_report[category]["f1-score"],
                "support": class_report[category]["support"]
            }

    # Calculate summary statistics
    summary = {
        "model_name": model_name,
        "total_output_questions": total_output_questions,
        "total_evaluated_questions": total_evaluated_questions,
        "outcome_classification": {
            "accuracy": float(outcome_accuracy),
            "category_metrics": category_metrics
        },
        "exact_outcome_matches": int((results_df["outcome_match"] == 0).sum()),
        "exact_outcome_match_percentage": float((results_df["outcome_match"] == 0).mean() * 100),
        "avg_justification_similarity": float(results_df["justification_similarity"].mean()),
        "avg_justification_iou": float(results_df["justification_iou"].mean()),
        "avg_payment_similarity": float(results_df["payment_similarity"].mean()),
        "avg_combined_justification_similarity": float(
            results_df["avg_justification_similarity"].dropna().mean()
            if not results_df["avg_justification_similarity"].dropna().empty else 0)
    }

    # Convert numpy types to native Python types for JSON serialization
    summary = numpy_to_python(summary)

    return results_df, summary


def save_evaluation_results(results_df: pd.DataFrame, summary: Dict,
                            model_name: str, output_dir: str) -> Dict[str, str]:
    """Save evaluation results to files and return file paths."""
    # Create model-specific directory
    model_output_dir = os.path.join(output_dir, model_name)
    os.makedirs(model_output_dir, exist_ok=True)

    # Generate timestamped filename
    timestamp = datetime.now().strftime("%d-%m-%Y_%H-%M-%S")

    # Save detailed results to CSV
    csv_filename = os.path.join(model_output_dir, f"evaluation_results_{timestamp}.csv")
    results_df.to_csv(csv_filename, index=False, na_rep='')

    # Save summary statistics
    summary_filename = os.path.join(model_output_dir, f"evaluation_summary_{timestamp}.json")
    with open(summary_filename, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2)

    # Save classification metrics
    metrics_filename = os.path.join(model_output_dir, f"classification_metrics_{timestamp}.json")

    # Extract classification metrics from summary
    conf_matrix = confusion_matrix(
        results_df["gt_outcome"].tolist(),
        results_df["output_outcome"].tolist(),
        labels=["Yes", "No - Unrelated event", "No - condition(s) not met"]
    )

    class_report = classification_report(
        results_df["gt_outcome"].tolist(),
        results_df["output_outcome"].tolist(),
        labels=["Yes", "No - Unrelated event", "No - condition(s) not met"],
        output_dict=True,
        zero_division=0
    )

    metrics_data = {
        "confusion_matrix": numpy_to_python(conf_matrix),
        "classification_report": class_report
    }

    with open(metrics_filename, 'w', encoding='utf-8') as f:
        json.dump(metrics_data, f, indent=2)

    return {
        'csv': csv_filename,
        'summary': summary_filename,
        'metrics': metrics_filename
    }


def print_evaluation_summary(summary: Dict):
    """Print evaluation summary for a model."""
    model_name = summary.get('model_name', 'Unknown')

    print(f"\n=== EVALUATION RESULTS FOR {model_name.upper()} ===")
    print(f"Total Questions in Output: {summary['total_output_questions']}")
    print(f"Total Questions Evaluated: {summary['total_evaluated_questions']}")
    print(f"Outcome Classification Accuracy: {summary['outcome_classification']['accuracy']:.4f} "
          f"({summary['exact_outcome_match_percentage']:.2f}%)")
    # print(f"Average Justification Similarity: {summary['avg_justification_similarity']:.4f}")
    print(f"Average Justification IoU: {summary['avg_justification_iou']:.4f}")
    # print(f"Average Payment Similarity: {summary['avg_payment_similarity']:.4f}")
    # print(f"Average Combined Justification Similarity: {summary['avg_combined_justification_similarity']:.4f}")


def compare_models(summaries: List[Dict]):
    """Print comparison between multiple models."""
    if len(summaries) < 2:
        return

    print(f"\n=== MODEL COMPARISON ===")
    print(f"{'Metric':<35} | " + " | ".join([f"{s['model_name']:<15}" for s in summaries]))
    print("-" * (35 + len(summaries) * 18))

    metrics = [
        ("Outcome Accuracy (%)", "exact_outcome_match_percentage", ".2f"),
        # ("Justification Similarity", "avg_justification_similarity", ".4f"),
        ("Justification IoU", "avg_justification_iou", ".4f"),
        # ("Payment Similarity", "avg_payment_similarity", ".4f"),
        # ("Combined Similarity", "avg_combined_justification_similarity", ".4f")
    ]

    for metric_name, key, fmt in metrics:
        values = [f"{s[key]:{fmt}}" for s in summaries]
        print(f"{metric_name:<35} | " + " | ".join([f"{v:<15}" for v in values]))


def main():
    parser = argparse.ArgumentParser(description='Evaluate insurance policy analysis results')
    parser.add_argument('--models', nargs='+',
                        help='Model names to evaluate (e.g., phi-4 qwen-2-5-72b-instruct). '
                             'If not specified, all available models will be evaluated.')
    parser.add_argument('--json-path', default=JSON_PATH,
                        help='Path to JSON output directory')
    parser.add_argument('--gt-path', default=GT_PATH,
                        help='Path to ground truth directory')
    parser.add_argument('--output-dir', default=EVALUATION_RESULTS_FILES_PATH,
                        help='Directory to save evaluation results')

    args = parser.parse_args()

    # Install required libraries if missing
    try:
        import editdistance
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
    except ImportError as e:
        print(f"Installing missing library: {e.name}...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", e.name])
        if e.name == "editdistance":
            import editdistance
        elif e.name == "scikit-learn":
            from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

    # Make global for use in other functions
    global accuracy_score, confusion_matrix, classification_report
    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

    # Get available model directories
    available_models = get_model_directories(args.json_path)

    if not available_models:
        print(f"No model directories found in {args.json_path}")
        return

    # Determine which models to evaluate
    if args.models:
        models_to_evaluate = []
        for model in args.models:
            if model in available_models:
                models_to_evaluate.append(model)
            else:
                print(f"Warning: Model '{model}' not found in available models: {available_models}")

        if not models_to_evaluate:
            print("No valid models specified for evaluation")
            return
    else:
        models_to_evaluate = available_models
        print(f"No specific models specified. Evaluating all available models: {models_to_evaluate}")

    # Evaluate each model
    all_summaries = []
    all_results_dfs = []

    for model_name in models_to_evaluate:
        model_dir_path = os.path.join(args.json_path, model_name)

        print(f"\nEvaluating model: {model_name}")
        results_df, summary = evaluate_model_outputs(model_name, model_dir_path, args.gt_path)

        if results_df is not None and summary is not None:
            # Save results
            saved_files = save_evaluation_results(results_df, summary, model_name, args.output_dir)

            # Print summary
            print_evaluation_summary(summary)

            print(f"\nResults saved:")
            print(f"  CSV: {saved_files['csv']}")
            print(f"  Summary: {saved_files['summary']}")
            print(f"  Metrics: {saved_files['metrics']}")

            all_summaries.append(summary)
            all_results_dfs.append(results_df)
        else:
            print(f"Failed to evaluate model: {model_name}")

    # Print comparison if multiple models were evaluated
    if len(all_summaries) > 1:
        compare_models(all_summaries)

    print(f"\n=== EVALUATION COMPLETE ===")
    print(f"Evaluated {len(all_summaries)} models successfully")
    print(f"Results saved to: {args.output_dir}")


if __name__ == "__main__":
    main()


# End of scripts/evaluate_results.py
# ================================================================================

# File 35/41: scripts/latex_tables.py
# --------------------------------------------------------------------------------

# scripts/latex_tables.py

import os
import json
import pandas as pd
import argparse
import sys
from datetime import datetime
from typing import Dict, List, Optional
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import EVALUATION_RESULTS_FILES_PATH


def find_most_recent_evaluation_files(eval_dir: str) -> Dict[str, str]:
    """Find the most recent evaluation files based on timestamp in filename."""
    if not os.path.exists(eval_dir):
        raise FileNotFoundError(f"Evaluation directory not found: {eval_dir}")

    all_files = os.listdir(eval_dir)

    # Get all files by type
    csv_files = [f for f in all_files if f.startswith("evaluation_results_") and f.endswith(".csv")]
    summary_files = [f for f in all_files if f.startswith("evaluation_summary_") and f.endswith(".json")]
    metrics_files = [f for f in all_files if f.startswith("classification_metrics_") and f.endswith(".json")]

    missing_types = []
    if not csv_files:
        missing_types.append("CSV files (evaluation_results_*.csv)")
    if not summary_files:
        missing_types.append("Summary files (evaluation_summary_*.json)")
    if not metrics_files:
        missing_types.append("Metrics files (classification_metrics_*.json)")

    if missing_types:
        raise FileNotFoundError(f"Missing evaluation files in {eval_dir}. Missing: {', '.join(missing_types)}")

    # Sort by timestamp and get most recent
    csv_files.sort(reverse=True)
    summary_files.sort(reverse=True)
    metrics_files.sort(reverse=True)

    return {
        'csv': os.path.join(eval_dir, csv_files[0]),
        'summary': os.path.join(eval_dir, summary_files[0]),
        'metrics': os.path.join(eval_dir, metrics_files[0])
    }


def load_model_data(results_dir: str, model_name: str) -> Dict:
    """Load evaluation data for a specific model."""
    model_dir = os.path.join(results_dir, model_name)

    try:
        recent_files = find_most_recent_evaluation_files(model_dir)

        # Read the files
        results_df = pd.read_csv(recent_files['csv'])

        with open(recent_files['summary'], 'r', encoding='utf-8') as f:
            summary_data = json.load(f)

        with open(recent_files['metrics'], 'r', encoding='utf-8') as f:
            metrics_data = json.load(f)

        return {
            'results_df': results_df,
            'summary_data': summary_data,
            'metrics_data': metrics_data,
            'files': recent_files
        }

    except Exception as e:
        raise Exception(f"Error loading data for model '{model_name}': {str(e)}")


def escape_latex(text: str) -> str:
    """Escape special LaTeX characters in text."""
    if not isinstance(text, str):
        return str(text)

    # Common LaTeX escape sequences
    replacements = {
        '_': '\\_',
        '%': '\\%',
        '$': '\\$',
        '#': '\\#',
        '&': '\\&',
        '{': '\\{',
        '}': '\\}',
        '^': '\\textasciicircum{}',
        '~': '\\textasciitilde{}',
    }

    for char, escape in replacements.items():
        text = text.replace(char, escape)

    return text


def generate_evaluation_summary_table(summary_data: Dict, model_name: Optional[str] = None) -> str:
    """Generate LaTeX table for evaluation summary with specific metrics."""
    predicted_outcome = summary_data.get('exact_outcome_match_percentage', 0)
    justification_outcome = summary_data.get('avg_justification_similarity', 0)
    justification_payment = summary_data.get('avg_payment_similarity', 0)
    justification_iou = summary_data.get('avg_justification_iou', 0)

    title_suffix = f" - {escape_latex(model_name)}" if model_name else ""
    label_suffix = f"_{model_name.lower().replace('-', '_').replace('.', '_')}" if model_name else ""

    latex = r'''\begin{table}[H]
\centering
\caption{Evaluation Summary Table''' + title_suffix + r'''}
\label{tab:evaluation_summary''' + label_suffix + r'''}
\begin{tabular}{@{}lp{2cm}@{}}
\toprule
\textbf{Field} & \textbf{Result} \\
\midrule
Predicted Outcome (Accu.) & \textbf{''' + f"{predicted_outcome:.2f}\\%" + r'''} \\
Justification Outcome (SED) &  \textbf{''' + f"{justification_outcome:.4f}" + r'''} \\
Justification Payment (SED) &  \textbf{''' + f"{justification_payment:.4f}" + r'''} \\
Justification Outcome (IoU) &  \textbf{''' + f"{justification_iou:.4f}" + r'''} \\
\bottomrule
\end{tabular}
\end{table}'''

    return latex


def generate_model_comparison_table(model_data_list: List[Dict], model_names: List[str]) -> str:
    """Generate LaTeX table comparing multiple models."""
    if len(model_data_list) < 2:
        raise ValueError("At least 2 models required for comparison")

    # Extract metrics for all models
    def extract_metrics(summary_data):
        return {
            'predicted_outcome': summary_data.get('exact_outcome_match_percentage', 0),
            'justification_outcome': summary_data.get('avg_justification_similarity', 0),
            'justification_payment': summary_data.get('avg_payment_similarity', 0),
            'justification_iou': summary_data.get('avg_justification_iou', 0)
        }

    all_metrics = [extract_metrics(data['summary_data']) for data in model_data_list]

    # Create table header with model names
    escaped_names = [escape_latex(name) for name in model_names]
    header_cols = 'l' + 'c' * len(model_names)

    latex = r'''\begin{table}[H]
\centering
\caption{Model Comparison: ''' + ' vs '.join(escaped_names) + r'''}
\label{tab:model_comparison}
\begin{tabular}{@{}''' + header_cols + r'''@{}}
\toprule
\textbf{Metric} & ''' + ' & '.join([f"\\textbf{{{name}}}" for name in escaped_names]) + r''' \\
\midrule'''

    # Add rows for each metric
    metrics_info = [
        ("Predicted Outcome (Accu.)", "predicted_outcome", ":.2f", "%"),
        ("Justification Outcome (SED)", "justification_outcome", ":.4f", ""),
        ("Justification Payment (SED)", "justification_payment", ":.4f", ""),
        ("Justification Outcome (IoU)", "justification_iou", ":.4f", "")
    ]

    for metric_name, key, fmt, suffix in metrics_info:
        values = [f"{metrics[key]:{fmt[1:]}}{suffix}" for metrics in all_metrics]
        latex += f"\n{metric_name} & " + " & ".join(values) + r" \\"

    latex += r'''
\bottomrule
\end{tabular}
\end{table}'''

    return latex


def generate_comparison_classification_metrics_table(model_data_list: List[Dict], model_names: List[str]) -> str:
    """Generate LaTeX table comparing classification metrics between multiple models."""
    if len(model_data_list) < 2:
        raise ValueError("At least 2 models required for comparison")

    valid_outcomes = [
        "Yes",
        "No - Unrelated event",
        "No - condition(s) not met"
    ]

    escaped_names = [escape_latex(name) for name in model_names]

    # Create dynamic column specification
    col_spec = "l" + "ccc" * len(model_names)

    latex = r'''\begin{table}[H]
\centering
\caption{Classification Metrics Comparison: ''' + ' vs '.join(escaped_names) + r'''}
\label{tab:classification_comparison}
\begin{tabular}{''' + col_spec + r'''}
\toprule
\multirow{2}{*}{\textbf{Outcome Category}}'''

    # Add headers for each model
    for i, name in enumerate(escaped_names):
        latex += f" & \\multicolumn{{3}}{{c}}{{\\textbf{{{name}}}}}"

    latex += r''' \\
\cmidrule(lr){2-4}'''

    # Add cmidrules for additional models
    for i in range(1, len(model_names)):
        start_col = 2 + i * 3
        end_col = start_col + 2
        latex += f" \\cmidrule(lr){{{start_col}-{end_col}}}"

    latex += r'''
'''

    # Add sub-headers (Prec., Rec., F1 for each model)
    subheaders = []
    for _ in model_names:
        subheaders.extend([r"\textbf{Prec.}", r"\textbf{Rec.}", r"\textbf{F1}"])

    latex += " & " + " & ".join(subheaders) + r" \\"
    latex += r'''
\midrule'''

    # Add rows for each outcome category
    for outcome in valid_outcomes:
        outcome_escaped = outcome.replace('-', '\\-')
        row_data = [outcome_escaped]

        for model_data in model_data_list:
            report = model_data['metrics_data'].get('classification_report', {})
            precision = report.get(outcome, {}).get('precision', 0)
            recall = report.get(outcome, {}).get('recall', 0)
            f1 = report.get(outcome, {}).get('f1-score', 0)

            row_data.extend([f"{precision:.3f}", f"{recall:.3f}", f"{f1:.3f}"])

        latex += f"\n{' & '.join(row_data)} \\\\"

    # Add overall accuracy comparison
    latex += r'''
\midrule
\textbf{Overall Accuracy}'''

    for model_data in model_data_list:
        accuracy = model_data['summary_data'].get('outcome_classification', {}).get('accuracy', 0)
        latex += f" & \\multicolumn{{3}}{{c}}{{{accuracy:.4f}}}"

    latex += r''' \\
\bottomrule
\end{tabular}
\end{table}'''

    return latex


def generate_confusion_matrix_table(metrics_data: Dict, model_name: Optional[str] = None) -> str:
    """Generate LaTeX table for confusion matrix."""
    conf_matrix = metrics_data.get('confusion_matrix', [])

    if not conf_matrix:
        return "% Error: Confusion matrix data not found"

    valid_outcomes = [
        "Yes",
        "No - Unrelated event",
        "No - condition(s) not met"
    ]

    # Calculate totals
    row_totals = [sum(row) for row in conf_matrix]
    col_totals = [sum(conf_matrix[row_idx][col_idx] for row_idx in range(len(conf_matrix)))
                  for col_idx in range(len(conf_matrix[0]))]
    total = sum(row_totals)

    title_suffix = f" - {escape_latex(model_name)}" if model_name else ""
    label_suffix = f"_{model_name.lower().replace('-', '_').replace('.', '_')}" if model_name else ""

    latex = r'''\begin{table}[H]
\centering
\caption{Confusion Matrix of Outcome Classifications''' + title_suffix + r'''}
\label{tab:confusion_matrix''' + label_suffix + r'''}
\begin{tabular}{lccc|c}
\toprule
\multirow{2}{*}{\textbf{Actual Outcome}} & \multicolumn{3}{c}{\textbf{Predicted Outcome}} & \multirow{2}{*}{\textbf{Total}} \\
\cmidrule{2-4}
& \textbf{Yes} & \textbf{\begin{tabular}[c]{@{}c@{}}No - Unrelated\\event\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}No - condition(s)\\not met\end{tabular}} & \\
\midrule'''

    for i, outcome in enumerate(valid_outcomes):
        if i < len(conf_matrix) and len(conf_matrix[i]) >= len(valid_outcomes):
            row = [str(conf_matrix[i][j]) for j in range(len(valid_outcomes))]
            row_str = " & ".join(row)
            latex += f"\n\\textbf{{{outcome.replace('-', '\\-')}}} & {row_str} & {row_totals[i]} \\\\"

    latex += r'''
\midrule
\textbf{Total} & ''' + " & ".join([str(val) for val in col_totals[:len(valid_outcomes)]]) + f" & {total}" + r''' \\
\bottomrule
\end{tabular}
\end{table}'''

    return latex


def generate_classification_metrics_table(metrics_data: Dict, summary_data: Dict,
                                          model_name: Optional[str] = None) -> str:
    """Generate LaTeX table for classification metrics."""
    report = metrics_data.get('classification_report', {})

    valid_outcomes = [
        "Yes",
        "No - Unrelated event",
        "No - condition(s) not met"
    ]

    accuracy = summary_data.get('outcome_classification', {}).get('accuracy', 0)

    title_suffix = f" - {escape_latex(model_name)}" if model_name else ""
    label_suffix = f"_{model_name.lower().replace('-', '_').replace('.', '_')}" if model_name else ""

    latex = r'''\begin{table}[H]
\centering
\caption{Performance Metrics by Outcome Category''' + title_suffix + r'''}
\label{tab:classification_metrics''' + label_suffix + r'''}
\begin{tabular}{lccccc}
\toprule
\textbf{Outcome Category} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} & \textbf{Accuracy} \\
\midrule'''

    # Add rows for each outcome category
    for i, outcome in enumerate(valid_outcomes):
        if outcome in report:
            precision = report[outcome].get('precision', 0)
            recall = report[outcome].get('recall', 0)
            f1 = report[outcome].get('f1-score', 0)
            support = report[outcome].get('support', 0)

            outcome_escaped = outcome.replace('-', '\\-')
            accuracy_cell = f"{accuracy:.4f}" if i == 0 else ""
            latex += f"\n{outcome_escaped} & {precision:.4f} & {recall:.4f} & {f1:.4f} & {int(support)} & {accuracy_cell} \\\\"

    # Add weighted average row
    if 'weighted avg' in report:
        w_precision = report['weighted avg'].get('precision', 0)
        w_recall = report['weighted avg'].get('recall', 0)
        w_f1 = report['weighted avg'].get('f1-score', 0)
        w_support = report['weighted avg'].get('support', 0)

        latex += r'''
\midrule
\textbf{Weighted Average} & ''' + f"{w_precision:.4f} & {w_recall:.4f} & {w_f1:.4f} & {int(w_support)} &" + r''' \\
\bottomrule
\multicolumn{6}{p{14cm}}{\textit{Note:} Overall outcome classification accuracy: ''' + f"{accuracy:.4f}" + r''' (''' + f"{summary_data.get('exact_outcome_match_percentage', 0):.2f}\\%" + r''').} \\
\end{tabular}
\end{table}'''

    return latex


def get_available_models(results_dir: str) -> List[str]:
    """Get list of available model directories."""
    if not os.path.exists(results_dir):
        return []

    models = []
    for item in os.listdir(results_dir):
        item_path = os.path.join(results_dir, item)
        if os.path.isdir(item_path):
            # Check if directory contains evaluation files
            try:
                find_most_recent_evaluation_files(item_path)
                models.append(item)
            except FileNotFoundError:
                continue

    return sorted(models)


def main():
    parser = argparse.ArgumentParser(description='Generate LaTeX tables for evaluation results')
    parser.add_argument('--input-results', default=EVALUATION_RESULTS_FILES_PATH,
                        help='Directory containing the evaluation results')
    parser.add_argument('--models', nargs='+',
                        help='Model names to generate tables for (e.g., phi-4 qwen-2-5-72b-instruct). '
                             'If not specified, all available models will be processed.')
    parser.add_argument('--output',
                        help='Output file path for LaTeX tables (default: auto-generated)')
    parser.add_argument('--compare-only', action='store_true',
                        help='Only generate comparison tables (requires multiple models)')

    args = parser.parse_args()

    try:
        # Get available models
        available_models = get_available_models(args.input_results)

        if not available_models:
            print(f"No models with evaluation results found in {args.input_results}")
            return

        # Determine which models to process
        if args.models:
            models_to_process = []
            for model in args.models:
                if model in available_models:
                    models_to_process.append(model)
                else:
                    print(f"Warning: Model '{model}' not found in available models: {available_models}")

            if not models_to_process:
                print("No valid models specified")
                return
        else:
            models_to_process = available_models
            print(f"No specific models specified. Processing all available models: {models_to_process}")

        # Load data for all models
        model_data_list = []
        for model_name in models_to_process:
            print(f"Loading data for model: {model_name}")
            try:
                model_data = load_model_data(args.input_results, model_name)
                model_data_list.append(model_data)
            except Exception as e:
                print(f"Error loading data for model '{model_name}': {str(e)}")
                continue

        if not model_data_list:
            print("No valid model data loaded")
            return

        print(f"Successfully loaded data for {len(model_data_list)} models")

        # Generate LaTeX tables
        all_latex = f"""% LaTeX Tables for Insurance Policy Analysis Evaluation
% Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
% Models: {', '.join(models_to_process)}

"""

        # Generate comparison tables if multiple models
        if len(model_data_list) > 1:
            print("Generating comparison tables...")

            comparison_table = generate_model_comparison_table(model_data_list, models_to_process)
            classification_comparison_table = generate_comparison_classification_metrics_table(
                model_data_list, models_to_process)

            all_latex += f"""% === COMPARISON TABLES ===

{comparison_table}

{classification_comparison_table}

"""

        # Generate individual tables unless comparison-only mode
        if not args.compare_only:
            for i, (model_data, model_name) in enumerate(zip(model_data_list, models_to_process)):
                print(f"Generating individual tables for model: {model_name}")

                evaluation_summary_table = generate_evaluation_summary_table(
                    model_data['summary_data'], model_name)
                confusion_table = generate_confusion_matrix_table(
                    model_data['metrics_data'], model_name)
                classification_table = generate_classification_metrics_table(
                    model_data['metrics_data'], model_data['summary_data'], model_name)

                all_latex += f"""% === INDIVIDUAL TABLES FOR {model_name.upper()} ===

{evaluation_summary_table}

{confusion_table}

{classification_table}

"""

        # Save to file
        if args.output:
            output_file = args.output
        else:
            timestamp = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')
            if len(models_to_process) == 1:
                output_filename = f"latex_tables_{models_to_process[0]}_{timestamp}.tex"
            else:
                output_filename = f"latex_tables_comparison_{timestamp}.tex"
            output_file = os.path.join(args.input_results, output_filename)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(all_latex)

        print(f"\nLaTeX tables generated and saved to: {output_file}")

        # Also print summary to console
        print("\n" + "=" * 80)
        print("LATEX TABLES GENERATED:")
        print("=" * 80)
        if len(model_data_list) > 1:
            print("✓ Model comparison tables")
            print("✓ Classification comparison tables")
        if not args.compare_only:
            for model_name in models_to_process:
                print(f"✓ Individual tables for {model_name}")

    except Exception as e:
        print(f"Error generating LaTeX tables: {str(e)}")


if __name__ == "__main__":
    main()


# End of scripts/latex_tables.py
# ================================================================================

# File 36/41: scripts/main_flask.py
# --------------------------------------------------------------------------------

from flask import Flask, render_template, request, jsonify, send_from_directory
import os
import json

from config import EVALUATION_RESULTS_PATH, EVALUATION_RESULTS_FILES_PATH

app = Flask(__name__, template_folder='templates', static_folder='static')


@app.route('/')
def index():
    # Pass the correct path to your evaluation results
    correct_path = EVALUATION_RESULTS_FILES_PATH
    print(f"Passing evaluation results path to template: {correct_path}")
    return render_template('dashboard.html', evaluation_results_path=correct_path)


@app.route('/api/evaluations')
def list_evaluations():
    path = request.args.get('path', EVALUATION_RESULTS_FILES_PATH)
    print(f"API - Looking for files in: {path}")

    if not os.path.exists(path):
        print(f"Path not found: {path}")
        return jsonify({'success': False, 'error': f'Path not found: {path}'})

    files = []
    for filename in os.listdir(path):
        if filename.endswith('.json') or filename.endswith('.csv'):
            files.append(os.path.join(path, filename))

    print(f"API - Found {len(files)} files: {files}")
    return jsonify({'success': True, 'files': files})


@app.route('/api/file')
def get_file():
    filepath = request.args.get('path', EVALUATION_RESULTS_PATH)
    print(f"API - Request to read file: {filepath}")

    if not filepath or not os.path.exists(filepath):
        print(f"File not found: {filepath}")
        return jsonify({'success': False, 'error': 'File not found'})

    if filepath.endswith('.json'):
        with open(filepath, 'r') as f:
            return jsonify(json.load(f))
    elif filepath.endswith('.csv'):
        with open(filepath, 'r') as f:
            return f.read(), 200, {'Content-Type': 'text/plain'}
    else:
        return jsonify({'success': False, 'error': 'Unsupported file type'})


# Static file handling (for development)
@app.route('/static/<path:filename>')
def serve_static(filename):
    return send_from_directory('static', filename)


if __name__ == '__main__':
    # Make sure the templates directory exists
    os.makedirs('templates', exist_ok=True)

    # Print configuration for debugging
    print(f"Using evaluation results path: {EVALUATION_RESULTS_FILES_PATH}")

    # Start the server
    app.run(debug=True, host='0.0.0.0', port=5000)


# End of scripts/main_flask.py
# ================================================================================

# File 37/41: scripts/policy_converter.py
# --------------------------------------------------------------------------------

import json
import os.path

import pandas as pd

from config import GT_PATH


def convert_policy_to_excel(json_data, output_file='policy_output.xlsx'):
    """
    Convert JSON policy data to Excel format

    Parameters:
    json_data (str): Path to the JSON file or JSON string
    output_file (str): Path for the output Excel file
    """
    # Load the JSON data
    if isinstance(json_data, str) and json_data.endswith('.json'):
        # It's a file path
        with open(json_data, 'r') as f:
            policy = json.load(f)
    else:
        # It's a JSON string
        policy = json.loads(json_data)

    # Extract the policy ID and questions
    policy_id = policy['policy_id']
    questions = policy['questions']

    # Prepare data for DataFrame
    data = []
    for question in questions:
        # Map outcome to status
        outcome = question['outcome']
        if outcome == 'Yes':
            status = 'validated by saverio'
        elif outcome == 'Maybe':
            status = 'unsure'
        else:  # No cases
            status = 'not done'

        # Create row with required data
        row = {
            'policy_id': policy_id,
            'request_id': question['request_id'],
            'status': status,
            'question': question['question'],
            'outcome': outcome,
            'outcome_justification': question['outcome_justification'],
            'payment_justification': question.get('payment_justification', '')
        }
        data.append(row)

    # Create DataFrame and save to Excel
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)
    print(f"Successfully converted policy {policy_id} to Excel: {output_file}")
    return df


if __name__ == "__main__":
    json_file = os.path.join(GT_PATH, 'GT_policy_26.json')
    convert_policy_to_excel(json_file)


# End of scripts/policy_converter.py
# ================================================================================

# File 38/41: scripts/prepare_GT_labels.py
# --------------------------------------------------------------------------------

import os
import json
import datetime

from config import RAW_GT_PATH, GT_PATH


def convert_ground_truth_files(input_dir, output_dir):
    """
    Convert raw ground truth JSON files to evaluation format.
    Args:
        input_dir: Directory containing raw ground truth JSON files
        output_dir: Directory to save formatted JSON files
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Define outcome mapping
    outcome_mapping = {
        "APPROVED": "Yes",
        "DENIED_EXCLUDED": "No - condition(s) not met",
        "DENIED_UNRELATED": "No - Unrelated event",
        "UNDECIDED": "Maybe"
    }

    # Process each JSON file in the input directory
    for filename in os.listdir(input_dir):
        if not filename.endswith('.json'):
            continue

        input_path = os.path.join(input_dir, filename)

        # Read the input JSON file
        with open(input_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)

        # Extract policy_id
        policy_id = str(raw_data.get('policy_id', ''))

        # Create output structure
        output_data = {
            "policy_id": policy_id,
            "questions": []
        }

        # Process each policy request
        for policy_request in raw_data.get('policy_requests', []):
            # Get the request data
            request = policy_request.get('request', {})
            request_id = str(request.get('request_id', ''))
            question = request.get('content', '')

            # Get the latest response (highest version number)
            responses = policy_request.get('responses', [])
            if not responses:
                continue

            # Sort responses by version and get the latest
            latest_response = max(responses, key=lambda r: r.get('updated_at', ''))

            # Extract outcome and justifications
            raw_outcome = latest_response.get('response_outcome', '')
            # Map the outcome to the desired format
            outcome = outcome_mapping.get(raw_outcome, raw_outcome)

            outcome_justification = latest_response.get('response_outcome_justification', '')
            payment_justification = latest_response.get('payout_justification', '')

            # Add to output data
            output_data['questions'].append({
                "request_id": request_id,
                "question": question,
                "outcome": outcome,
                "outcome_justification": outcome_justification,
                "payment_justification": payment_justification if payment_justification else None
            })

        # Sort questions by request_id
        output_data['questions'].sort(
            key=lambda q: int(q['request_id']) if q['request_id'].isdigit() else q['request_id'])

        # Write output file
        output_filename = f"GT_policy_{policy_id}.json"
        output_path = os.path.join(output_dir, output_filename)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        print(f"Converted {filename} -> {output_filename}")


# Example usage
if __name__ == "__main__":
    # Define source and target directories
    raw_gt_dir = RAW_GT_PATH
    formatted_gt_dir = GT_PATH

    convert_ground_truth_files(raw_gt_dir, formatted_gt_dir)

# End of scripts/prepare_GT_labels.py
# ================================================================================

# File 39/41: scripts/text_iou.py
# --------------------------------------------------------------------------------

import re
import json
import argparse
import os
import sys

from pathlib import Path

# Fix import error by using absolute import
# Add the parent directory to sys.path to make the import work
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import JSON_PATH, GT_PATH, base_dir


class TextIoU:
    def __init__(self):
        pass

    def normalize_field(self, text):
        """Placeholder for text normalization function."""
        # Replace this with your actual normalize_field implementation
        if text is None:
            return ""
        return str(text).strip()

    def calculate(self, text1, text2):
        """Calculate Intersection over Union (IoU) for text strings based on word sets."""
        # Normalize inputs
        text1 = self.normalize_field(text1)
        text2 = self.normalize_field(text2)

        # Handle None and empty cases
        if text1 is None:
            text1 = ""
        if text2 is None:
            text2 = ""

        # If both are empty, return 1.0 (perfect match)
        if text1 == "" and text2 == "":
            return 1.0

        # If one is empty and the other is not, return 0.0
        if (text1 == "" and text2 != "") or (text1 != "" and text2 == ""):
            return 0.0

        # Simple tokenization: split by whitespace and remove punctuation, convert to lowercase
        def tokenize(text):
            # Remove punctuation and convert to lowercase, then split
            tokens = re.findall(r'\b\w+\b', text.lower())
            return set(tokens)

        tokens1 = tokenize(str(text1))
        tokens2 = tokenize(str(text2))

        # Calculate intersection and union
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)

        # Calculate IoU
        if len(union) == 0:
            return 1.0  # Both have no tokens after processing

        iou = len(intersection) / len(union)
        return iou


def load_json_file(file_path):
    """Load and parse a JSON file."""
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)


def compare_outcome_justifications(llm_json, gt_json):
    """Compare outcome_justifications between LLM output and ground truth."""
    text_iou = TextIoU()
    results = []

    # Create a dictionary of GT questions by request_id for easy lookup
    gt_questions_dict = {q["request_id"]: q for q in gt_json["questions"]}

    for llm_question in llm_json["questions"]:
        request_id = llm_question["request_id"]

        if request_id in gt_questions_dict:
            gt_question = gt_questions_dict[request_id]

            # Extract outcome_justification fields
            llm_justification = llm_question.get("outcome_justification", "")
            gt_justification = gt_question.get("outcome_justification", "")

            # Calculate IoU score
            iou_score = text_iou.calculate(llm_justification, gt_justification)

            # Store results
            results.append({
                "request_id": request_id,
                "llm_justification": llm_justification,
                "gt_justification": gt_justification,
                "iou_score": iou_score
            })
        else:
            print(f"Warning: Request ID {request_id} not found in ground truth data")

    return results


def calculate_average_iou(comparison_results):
    """Calculate the average IoU score from the comparison results."""
    if not comparison_results:
        return 0.0

    total_score = sum(result["iou_score"] for result in comparison_results)
    return total_score / len(comparison_results)


def get_file_path(file_arg, default_dir):
    """Get the file path, checking if the file exists in default location or as a direct path."""
    # First try the default directory
    default_path = os.path.join(default_dir, file_arg)
    if os.path.isfile(default_path):
        return default_path

    # If not found, check if the argument is a direct path to an existing file
    if os.path.isfile(file_arg):
        return file_arg

    # Look in the current directory
    current_dir_path = os.path.join(os.getcwd(), file_arg)
    if os.path.isfile(current_dir_path):
        return current_dir_path

    # If not found anywhere, return the default path (will cause error later)
    print(f"WARNING: Could not find file '{file_arg}' in default directory or as direct path.")
    print(f"Tried: \n- {default_path}\n- {file_arg}\n- {current_dir_path}")
    return default_path


def main():
    parser = argparse.ArgumentParser(
        description='Compare outcome justifications between LLM and ground truth JSON files')
    parser.add_argument('llm_file', help='Filename or path to the LLM-generated JSON file')
    parser.add_argument('gt_file', help='Filename or path to the ground truth JSON file')
    parser.add_argument('--output', help='Filename or path for saving detailed comparison results')

    args = parser.parse_args()

    # Get file paths, checking multiple locations
    llm_file_path = get_file_path(args.llm_file, JSON_PATH)
    gt_file_path = get_file_path(args.gt_file, GT_PATH)

    # Load JSON files
    print(f"Loading LLM file: {llm_file_path}")
    try:
        llm_json = load_json_file(llm_file_path)
    except FileNotFoundError:
        print(f"ERROR: LLM file not found at '{llm_file_path}'")
        print("Please provide a valid path to the LLM output JSON file.")
        return

    print(f"Loading ground truth file: {gt_file_path}")
    try:
        gt_json = load_json_file(gt_file_path)
    except FileNotFoundError:
        print(f"ERROR: Ground truth file not found at '{gt_file_path}'")
        print("Please provide a valid path to the ground truth JSON file.")
        return

    # Compare outcome justifications
    print("Comparing outcome justifications...")
    comparison_results = compare_outcome_justifications(llm_json, gt_json)

    # Calculate and display average IoU
    avg_iou = calculate_average_iou(comparison_results)
    print(f"\nAverage IoU Score: {avg_iou:.4f}")

    # Output detailed results if requested
    if args.output:
        # Determine output path
        if os.path.isabs(args.output):
            output_path = args.output
        else:
            output_path = os.path.join(JSON_PATH, args.output)

        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as file:
            json.dump({
                "average_iou": avg_iou,
                "detailed_results": comparison_results
            }, file, indent=2)
        print(f"Detailed results saved to {output_path}")

    print("\nSample of comparison results:")
    for result in comparison_results[:3]:  # Show first 3 results
        print(f"Request ID: {result['request_id']}")
        print(f"LLM justification: {result['llm_justification'][:50]}..." if len(
            result['llm_justification']) > 50 else f"LLM justification: {result['llm_justification']}")
        print(f"GT justification: {result['gt_justification'][:50]}..." if len(
            result['gt_justification']) > 50 else f"GT justification: {result['gt_justification']}")
        print(f"IoU Score: {result['iou_score']:.4f}\n")


if __name__ == "__main__":
    main()


# End of scripts/text_iou.py
# ================================================================================

# File 40/41: show_table.py
# --------------------------------------------------------------------------------

import csv
import textwrap

from config import RESULT_PATH

# Path to your result file
file_path = RESULT_PATH

# Field labels (in the order they appear in the TSV file)
field_labels = [
    "Vector Store",
    "Question ID",
    "Question",
    "Eligibility",
    "Eligibility Policy",
    "Amount Policy",
    "Amount Policy Line"
]

# Max line width before wrapping
wrap_width = 80

def display_result_table(row):
    print("\n" + "-" * wrap_width*2)
    print("{:<20}{}".format("Field", "Value"))
    print("-" * wrap_width*2)
    for label, value in zip(field_labels, row):
        wrapped_value = textwrap.fill(value, width=wrap_width, subsequent_indent=" " * 22)
        print(f"{label:<20}{wrapped_value}")
    print("-" * wrap_width*2)

# Read and display each row from the TSV file
with open(file_path, "r", encoding="utf-8") as file:
    reader = csv.reader(file, delimiter="\t")
    for row in reader:
        if row:  # skip empty lines
            display_result_table(row)


# End of show_table.py
# ================================================================================

# File 41/41: utils.py
# --------------------------------------------------------------------------------

# src/utils.py
import json

import pandas as pd
import os
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def read_questions(path: str) -> pd.DataFrame:
    """
    Load an Excel file containing insurance-related questions.

    Args:
        path (str): Path to the Excel file.

    Returns:
        pd.DataFrame: DataFrame of questions.
    """
    logger.info(f"Read question path: {path}")
    df = pd.read_excel(path)
    return df

# def list_pdf_paths(directory: str) -> List[str]:
#     """
#     List all PDF file paths within the specified directory.
#
#     Args:
#         directory (str): Path to the directory containing PDFs.
#
#     Returns:
#         list[str]: List of absolute file paths.
#     """
#     logger.info(f"List pdf paths: {directory}")
#     return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(".pdf")]

def load_response_schema(path: str) -> Dict[str, Any]:
    """
    Load the JSON schema used to enforce assistant response structure.

    Args:
        path (str): Path to the JSON schema file.

    Returns:
        dict: JSON schema dictionary.
    """
    logger.info(f"Load response schema: {path}")
    with open(path, 'r', encoding='utf-8') as file:
        return json.load(file)


def list_policy_paths(directory: str) -> List[str]:
    """
    List all PDF and TXT file paths within the specified directory.

    Args:
        directory (str): Path to the directory containing policy files.

    Returns:
        list[str]: List of absolute file paths.
    """
    logger.info(f"List policy paths: {directory}")
    policy_files = []

    for f in os.listdir(directory):
        if f.endswith((".pdf", ".txt")):
            policy_files.append(os.path.join(directory, f))

    logger.info(f"Found {len(policy_files)} policy files (PDF and TXT)")
    return policy_files






# End of utils.py
# ================================================================================
